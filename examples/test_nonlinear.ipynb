{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/amine.mcharrak/miniconda3/envs/pcax24/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Detecting CUDA device(s) : [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjacency matrix loaded from ../data/G_A_init_t_ordered_adj_matrix.npy\n",
      "Adjacency matrix loaded from ../data/G_A_init_t_ordered_dag_adj_matrix.npy\n",
      "Adjacency matrix loaded from ../data/ER_adj_matrix.npy\n",
      "Adjacency matrix loaded from ../data/ER_dag_adj_matrix.npy\n",
      "20.0\n",
      "['Mask', 'Optim', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '_mask', '_misc', '_optim', '_serialisation', 'load_params', 'm', 'save_params', 'step']\n"
     ]
    }
   ],
   "source": [
    "from typing import Callable\n",
    "import os\n",
    "\n",
    "# choose the GPU to use\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "# disable preallocation of memory\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\n",
    "\n",
    "# pcax\n",
    "import pcax as px\n",
    "import pcax.predictive_coding as pxc\n",
    "import pcax.nn as pxnn\n",
    "import pcax.functional as pxf\n",
    "import pcax.utils as pxu\n",
    "\n",
    "# 3rd party\n",
    "import jax\n",
    "from jax import jit\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "import numpy as np\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import timeit\n",
    "\n",
    "# own\n",
    "import causal_helpers\n",
    "from causal_helpers import simulate_dag, simulate_parameter, simulate_linear_sem, simulate_linear_sem_cyclic\n",
    "from causal_helpers import load_adjacency_matrix, set_random_seed, plot_adjacency_matrices\n",
    "\n",
    "# Set random seed\n",
    "seed = 23\n",
    "set_random_seed(seed)\n",
    "\n",
    "# causal libraries\n",
    "import cdt, castle\n",
    "\n",
    "# causal metrics\n",
    "from cdt.metrics import precision_recall, SHD, SID\n",
    "from castle.metrics import MetricsDAG\n",
    "from castle.common import GraphDAG\n",
    "from causallearn.graph.SHD import SHD as SHD_causallearn\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Load the actual connectome data\n",
    "\n",
    "# %%\n",
    "# load the weighted adjacency matrices for ER and connectome\n",
    "\n",
    "# Specify the folder where the adjacency matrices were saved\n",
    "folder = '../data/'\n",
    "\n",
    "# Example usage to load the saved adjacency matrices\n",
    "G_A_init_t_ordered_adj_matrix = load_adjacency_matrix(os.path.join(folder, 'G_A_init_t_ordered_adj_matrix.npy'))\n",
    "G_A_init_t_ordered_dag_adj_matrix = load_adjacency_matrix(os.path.join(folder, 'G_A_init_t_ordered_dag_adj_matrix.npy'))\n",
    "ER = load_adjacency_matrix(os.path.join(folder, 'ER_adj_matrix.npy'))\n",
    "ER_dag = load_adjacency_matrix(os.path.join(folder, 'ER_dag_adj_matrix.npy'))\n",
    "\n",
    "# Change name of the connectome adjacency matrix to C and C_dag\n",
    "C = G_A_init_t_ordered_adj_matrix\n",
    "C_dag = G_A_init_t_ordered_dag_adj_matrix\n",
    "\n",
    "# Now ensure that both DAG adjacency matrices are binary, if they aren't already\n",
    "ER_dag_bin = (ER_dag != 0).astype(int)\n",
    "C_dag_bin = (C_dag != 0).astype(int)\n",
    "\n",
    "ER_true = ER_dag_bin\n",
    "C_true = C_dag_bin\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Create data to debug and implement the pcax version of NOTEARS\n",
    "\n",
    "# %%\n",
    "# actual data\n",
    "#B_true = simulate_dag(d=100, s0=400, graph_type='ER') # ER4\n",
    "# debugging data\n",
    "#B_true = simulate_dag(d=10, s0=20, graph_type='ER') # ER2\n",
    "\n",
    "\n",
    "#B_true = C_dag_bin # if you want to use the connectome-based DAG # best performance so far with 200,000 samples: 0.06 \n",
    "#B_true = ER_dag_bin # if you want to use the ER-based DAG\n",
    "\n",
    "#B_true = simulate_dag(d=5, s0=10, graph_type='ER') # ER2\n",
    "B_true = simulate_dag(d=10, s0=20, graph_type='ER') # ER2\n",
    "#B_true = simulate_dag(d=50, s0=100, graph_type='ER') # ER2\n",
    "#B_true = simulate_dag(d=100, s0=200, graph_type='ER') # ER2\n",
    "#B_true = simulate_dag(d=279, s0=558, graph_type='ER') # ER2\n",
    "\n",
    "# create equivalent ER4 and ER6 graphs\n",
    "#B_true = simulate_dag(d=279, s0=1116, graph_type='ER') # ER4\n",
    "#B_true = simulate_dag(d=279, s0=1674, graph_type='ER') # ER6\n",
    "\n",
    "# create equivalent SF4 and SF6 graphs\n",
    "#B_true = simulate_dag(d=100, s0=600, graph_type='SF') # SF6\n",
    "#B_true = simulate_dag(d=279, s0=1116, graph_type='SF') # SF4\n",
    "#B_true = simulate_dag(d=279, s0=1674, graph_type='SF') # SF6\n",
    "\n",
    "\n",
    "# create simple data using simulate_dag method from causal_helpers with expected number of edges (s0) and number of nodes (d)\n",
    "#B_true = simulate_dag(d=100, s0=199, graph_type='ER') # we use p≈0.040226 for the connectome-based ER_dag graph. This means that the expected number of edges is 0.040226 * d * (d-1) / 2\n",
    "# examples: d=50 -> s0=49 (works), d=100 -> s0=199, d=200 -> s0=800\n",
    "W_true = simulate_parameter(B_true)\n",
    "\n",
    "# sample data from the linear SEM\n",
    "# actual data\n",
    "#X = simulate_linear_sem(W_true, n=25000, sem_type='gauss')\n",
    "# for debugging\n",
    "X = simulate_linear_sem(W_true, n=1000, sem_type='gauss')\n",
    "#X = simulate_linear_sem(W_true, n=2500, sem_type='gauss')\n",
    "#X = simulate_linear_sem(W_true, n=6250, sem_type='gauss')\n",
    "#X = simulate_linear_sem(W_true, n=50000, sem_type='gauss')\n",
    "#X = simulate_linear_sem(W_true, n=100000, sem_type='gauss') # 1000*(279**2)/(20**2) = 194602\n",
    "\n",
    "# now standardized data, where each variable is normalized to unit variance\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_std = scaler.fit_transform(X)\n",
    "\n",
    "# NOTE: you may not write positional arguments after keyword arguments. \n",
    "# That is, the values that you are passing positionally have to come first!\n",
    "\n",
    "# create a dataset using the simulated data\n",
    "# NOTE: NOTEARS paper uses n=1000 for graph with d=20.\n",
    "# NOTE: d... number of nodes, p=d^2... number of parameters, n... number of samples. Then: comparing p1=d1^2 vs p2=d2^2 we have that: n1/p1 must be equal to n2/p2\n",
    "# Thus we have n2 = n1 * p2 / p1. For the case of d2=100 we have that n2 = (n1*p2)/p1 = 1000*(100^2)/(20^2) = 25000 \n",
    "# we should expect to use that many samples actually to be able to learn the graph in a comparable way.\n",
    "#dataset = IIDSimulation(W=W_true, n=25000, method='linear', sem_type='gauss')\n",
    "#true_dag, X = dataset.B, dataset.X\n",
    "\n",
    "# %%\n",
    "print(np.sum(B_true))\n",
    "\n",
    "# %%\n",
    "import pcax.utils as pxu\n",
    "print(dir(pxu))\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Implement the PCAX version of NOTEARS ##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# v1: single vode\n",
    "class Complete_Graph(pxc.EnergyModule):\n",
    "    def __init__(\n",
    "        self, \n",
    "        input_dim: int, \n",
    "        n_nodes: int, \n",
    "        hidden_dim: int = 3, \n",
    "        act_fn: Callable[[jax.Array], jax.Array] = jax.nn.relu,\n",
    "        has_bias: bool = False\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = px.static(input_dim)\n",
    "        self.n_nodes = px.static(n_nodes)\n",
    "        self.hidden_dim = px.static(hidden_dim)\n",
    "        self.has_bias = has_bias\n",
    "        self.act_fn = px.static(act_fn)\n",
    "\n",
    "        # Initialize MLPs for each connection (n_nodes x n_nodes matrix of MLPs)\n",
    "        self.mlp_layers = []\n",
    "        for i in range(n_nodes):\n",
    "            node_layers = []\n",
    "            for j in range(n_nodes):\n",
    "                # Create MLP: input_dim -> hidden_dim -> input_dim\n",
    "                mlp = [\n",
    "                    pxnn.Linear(input_dim, hidden_dim, bias=has_bias),\n",
    "                    pxnn.Linear(hidden_dim, input_dim, bias=has_bias)\n",
    "                ]\n",
    "                node_layers.append(mlp)\n",
    "\n",
    "            self.mlp_layers.append(node_layers)\n",
    "\n",
    "        # Initialize adjacency matrix as a LayerParam\n",
    "        init_weights = jnp.ones((n_nodes, n_nodes))\n",
    "        init_weights = jax.numpy.fill_diagonal(init_weights, 0.0, inplace=False)\n",
    "        self.adj_weights = pxnn.LayerParam(init_weights)\n",
    "\n",
    "        # Initialize vodes\n",
    "        self.vodes = [pxc.Vode((n_nodes, input_dim))]\n",
    "\n",
    "    def freeze_nodes(self, freeze=True):\n",
    "        self.vodes[0].h.frozen = freeze\n",
    "\n",
    "    def are_vodes_frozen(self):\n",
    "        return self.vodes[0].h.frozen\n",
    "    \n",
    "    def get_W(self):\n",
    "        \"\"\"Returns the weighted adjacency matrix.\"\"\"\n",
    "        return self.adj_weights.get()\n",
    "\n",
    "    def mlp_forward(self, x, i, j):\n",
    "\n",
    "        # print the shape of x\n",
    "        print(f\"The shape of x in mlp_forward: {x.shape}\")\n",
    "\n",
    "        \"\"\"Forward pass through MLP for connection i->j with nonlinear activation.\"\"\"\n",
    "        if i == j:  # Skip self-loops\n",
    "            return 0.0\n",
    "        \n",
    "        # First layer with nonlinear activation\n",
    "        h = self.act_fn(self.mlp_layers[i][j][0](x))\n",
    "        # Second layer\n",
    "        out = self.mlp_layers[i][j][1](h)\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def __call__(self, x=None):\n",
    "        n_nodes = self.n_nodes.get()\n",
    "        input_dim = self.input_dim.get()\n",
    "        \n",
    "        if x is not None:\n",
    "            # Initialize nodes with given data\n",
    "            reshaped_x = x.reshape(n_nodes, input_dim)\n",
    "            self.vodes[0](reshaped_x)\n",
    "        else:\n",
    "            # Get current node values\n",
    "            x_ = self.vodes[0].get('h')\n",
    "\n",
    "            # print the shape of x_ in __call__ else statement\n",
    "            print(f\"The shape of x_ in __call__ else statement: {x_.shape}\")\n",
    "            \n",
    "            # Compute weighted sum of MLP outputs for each node\n",
    "            outputs = []\n",
    "            for j in range(n_nodes):\n",
    "                node_output = 0\n",
    "                for i in range(n_nodes):\n",
    "                    # Apply MLP and weight by adjacency matrix entry\n",
    "                    mlp_out = self.mlp_forward(x_[i], i, j)\n",
    "                    node_output += self.adj_weights.get()[i, j] * mlp_out\n",
    "                outputs.append(node_output)\n",
    "            \n",
    "            # Stack outputs and update vodes\n",
    "            output = jnp.stack(outputs)\n",
    "            self.vodes[0](output) # TODO: do we need this line or should we remove it?\n",
    "\n",
    "        return self.vodes[0].get('h')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 0. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 0. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 0. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 0. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 0. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 0. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 0.]]\n",
      "\n",
      "(10, 10)\n",
      "(Complete_Graph):\n",
      "  .has_bias: True\n",
      "  .mlp_layers[0][0][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[0][0][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[0][0][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[0][0][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[0][1][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[0][1][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[0][1][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[0][1][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[0][2][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[0][2][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[0][2][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[0][2][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[0][3][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[0][3][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[0][3][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[0][3][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[0][4][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[0][4][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[0][4][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[0][4][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[0][5][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[0][5][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[0][5][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[0][5][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[0][6][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[0][6][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[0][6][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[0][6][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[0][7][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[0][7][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[0][7][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[0][7][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[0][8][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[0][8][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[0][8][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[0][8][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[0][9][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[0][9][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[0][9][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[0][9][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[1][0][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[1][0][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[1][0][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[1][0][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[1][1][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[1][1][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[1][1][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[1][1][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[1][2][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[1][2][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[1][2][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[1][2][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[1][3][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[1][3][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[1][3][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[1][3][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[1][4][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[1][4][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[1][4][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[1][4][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[1][5][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[1][5][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[1][5][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[1][5][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[1][6][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[1][6][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[1][6][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[1][6][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[1][7][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[1][7][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[1][7][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[1][7][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[1][8][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[1][8][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[1][8][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[1][8][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[1][9][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[1][9][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[1][9][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[1][9][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[2][0][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[2][0][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[2][0][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[2][0][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[2][1][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[2][1][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[2][1][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[2][1][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[2][2][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[2][2][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[2][2][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[2][2][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[2][3][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[2][3][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[2][3][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[2][3][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[2][4][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[2][4][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[2][4][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[2][4][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[2][5][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[2][5][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[2][5][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[2][5][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[2][6][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[2][6][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[2][6][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[2][6][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[2][7][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[2][7][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[2][7][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[2][7][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[2][8][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[2][8][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[2][8][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[2][8][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[2][9][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[2][9][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[2][9][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[2][9][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[3][0][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[3][0][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[3][0][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[3][0][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[3][1][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[3][1][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[3][1][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[3][1][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[3][2][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[3][2][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[3][2][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[3][2][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[3][3][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[3][3][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[3][3][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[3][3][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[3][4][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[3][4][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[3][4][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[3][4][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[3][5][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[3][5][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[3][5][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[3][5][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[3][6][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[3][6][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[3][6][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[3][6][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[3][7][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[3][7][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[3][7][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[3][7][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[3][8][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[3][8][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[3][8][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[3][8][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[3][9][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[3][9][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[3][9][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[3][9][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[4][0][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[4][0][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[4][0][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[4][0][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[4][1][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[4][1][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[4][1][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[4][1][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[4][2][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[4][2][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[4][2][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[4][2][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[4][3][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[4][3][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[4][3][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[4][3][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[4][4][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[4][4][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[4][4][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[4][4][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[4][5][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[4][5][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[4][5][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[4][5][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[4][6][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[4][6][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[4][6][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[4][6][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[4][7][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[4][7][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[4][7][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[4][7][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[4][8][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[4][8][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[4][8][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[4][8][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[4][9][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[4][9][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[4][9][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[4][9][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[5][0][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[5][0][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[5][0][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[5][0][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[5][1][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[5][1][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[5][1][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[5][1][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[5][2][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[5][2][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[5][2][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[5][2][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[5][3][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[5][3][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[5][3][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[5][3][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[5][4][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[5][4][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[5][4][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[5][4][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[5][5][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[5][5][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[5][5][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[5][5][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[5][6][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[5][6][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[5][6][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[5][6][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[5][7][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[5][7][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[5][7][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[5][7][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[5][8][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[5][8][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[5][8][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[5][8][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[5][9][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[5][9][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[5][9][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[5][9][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[6][0][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[6][0][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[6][0][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[6][0][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[6][1][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[6][1][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[6][1][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[6][1][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[6][2][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[6][2][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[6][2][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[6][2][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[6][3][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[6][3][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[6][3][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[6][3][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[6][4][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[6][4][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[6][4][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[6][4][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[6][5][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[6][5][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[6][5][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[6][5][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[6][6][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[6][6][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[6][6][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[6][6][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[6][7][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[6][7][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[6][7][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[6][7][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[6][8][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[6][8][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[6][8][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[6][8][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[6][9][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[6][9][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[6][9][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[6][9][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[7][0][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[7][0][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[7][0][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[7][0][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[7][1][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[7][1][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[7][1][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[7][1][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[7][2][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[7][2][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[7][2][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[7][2][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[7][3][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[7][3][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[7][3][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[7][3][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[7][4][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[7][4][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[7][4][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[7][4][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[7][5][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[7][5][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[7][5][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[7][5][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[7][6][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[7][6][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[7][6][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[7][6][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[7][7][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[7][7][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[7][7][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[7][7][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[7][8][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[7][8][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[7][8][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[7][8][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[7][9][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[7][9][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[7][9][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[7][9][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[8][0][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[8][0][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[8][0][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[8][0][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[8][1][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[8][1][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[8][1][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[8][1][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[8][2][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[8][2][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[8][2][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[8][2][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[8][3][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[8][3][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[8][3][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[8][3][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[8][4][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[8][4][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[8][4][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[8][4][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[8][5][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[8][5][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[8][5][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[8][5][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[8][6][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[8][6][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[8][6][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[8][6][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[8][7][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[8][7][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[8][7][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[8][7][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[8][8][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[8][8][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[8][8][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[8][8][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[8][9][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[8][9][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[8][9][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[8][9][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[9][0][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[9][0][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[9][0][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[9][0][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[9][1][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[9][1][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[9][1][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[9][1][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[9][2][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[9][2][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[9][2][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[9][2][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[9][3][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[9][3][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[9][3][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[9][3][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[9][4][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[9][4][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[9][4][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[9][4][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[9][5][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[9][5][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[9][5][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[9][5][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[9][6][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[9][6][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[9][6][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[9][6][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[9][7][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[9][7][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[9][7][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[9][7][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[9][8][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[9][8][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[9][8][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[9][8][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[9][9][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[9][9][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[9][9][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[9][9][1].nn.bias: LayerParam([1], float32)\n",
      "  .adj_weights: LayerParam([10,10], float32)\n",
      "  .vodes[0].h: VodeParam(None)\n",
      "  .vodes[0].cache: Cache(params=None)\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Usage\n",
    "input_dim = 1\n",
    "n_nodes = X.shape[1]\n",
    "#model = Complete_Graph(input_dim, n_nodes, has_bias=False)\n",
    "model = Complete_Graph(input_dim, n_nodes, has_bias=True, act_fn=jax.nn.leaky_relu)\n",
    "\n",
    "# Get weighted adjacency matrix\n",
    "W = model.get_W()\n",
    "print(W)\n",
    "print()\n",
    "print(W.shape)\n",
    "print(model)\n",
    "\n",
    "\n",
    "# Freezing all nodes\n",
    "model.freeze_nodes(freeze=True)\n",
    "\n",
    "# Check if all nodes are frozen\n",
    "print(model.are_vodes_frozen())\n",
    "\n",
    "# Unfreezing all nodes\n",
    "model.freeze_nodes(freeze=False)\n",
    "\n",
    "# Check if all nodes are frozen\n",
    "print(model.are_vodes_frozen())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "# TODO: make the below params global or input to the functions in which it is used.\n",
    "w_learning_rate = 5e-2 # Notes: 5e-1 is too high\n",
    "h_learning_rate = 5e-4\n",
    "T = 1\n",
    "\n",
    "nm_epochs = 1000\n",
    "batch_size = 128\n",
    "\n",
    "lam_h = 1e1 # 2e2 -> 5e2 # this move works well! FIRST MOVE\n",
    "lam_l1 = 1e-3 # 1e-2 -> 3e-2 # this move works well! SECOND MOVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE between the true adjacency matrix and an all-zero matrix:  0.27927107\n",
      "SHD between the true adjacency matrix and an all-zero matrix:  20.0\n",
      "Forward: Starting\n",
      "Forward: Completed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# %%\n",
    "# Training and evaluation functions\n",
    "@pxf.vmap(pxu.Mask(pxc.VodeParam | pxc.VodeParam.Cache, (None, 0)), in_axes=(0,), out_axes=0)\n",
    "def forward(x, *, model: Complete_Graph):\n",
    "    print(\"Forward: Starting\")\n",
    "    result = model(x)\n",
    "    print(\"Forward: Completed\")\n",
    "    return result\n",
    "\n",
    "@pxf.vmap(pxu.Mask(pxc.VodeParam | pxc.VodeParam.Cache, (None, 0)), out_axes=(None, 0), axis_name=\"batch\")\n",
    "def energy(*, model: Complete_Graph):\n",
    "    print(\"Energy: Starting computation\")\n",
    "    x_ = model(None)\n",
    "    print(\"Energy: Got model output\")\n",
    "    \n",
    "    W = model.get_W()\n",
    "    d = model.n_nodes.get()\n",
    "    print(f\"Energy: Got W (shape: {W.shape}) and d: {d}\")\n",
    "\n",
    "    # PC energy term\n",
    "    energy = model.energy()\n",
    "    print(f\"Energy: PC energy term: {energy}\")\n",
    "\n",
    "    # L1 regularization using adjacency matrix\n",
    "    l1_reg = jnp.sum(jnp.abs(W))\n",
    "    print(f\"Energy: L1 reg term: {l1_reg}\")\n",
    "\n",
    "    # DAG constraint\n",
    "    h_reg = jnp.trace(jax.scipy.linalg.expm(jnp.multiply(W, W))) - d\n",
    "    print(f\"Energy: DAG constraint term: {h_reg}\")\n",
    "    \n",
    "    # Combined loss\n",
    "    obj = jax.lax.pmean(energy, axis_name=\"batch\") + lam_h * h_reg + lam_l1 * l1_reg\n",
    "    print(f\"Energy: Final objective: {obj}\")\n",
    "\n",
    "    return obj, x_\n",
    "\n",
    "@pxf.jit(static_argnums=0)\n",
    "def train_on_batch(T: int, x: jax.Array, *, model: Complete_Graph, optim_w: pxu.Optim, optim_h: pxu.Optim):\n",
    "    print(\"1. Starting train_on_batch\")  \n",
    "\n",
    "    model.train()\n",
    "    print(\"2. Model set to train mode\")\n",
    "\n",
    "    model.freeze_nodes(freeze=True)\n",
    "    print(\"3. Nodes frozen\")\n",
    "\n",
    "    # init step\n",
    "    with pxu.step(model, pxc.STATUS.INIT, clear_params=pxc.VodeParam.Cache):\n",
    "        print(\"4. Doing forward for initialization\")\n",
    "        forward(x, model=model)\n",
    "        print(\"5. After forward for initialization\")\n",
    "        \n",
    "    with pxu.step(model, clear_params=pxc.VodeParam.Cache):\n",
    "        print(\"6. Before computing gradients\")\n",
    "        _, g = pxf.value_and_grad(pxu.Mask(pxnn.LayerParam, [False, True]), has_aux=True)(energy)(model=model)\n",
    "        print(\"7. After computing gradients\")\n",
    "        print(\"Gradient structure:\", g)\n",
    "\n",
    "        print(\"8. Before zeroing out the diagonal gradients\")\n",
    "        # Zero out the diagonal gradients using jax.numpy.fill_diagonal\n",
    "        weight_grads = g[\"model\"].adj_weights.get()\n",
    "        weight_grads = jax.numpy.fill_diagonal(weight_grads, 0.0, inplace=False)\n",
    "        g[\"model\"].adj_weights.set(weight_grads)\n",
    "        print(\"9. After zeroing out the diagonal gradients\")\n",
    "\n",
    "        \n",
    "    print(\"10. Before optimizer step\")\n",
    "    #optim_w.step(model, g[\"model\"])\n",
    "    optim_w.step(model, g[\"model\"], scale_by=1.0/x.shape[0])\n",
    "    print(\"11. After optimizer step\")\n",
    "\n",
    "    with pxu.step(model, clear_params=pxc.VodeParam.Cache):\n",
    "        print(\"12. Before final forward\")\n",
    "        forward(None, model=model)\n",
    "        e_avg_per_sample = model.energy()\n",
    "        print(\"13. After final forward\")\n",
    "\n",
    "    model.freeze_nodes(freeze=False)\n",
    "    print(\"14. Nodes unfrozen\")\n",
    "\n",
    "    return e_avg_per_sample\n",
    "\n",
    "def train(dl, T, *, model: Complete_Graph, optim_w: pxu.Optim, optim_h: pxu.Optim):\n",
    "    e_avg_per_sample_energies = []\n",
    "    for batch in dl:\n",
    "\n",
    "        e_avg_per_sample = train_on_batch(T, batch, model=model, optim_w=optim_w, optim_h=optim_h)\n",
    "        e_avg_per_sample_energies.append(e_avg_per_sample)\n",
    "\n",
    "    W = model.get_W()\n",
    "\n",
    "    # compute epoch energy\n",
    "    epoch_energy = jnp.mean(jnp.array(e_avg_per_sample_energies))\n",
    "    return W, epoch_energy\n",
    "\n",
    "# %%\n",
    "@jit\n",
    "def MAE(W_true, W):\n",
    "    \"\"\"This function returns the Mean Absolute Error for the difference between the true weighted adjacency matrix W_true and th estimated one, W.\"\"\"\n",
    "    MAE_ = jnp.mean(jnp.abs(W - W_true))\n",
    "    return MAE_\n",
    "\n",
    "def compute_binary_adjacency(W, threshold=0.3):\n",
    "    \"\"\"\n",
    "    Compute the binary adjacency matrix by thresholding the input matrix.\n",
    "\n",
    "    Args:\n",
    "    - W (array-like): The weighted adjacency matrix (can be a JAX array or a NumPy array).\n",
    "    - threshold (float): The threshold value to determine the binary matrix. Default is 0.3.\n",
    "\n",
    "    Returns:\n",
    "    - B_est (np.ndarray): The binary adjacency matrix where each element is True if the corresponding \n",
    "                          element in W is greater than the threshold, otherwise False.\n",
    "    \"\"\"\n",
    "    # Convert JAX array to NumPy array if necessary\n",
    "    if isinstance(W, jnp.ndarray):\n",
    "        W = np.array(W)\n",
    "\n",
    "    # Compute the binary adjacency matrix\n",
    "    B_est = np.array(np.abs(W) > threshold)\n",
    "    \n",
    "    return B_est\n",
    "\n",
    "\n",
    "def ensure_DAG(W):\n",
    "    \"\"\"\n",
    "    Ensure that the weighted adjacency matrix corresponds to a DAG.\n",
    "\n",
    "    Inputs:\n",
    "        W: numpy.ndarray - a weighted adjacency matrix representing a directed graph\n",
    "\n",
    "    Outputs:\n",
    "        W: numpy.ndarray - a weighted adjacency matrix without cycles (DAG)\n",
    "    \"\"\"\n",
    "    # Convert the adjacency matrix to a directed graph\n",
    "    g = nx.DiGraph(W)\n",
    "\n",
    "    # Make a copy of the graph to modify\n",
    "    gg = g.copy()\n",
    "\n",
    "    # Remove cycles by removing edges\n",
    "    while not nx.is_directed_acyclic_graph(gg):\n",
    "        h = gg.copy()\n",
    "\n",
    "        # Remove all the sources and sinks\n",
    "        while True:\n",
    "            finished = True\n",
    "\n",
    "            for node, in_degree in nx.in_degree_centrality(h).items():\n",
    "                if in_degree == 0:\n",
    "                    h.remove_node(node)\n",
    "                    finished = False\n",
    "\n",
    "            for node, out_degree in nx.out_degree_centrality(h).items():\n",
    "                if out_degree == 0:\n",
    "                    h.remove_node(node)\n",
    "                    finished = False\n",
    "\n",
    "            if finished:\n",
    "                break\n",
    "\n",
    "        # Find a cycle with a random walk starting at a random node\n",
    "        node = list(h.nodes)[0]\n",
    "        cycle = [node]\n",
    "        while True:\n",
    "            edges = list(h.out_edges(node))\n",
    "            _, node = edges[np.random.choice(len(edges))]\n",
    "\n",
    "            if node in cycle:\n",
    "                break\n",
    "\n",
    "            cycle.append(node)\n",
    "\n",
    "        # Extract the cycle path and adjust it to start at the first occurrence of the repeated node\n",
    "        cycle = np.array(cycle)\n",
    "        i = np.argwhere(cycle == node)[0][0]\n",
    "        cycle = cycle[i:]\n",
    "        cycle = cycle.tolist() + [node]\n",
    "\n",
    "        # Find edges in that cycle\n",
    "        edges = list(zip(cycle[:-1], cycle[1:]))\n",
    "\n",
    "        # Randomly pick an edge to remove\n",
    "        edge = edges[np.random.choice(len(edges))]\n",
    "        gg.remove_edge(*edge)\n",
    "\n",
    "    # Convert the modified graph back to a weighted adjacency matrix\n",
    "    W_acyclic = nx.to_numpy_array(gg)\n",
    "\n",
    "    return W_acyclic\n",
    "\n",
    "# %%\n",
    "# for reference compute the MAE, SID, and SHD between the true adjacency matrix and an all-zero matrix and then print it\n",
    "# this acts as a baseline for the MAE, SID, and SHD similar to how 1/K accuracy acts as a baseline for classification tasks where K is the number of classes\n",
    "W_zero = np.zeros_like(W_true)\n",
    "print(\"MAE between the true adjacency matrix and an all-zero matrix: \", MAE(W_true, W_zero))\n",
    "print(\"SHD between the true adjacency matrix and an all-zero matrix: \", SHD(B_true, compute_binary_adjacency(W_zero)))\n",
    "#print(\"SID between the true adjacency matrix and an all-zero matrix: \", SID(W_true, W_zero))\n",
    "\n",
    "# %%\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X):\n",
    "        self.X = X\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx]\n",
    "\n",
    "# This is a simple collate function that stacks numpy arrays used to interface\n",
    "# the PyTorch dataloader with JAX. In the future we hope to provide custom dataloaders\n",
    "# that are independent of PyTorch.\n",
    "def numpy_collate(batch):\n",
    "    if isinstance(batch[0], np.ndarray):\n",
    "        return np.stack(batch)\n",
    "    elif isinstance(batch[0], (tuple, list)):\n",
    "        transposed = zip(*batch)\n",
    "        return [numpy_collate(samples) for samples in transposed]\n",
    "    else:\n",
    "        return np.array(batch)\n",
    "\n",
    "# The dataloader assumes cuda is being used, as such it sets 'pin_memory = True' and\n",
    "# 'prefetch_factor = 2'. Note that the batch size should be constant during training, so\n",
    "# we set 'drop_last = True' to avoid having to deal with variable batch sizes.\n",
    "class TorchDataloader(torch.utils.data.DataLoader):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset,\n",
    "        batch_size=1,\n",
    "        shuffle=None,\n",
    "        sampler=None,\n",
    "        batch_sampler=None,\n",
    "        num_workers=1,\n",
    "        pin_memory=True,\n",
    "        timeout=0,\n",
    "        worker_init_fn=None,\n",
    "        persistent_workers=True,\n",
    "        prefetch_factor=2,\n",
    "    ):\n",
    "        super(self.__class__, self).__init__(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            sampler=sampler,\n",
    "            batch_sampler=batch_sampler,\n",
    "            num_workers=num_workers,\n",
    "            collate_fn=numpy_collate,\n",
    "            pin_memory=pin_memory,\n",
    "            drop_last=True if batch_sampler is None else None,\n",
    "            timeout=timeout,\n",
    "            worker_init_fn=worker_init_fn,\n",
    "            persistent_workers=persistent_workers,\n",
    "            prefetch_factor=prefetch_factor,\n",
    "        )\n",
    "\n",
    "\n",
    "# Create the custom dataset\n",
    "dataset = CustomDataset(X)\n",
    "# Create the custom dataset with standardized data\n",
    "dataset_std = CustomDataset(X_std)\n",
    "\n",
    "# Create the dataloader\n",
    "dl = TorchDataloader(dataset, batch_size=batch_size, shuffle=True)\n",
    "######## OR ########\n",
    "#dl = TorchDataloader(dataset_std, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# %%\n",
    "# Initialize the model and optimizers\n",
    "with pxu.step(model, pxc.STATUS.INIT, clear_params=pxc.VodeParam.Cache):\n",
    "    forward(jnp.zeros((batch_size, model.n_nodes.get())), model=model)\n",
    "    optim_h = pxu.Optim(optax.sgd(h_learning_rate), pxu.Mask(pxc.VodeParam)(model))\n",
    "    #optim_w = pxu.Optim(optax.sgd(w_learning_rate), pxu.Mask(pxnn.LayerParam)(model))\n",
    "\n",
    "    \"\"\"\n",
    "    optim_w = pxu.Optim(\n",
    "    optax.chain(\n",
    "        optax.clip_by_global_norm(clip_value),  # Clip gradients by global norm\n",
    "        optax.sgd(w_learning_rate)  # Apply SGD optimizer\n",
    "    ),\n",
    "    pxu.Mask(pxnn.LayerParam)(model)  # Masking the parameters of the model\n",
    ")\n",
    "    \"\"\"\n",
    "    #optim_w = pxu.Optim(optax.adafactor(w_learning_rate), pxu.Mask(pxnn.LayerParam)(model))\n",
    "    #optim_w = pxu.Optim(optax.sgd(w_learning_rate, momentum=0.95), pxu.Mask(pxnn.LayerParam)(model))\n",
    "    #optim_w = pxu.Optim(optax.adamw(w_learning_rate, weight_decay=5e-2), pxu.Mask(pxnn.LayerParam)(model))\n",
    "    #optim_w = pxu.Optim(optax.adamw(w_learning_rate, nesterov=True), pxu.Mask(pxnn.LayerParam)(model))\n",
    "    #optim_w = pxu.Optim(optax.adamw(w_learning_rate, nesterov=False), pxu.Mask(pxnn.LayerParam)(model))\n",
    "    optim_w = pxu.Optim(optax.adam(w_learning_rate), pxu.Mask(pxnn.LayerParam)(model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start difference (cont.) between W_true and W_init: 0.9835\n",
      "Start SHD between B_true and B_init: 70.0000\n",
      "The diagonal of the initial W:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Starting train_on_batch\n",
      "2. Model set to train mode\n",
      "3. Nodes frozen\n",
      "4. Doing forward for initialization\n",
      "Forward: Starting\n",
      "Forward: Completed\n",
      "5. After forward for initialization\n",
      "6. Before computing gradients\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Energy: Starting computation\n",
      "The shape of x_ in __call__ else statement: (10, 1)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "The shape of x in mlp_forward: (1,)\n",
      "Energy: Got model output\n",
      "Energy: Got W (shape: (10, 10)) and d: 10\n",
      "Energy: PC energy term: Traced<ShapedArray(float32[])>with<BatchTrace(level=4/0)> with\n",
      "  val = Traced<ShapedArray(float32[128])>with<JVPTrace(level=3/0)> with\n",
      "    primal = Traced<ShapedArray(float32[128])>with<DynamicJaxprTrace(level=1/0)>\n",
      "    tangent = Traced<ShapedArray(float32[128])>with<JaxprTrace(level=2/0)> with\n",
      "      pval = (ShapedArray(float32[128]), None)\n",
      "      recipe = JaxprEqnRecipe(eqn_id=<object object at 0x7f35ddd8f4a0>, in_tracers=(Traced<ShapedArray(float32[128,10,1]):JaxprTrace(level=2/0)>,), out_tracer_refs=[<weakref at 0x7f35dddfcd10; to 'JaxprTracer' at 0x7f35dddfce00>], out_avals=[ShapedArray(float32[128])], primitive=pjit, params={'jaxpr': { \u001b[34m\u001b[22m\u001b[1mlambda \u001b[39m\u001b[22m\u001b[22m; a\u001b[35m:f32[128,10,1]\u001b[39m. \u001b[34m\u001b[22m\u001b[1mlet\u001b[39m\u001b[22m\u001b[22m b\u001b[35m:f32[128]\u001b[39m = reduce_sum[axes=(1, 2)] a \u001b[34m\u001b[22m\u001b[1min \u001b[39m\u001b[22m\u001b[22m(b,) }, 'in_shardings': (UnspecifiedValue,), 'out_shardings': (UnspecifiedValue,), 'in_layouts': (None,), 'out_layouts': (None,), 'resource_env': None, 'donated_invars': (False,), 'name': '_reduce_sum', 'keep_unused': False, 'inline': True}, effects=set(), source_info=<jax._src.source_info_util.SourceInfo object at 0x7f35dddfb550>, ctx=JaxprEqnContext(compute_type=None,threefry_partitionable=False))\n",
      "  batch_dim = 0\n",
      "Energy: L1 reg term: Traced<ShapedArray(float32[])>with<JVPTrace(level=3/0)> with\n",
      "  primal = Traced<ShapedArray(float32[])>with<DynamicJaxprTrace(level=1/0)>\n",
      "  tangent = Traced<ShapedArray(float32[])>with<JaxprTrace(level=2/0)> with\n",
      "    pval = (ShapedArray(float32[]), None)\n",
      "    recipe = JaxprEqnRecipe(eqn_id=<object object at 0x7f35ddd8f980>, in_tracers=(Traced<ShapedArray(float32[10,10]):JaxprTrace(level=2/0)>,), out_tracer_refs=[<weakref at 0x7f35dddfd8a0; to 'JaxprTracer' at 0x7f35dddfd990>], out_avals=[ShapedArray(float32[])], primitive=pjit, params={'jaxpr': { \u001b[34m\u001b[22m\u001b[1mlambda \u001b[39m\u001b[22m\u001b[22m; a\u001b[35m:f32[10,10]\u001b[39m. \u001b[34m\u001b[22m\u001b[1mlet\u001b[39m\u001b[22m\u001b[22m b\u001b[35m:f32[]\u001b[39m = reduce_sum[axes=(0, 1)] a \u001b[34m\u001b[22m\u001b[1min \u001b[39m\u001b[22m\u001b[22m(b,) }, 'in_shardings': (UnspecifiedValue,), 'out_shardings': (UnspecifiedValue,), 'in_layouts': (None,), 'out_layouts': (None,), 'resource_env': None, 'donated_invars': (False,), 'name': '_reduce_sum', 'keep_unused': False, 'inline': True}, effects=set(), source_info=<jax._src.source_info_util.SourceInfo object at 0x7f35dde111b0>, ctx=JaxprEqnContext(compute_type=None,threefry_partitionable=False))\n",
      "Energy: DAG constraint term: Traced<ShapedArray(float32[])>with<JVPTrace(level=3/0)> with\n",
      "  primal = Traced<ShapedArray(float32[])>with<DynamicJaxprTrace(level=1/0)>\n",
      "  tangent = Traced<ShapedArray(float32[])>with<JaxprTrace(level=2/0)> with\n",
      "    pval = (ShapedArray(float32[]), None)\n",
      "    recipe = JaxprEqnRecipe(eqn_id=<object object at 0x7f35ddbf2ee0>, in_tracers=(Traced<ShapedArray(float32[]):JaxprTrace(level=2/0)>,), out_tracer_refs=[<weakref at 0x7f35ddad4400; to 'JaxprTracer' at 0x7f35ddad43b0>], out_avals=[ShapedArray(float32[])], primitive=pjit, params={'jaxpr': { \u001b[34m\u001b[22m\u001b[1mlambda \u001b[39m\u001b[22m\u001b[22m; a\u001b[35m:f32[]\u001b[39m. \u001b[34m\u001b[22m\u001b[1mlet\u001b[39m\u001b[22m\u001b[22m  \u001b[34m\u001b[22m\u001b[1min \u001b[39m\u001b[22m\u001b[22m(a,) }, 'in_shardings': (UnspecifiedValue,), 'out_shardings': (UnspecifiedValue,), 'in_layouts': (None,), 'out_layouts': (None,), 'resource_env': None, 'donated_invars': (False,), 'name': 'subtract', 'keep_unused': False, 'inline': True}, effects=set(), source_info=<jax._src.source_info_util.SourceInfo object at 0x7f35ddac1f60>, ctx=JaxprEqnContext(compute_type=None,threefry_partitionable=False))\n",
      "Energy: Final objective: Traced<ShapedArray(float32[])>with<JVPTrace(level=3/0)> with\n",
      "  primal = Traced<ShapedArray(float32[])>with<DynamicJaxprTrace(level=1/0)>\n",
      "  tangent = Traced<ShapedArray(float32[])>with<JaxprTrace(level=2/0)> with\n",
      "    pval = (ShapedArray(float32[]), None)\n",
      "    recipe = JaxprEqnRecipe(eqn_id=<object object at 0x7f35ddbf3580>, in_tracers=(Traced<ShapedArray(float32[]):JaxprTrace(level=2/0)>, Traced<ShapedArray(float32[]):JaxprTrace(level=2/0)>), out_tracer_refs=[<weakref at 0x7f35ddad58a0; to 'JaxprTracer' at 0x7f35ddad5850>], out_avals=[ShapedArray(float32[])], primitive=pjit, params={'jaxpr': { \u001b[34m\u001b[22m\u001b[1mlambda \u001b[39m\u001b[22m\u001b[22m; a\u001b[35m:f32[]\u001b[39m b\u001b[35m:f32[]\u001b[39m. \u001b[34m\u001b[22m\u001b[1mlet\u001b[39m\u001b[22m\u001b[22m c\u001b[35m:f32[]\u001b[39m = add a b \u001b[34m\u001b[22m\u001b[1min \u001b[39m\u001b[22m\u001b[22m(c,) }, 'in_shardings': (UnspecifiedValue, UnspecifiedValue), 'out_shardings': (UnspecifiedValue,), 'in_layouts': (None, None), 'out_layouts': (None,), 'resource_env': None, 'donated_invars': (False, False), 'name': 'add', 'keep_unused': False, 'inline': True}, effects=set(), source_info=<jax._src.source_info_util.SourceInfo object at 0x7f35ddae5450>, ctx=JaxprEqnContext(compute_type=None,threefry_partitionable=False))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. After computing gradients\n",
      "Gradient structure: {'__RKG': (RandomKeyGenerator):, 'model': (Complete_Graph):\n",
      "  .mlp_layers[0][0][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[0][0][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[0][0][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[0][0][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[0][1][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[0][1][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[0][1][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[0][1][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[0][2][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[0][2][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[0][2][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[0][2][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[0][3][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[0][3][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[0][3][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[0][3][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[0][4][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[0][4][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[0][4][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[0][4][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[0][5][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[0][5][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[0][5][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[0][5][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[0][6][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[0][6][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[0][6][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[0][6][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[0][7][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[0][7][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[0][7][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[0][7][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[0][8][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[0][8][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[0][8][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[0][8][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[0][9][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[0][9][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[0][9][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[0][9][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[1][0][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[1][0][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[1][0][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[1][0][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[1][1][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[1][1][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[1][1][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[1][1][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[1][2][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[1][2][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[1][2][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[1][2][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[1][3][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[1][3][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[1][3][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[1][3][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[1][4][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[1][4][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[1][4][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[1][4][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[1][5][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[1][5][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[1][5][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[1][5][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[1][6][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[1][6][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[1][6][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[1][6][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[1][7][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[1][7][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[1][7][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[1][7][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[1][8][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[1][8][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[1][8][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[1][8][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[1][9][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[1][9][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[1][9][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[1][9][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[2][0][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[2][0][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[2][0][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[2][0][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[2][1][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[2][1][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[2][1][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[2][1][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[2][2][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[2][2][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[2][2][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[2][2][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[2][3][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[2][3][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[2][3][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[2][3][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[2][4][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[2][4][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[2][4][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[2][4][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[2][5][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[2][5][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[2][5][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[2][5][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[2][6][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[2][6][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[2][6][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[2][6][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[2][7][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[2][7][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[2][7][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[2][7][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[2][8][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[2][8][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[2][8][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[2][8][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[2][9][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[2][9][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[2][9][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[2][9][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[3][0][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[3][0][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[3][0][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[3][0][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[3][1][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[3][1][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[3][1][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[3][1][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[3][2][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[3][2][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[3][2][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[3][2][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[3][3][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[3][3][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[3][3][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[3][3][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[3][4][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[3][4][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[3][4][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[3][4][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[3][5][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[3][5][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[3][5][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[3][5][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[3][6][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[3][6][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[3][6][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[3][6][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[3][7][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[3][7][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[3][7][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[3][7][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[3][8][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[3][8][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[3][8][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[3][8][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[3][9][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[3][9][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[3][9][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[3][9][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[4][0][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[4][0][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[4][0][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[4][0][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[4][1][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[4][1][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[4][1][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[4][1][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[4][2][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[4][2][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[4][2][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[4][2][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[4][3][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[4][3][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[4][3][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[4][3][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[4][4][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[4][4][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[4][4][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[4][4][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[4][5][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[4][5][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[4][5][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[4][5][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[4][6][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[4][6][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[4][6][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[4][6][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[4][7][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[4][7][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[4][7][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[4][7][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[4][8][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[4][8][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[4][8][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[4][8][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[4][9][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[4][9][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[4][9][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[4][9][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[5][0][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[5][0][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[5][0][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[5][0][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[5][1][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[5][1][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[5][1][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[5][1][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[5][2][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[5][2][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[5][2][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[5][2][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[5][3][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[5][3][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[5][3][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[5][3][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[5][4][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[5][4][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[5][4][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[5][4][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[5][5][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[5][5][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[5][5][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[5][5][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[5][6][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[5][6][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[5][6][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[5][6][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[5][7][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[5][7][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[5][7][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[5][7][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[5][8][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[5][8][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[5][8][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[5][8][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[5][9][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[5][9][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[5][9][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[5][9][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[6][0][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[6][0][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[6][0][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[6][0][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[6][1][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[6][1][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[6][1][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[6][1][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[6][2][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[6][2][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[6][2][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[6][2][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[6][3][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[6][3][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[6][3][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[6][3][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[6][4][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[6][4][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[6][4][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[6][4][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[6][5][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[6][5][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[6][5][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[6][5][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[6][6][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[6][6][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[6][6][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[6][6][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[6][7][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[6][7][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[6][7][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[6][7][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[6][8][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[6][8][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[6][8][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[6][8][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[6][9][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[6][9][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[6][9][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[6][9][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[7][0][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[7][0][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[7][0][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[7][0][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[7][1][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[7][1][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[7][1][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[7][1][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[7][2][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[7][2][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[7][2][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[7][2][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[7][3][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[7][3][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[7][3][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[7][3][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[7][4][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[7][4][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[7][4][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[7][4][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[7][5][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[7][5][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[7][5][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[7][5][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[7][6][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[7][6][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[7][6][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[7][6][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[7][7][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[7][7][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[7][7][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[7][7][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[7][8][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[7][8][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[7][8][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[7][8][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[7][9][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[7][9][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[7][9][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[7][9][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[8][0][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[8][0][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[8][0][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[8][0][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[8][1][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[8][1][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[8][1][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[8][1][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[8][2][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[8][2][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[8][2][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[8][2][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[8][3][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[8][3][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[8][3][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[8][3][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[8][4][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[8][4][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[8][4][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[8][4][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[8][5][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[8][5][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[8][5][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[8][5][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[8][6][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[8][6][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[8][6][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[8][6][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[8][7][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[8][7][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[8][7][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[8][7][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[8][8][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[8][8][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[8][8][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[8][8][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[8][9][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[8][9][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[8][9][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[8][9][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[9][0][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[9][0][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[9][0][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[9][0][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[9][1][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[9][1][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[9][1][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[9][1][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[9][2][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[9][2][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[9][2][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[9][2][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[9][3][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[9][3][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[9][3][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[9][3][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[9][4][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[9][4][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[9][4][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[9][4][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[9][5][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[9][5][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[9][5][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[9][5][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[9][6][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[9][6][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[9][6][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[9][6][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[9][7][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[9][7][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[9][7][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[9][7][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[9][8][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[9][8][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[9][8][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[9][8][1].nn.bias: LayerParam([1], float32)\n",
      "  .mlp_layers[9][9][0].nn.weight: LayerParam([3,1], float32)\n",
      "  .mlp_layers[9][9][0].nn.bias: LayerParam([3], float32)\n",
      "  .mlp_layers[9][9][1].nn.weight: LayerParam([1,3], float32)\n",
      "  .mlp_layers[9][9][1].nn.bias: LayerParam([1], float32)\n",
      "  .adj_weights: LayerParam([10,10], float32)}\n",
      "8. Before zeroing out the diagonal gradients\n",
      "9. After zeroing out the diagonal gradients\n",
      "10. Before optimizer step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Optim.step() got an unexpected keyword argument 'scale_by'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(nm_epochs), position\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[1;32m     22\u001b[0m         \u001b[38;5;66;03m# Train for one epoch using the dataloader\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m         W, epoch_energy \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptim_w\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptim_w\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptim_h\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptim_h\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;66;03m# Calculate the metrics and store them\u001b[39;00m\n\u001b[1;32m     26\u001b[0m         W \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(W)\n",
      "Cell \u001b[0;32mIn[5], line 88\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dl, T, model, optim_w, optim_h)\u001b[0m\n\u001b[1;32m     85\u001b[0m e_avg_per_sample_energies \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m dl:\n\u001b[0;32m---> 88\u001b[0m     e_avg_per_sample \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_on_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptim_w\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptim_w\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptim_h\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptim_h\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m     e_avg_per_sample_energies\u001b[38;5;241m.\u001b[39mappend(e_avg_per_sample)\n\u001b[1;32m     91\u001b[0m W \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_W()\n",
      "File \u001b[0;32m~/pcax/pcax/functional/_transform.py:175\u001b[0m, in \u001b[0;36m_BaseTransform.__call__\u001b[0;34m(self, _is_root, *args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_root \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    174\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m tree_ref(kwargs)\n\u001b[0;32m--> 175\u001b[0m _r, _kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_t\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;66;03m# This is the key part: the updated values are injected back into the original parameters in 'kwargs'. 'kwargs'\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;66;03m# is still the original structure as it hasn't undergone any transformation (which happens only inside '_t').\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;66;03m# The update values are obtained by calling 'extract', which is done automatically by the wrapped 'fn'.\u001b[39;00m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;66;03m# If 'tree_extract' is called before returning '_kwargs', a list of value was returned, so we tell 'tree_inject'\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;66;03m# to handle it correctly.\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(kwargs, \u001b[38;5;28mdict\u001b[39m):\n",
      "File \u001b[0;32m~/pcax/pcax/functional/_transform.py:260\u001b[0m, in \u001b[0;36mJit._t\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_t\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 260\u001b[0m     _r, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrap_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _r, kwargs\n",
      "    \u001b[0;31m[... skipping hidden 11 frame]\u001b[0m\n",
      "File \u001b[0;32m~/pcax/pcax/functional/_transform.py:253\u001b[0m, in \u001b[0;36mJit.__init__.<locals>._wrap_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrap_fn\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 253\u001b[0m     _r, _kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _r, tree_extract(_kwargs, is_pytree\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/pcax/pcax/functional/_transform.py:147\u001b[0m, in \u001b[0;36m_BaseTransform.__init__.<locals>._map_fn.<locals>._wrap_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m _fn_kwargs \u001b[38;5;241m=\u001b[39m tree_unref(kwargs)\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m _fn_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__RKG\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 147\u001b[0m _r \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_fn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m RKG\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;241m=\u001b[39m _old_key\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _r, kwargs\n",
      "Cell \u001b[0;32mIn[5], line 70\u001b[0m, in \u001b[0;36mtrain_on_batch\u001b[0;34m(T, x, model, optim_w, optim_h)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m10. Before optimizer step\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m#optim_w.step(model, g[\"model\"])\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m \u001b[43moptim_w\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_by\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m11. After optimizer step\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pxu\u001b[38;5;241m.\u001b[39mstep(model, clear_params\u001b[38;5;241m=\u001b[39mpxc\u001b[38;5;241m.\u001b[39mVodeParam\u001b[38;5;241m.\u001b[39mCache):\n",
      "\u001b[0;31mTypeError\u001b[0m: Optim.step() got an unexpected keyword argument 'scale_by'"
     ]
    }
   ],
   "source": [
    "# Initialize lists to store differences and energies\n",
    "MAEs = []\n",
    "SHDs = []\n",
    "energies = []\n",
    "\n",
    "# Calculate the initial MAE, SID, and SHD\n",
    "MAE_init = MAE(W_true, model.get_W())\n",
    "print(f\"Start difference (cont.) between W_true and W_init: {MAE_init:.4f}\")\n",
    "\n",
    "SHD_init = SHD(B_true, compute_binary_adjacency(model.get_W()))\n",
    "print(f\"Start SHD between B_true and B_init: {SHD_init:.4f}\")\n",
    "\n",
    "# print the values of the diagonal of the initial W\n",
    "print(\"The diagonal of the initial W: \", jnp.diag(model.get_W()))\n",
    "\n",
    "# Start timing\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "# Training loop\n",
    "with tqdm(range(nm_epochs), position=0, leave=True) as pbar:\n",
    "    for epoch in pbar:\n",
    "        # Train for one epoch using the dataloader\n",
    "        W, epoch_energy = train(dl, T=T, model=model, optim_w=optim_w, optim_h=optim_h)\n",
    "        \n",
    "        # Calculate the metrics and store them\n",
    "        W = np.array(W)\n",
    "        MAEs.append(float(MAE(W_true, W)))\n",
    "        SHDs.append(float(SHD(B_true, compute_binary_adjacency(W))))\n",
    "        energies.append(float(epoch_energy))\n",
    "        \n",
    "        # Update progress bar with the current status\n",
    "        pbar.set_description(f\"MAE {MAEs[-1]:.4f}, SHD {SHDs[-1]:.4f} || Energy {energies[-1]:.4f}\")\n",
    "\n",
    "# End timing\n",
    "end_time = timeit.default_timer()\n",
    "\n",
    "# Print the average time per epoch\n",
    "average_time_per_epoch = (end_time - start_time) / nm_epochs\n",
    "print(f\"An epoch (with compiling and testing) took on average: {average_time_per_epoch:.4f} seconds\")\n",
    "# print the values of the diagonal of the final W\n",
    "print(\"The diagonal of the final W: \", jnp.diag(model.get_W()))\n",
    "\n",
    "# %%\n",
    "print(model)\n",
    "print()\n",
    "with pxu.step(model, clear_params=pxc.VodeParam.Cache):\n",
    "    _, g = pxf.value_and_grad(pxu.Mask(pxnn.LayerParam, [False, True]), has_aux=True)(energy)(model=model)\n",
    "    print(g[\"model\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "# Set the style and color palette\n",
    "sns.set(style=\"whitegrid\")\n",
    "palette = sns.color_palette(\"tab10\")\n",
    "\n",
    "# Create a figure and axis with a 1x3 layout for side-by-side plots\n",
    "fig, axs = plt.subplots(1, 3, figsize=(18, 5))  # Adjusting layout to 1 row and 3 columns\n",
    "fig.suptitle('Performance Metrics Over Epochs', fontsize=16, weight='bold')\n",
    "\n",
    "# Plot the MAE\n",
    "sns.lineplot(x=range(len(MAEs)), y=MAEs, ax=axs[0], color=palette[0])\n",
    "axs[0].set_title(\"Mean Absolute Error (MAE)\", fontsize=14)\n",
    "axs[0].set_xlabel(\"Epoch\", fontsize=12)\n",
    "axs[0].set_ylabel(\"MAE\", fontsize=12)\n",
    "axs[0].grid(True)\n",
    "\n",
    "# Plot the SHD\n",
    "sns.lineplot(x=range(len(SHDs)), y=SHDs, ax=axs[1], color=palette[2])\n",
    "axs[1].set_title(\"Structural Hamming Distance (SHD)\", fontsize=14)\n",
    "axs[1].set_xlabel(\"Epoch\", fontsize=12)\n",
    "axs[1].set_ylabel(\"SHD\", fontsize=12)\n",
    "axs[1].grid(True)\n",
    "\n",
    "# Plot the Energy\n",
    "sns.lineplot(x=range(len(energies)), y=energies, ax=axs[2], color=palette[3])\n",
    "axs[2].set_title(\"Energy\", fontsize=14)\n",
    "axs[2].set_xlabel(\"Epoch\", fontsize=12)\n",
    "axs[2].set_ylabel(\"Energy\", fontsize=12)\n",
    "axs[2].grid(True)\n",
    "\n",
    "# Improve layout and show the plot\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjust layout to fit the suptitle\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# Now use a threshold of 0.3 to binarize the weighted adjacency matrix W\n",
    "W_est = np.array(model.get_W())\n",
    "B_est = compute_binary_adjacency(W_est, threshold=0.3)\n",
    "\n",
    "W_fix = ensure_DAG(W_est)\n",
    "B_fix = 1.0*(W_fix != 0)\n",
    "\n",
    "# %%\n",
    "# Check if B_est is indeed a DAG\n",
    "def is_dag(adjacency_matrix):\n",
    "    \"\"\"\n",
    "    Check if a given adjacency matrix represents a Directed Acyclic Graph (DAG).\n",
    "    \n",
    "    Parameters:\n",
    "        adjacency_matrix (numpy.ndarray): A square matrix representing the adjacency of a directed graph.\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if the graph is a DAG, False otherwise.\n",
    "    \"\"\"\n",
    "    # Create a directed graph from the adjacency matrix\n",
    "    graph = nx.DiGraph(adjacency_matrix)\n",
    "    \n",
    "    # Check if the graph is a DAG\n",
    "    return nx.is_directed_acyclic_graph(graph)\n",
    "\n",
    "# Example usage:\n",
    "adj_matrix = np.array([\n",
    "    [0, 1, 0],\n",
    "    [0, 0, 1],\n",
    "    [1, 0, 0]\n",
    "])\n",
    "\n",
    "print(is_dag(adj_matrix))  # Output: False, since the graph has a cycle.\n",
    "\n",
    "# Check if the estimated binary adjacency matrix B_est is a DAG\n",
    "is_dag_B_est = is_dag(B_est)\n",
    "print(f\"Is the estimated binary adjacency matrix a DAG? {is_dag_B_est}\")\n",
    "\n",
    "# Define fucntion to compute h_reg based W with h_reg = jnp.trace(jax.scipy.linalg.expm(W * W)) - d, here * denotes the hadamard product\n",
    "def compute_h_reg(W):\n",
    "    \"\"\"This function computes the h_reg term based on the matrix W.\"\"\"\n",
    "    h_reg = jnp.trace(jax.scipy.linalg.expm(W * W)) - W.shape[0]\n",
    "    return h_reg\n",
    "\n",
    "# Compute the h_reg term for the true weighted adjacency matrix W_true\n",
    "h_reg_true = compute_h_reg(W_true)\n",
    "print(f\"The h_reg term for the true weighted adjacency matrix W_true is: {h_reg_true:.4f}\")\n",
    "# Compute the h_reg term for the estimated weighted adjacency matrix W_est\n",
    "h_reg_est = compute_h_reg(W_est)\n",
    "print(f\"The h_reg term for the estimated weighted adjacency matrix W_est is: {h_reg_est:.4f}\")\n",
    "h_reg_fix = compute_h_reg(W_fix)\n",
    "print(f\"The h_reg term for the fixed weighted adjacency matrix W_fix is: {h_reg_fix:.4f}\")\n",
    "\n",
    "# We note that B_est is a DAG even though h_reg is not equal to 0.0 (but only close to 0.0). \n",
    "# This is because the matrix exponential is not a perfect measure of the DAG constraint, but it is a good approximation.\n",
    "\n",
    "# %%\n",
    "# print first 5 rows and columsn of W_est and round values to 4 decimal places and show as non-scientific notation\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "print(\"The first 5 rows and columns of the estimated weighted adjacency matrix W_est\\n{}\".format(W_est[:5, :5]))\n",
    "\n",
    "# %%\n",
    "# now show the adjacency matrix of the true graph and the estimated graph side by side\n",
    "plot_adjacency_matrices(B_true, B_est)\n",
    "\n",
    "# now show the adjacency matrix of the true graph and the estimated graph side by side\n",
    "plot_adjacency_matrices(B_true, B_fix)\n",
    "\n",
    "# %%\n",
    "print(np.sum(B_est))\n",
    "print(np.sum(B_fix))\n",
    "print(np.sum(B_true))\n",
    "\n",
    "# %%\n",
    "# plot est_dag and true_dag\n",
    "GraphDAG(B_est, B_true)\n",
    "# calculate accuracy\n",
    "met_pcax = MetricsDAG(B_est, B_true)\n",
    "print(met_pcax.metrics)\n",
    "\n",
    "met_pcax_fix = MetricsDAG(B_fix, B_true)\n",
    "print(met_pcax_fix.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pcax24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
