{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train with scan and momentum but without resetting the momentum at each inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "import jax\n",
    "from optax._src import base \n",
    "from optax._src import combine\n",
    "from optax._src import transform\n",
    "import optax\n",
    "from typing import Any, Callable, Optional\n",
    "\n",
    "import pcx as px\n",
    "import pcx.predictive_coding as pxc\n",
    "import pcx.nn as pxnn\n",
    "import pcx.functional as pxf\n",
    "import pcx.utils as pxu\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from scipy.stats import wasserstein_distance\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "px.RKG.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(pxc.EnergyModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        hidden_dim: int,\n",
    "        output_dim: int,\n",
    "        nm_layers: int,\n",
    "        act_fn: Callable[[jax.Array], jax.Array]\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.act_fn = px.static(act_fn)\n",
    "        \n",
    "        self.layers = [pxnn.Linear(input_dim, hidden_dim)] + [\n",
    "            pxnn.Linear(hidden_dim, hidden_dim, bias=False) for _ in range(nm_layers - 2)\n",
    "        ] + [pxnn.Linear(hidden_dim, output_dim, bias=False)]\n",
    "\n",
    "        self.vodes = [\n",
    "            pxc.Vode() for _ in range(nm_layers)\n",
    "        ]\n",
    "        \n",
    "        self.vodes[-1].h.frozen = True\n",
    "\n",
    "    def __call__(self, x, y):\n",
    "        for v, l in zip(self.vodes[:-1], self.layers[:-1]):\n",
    "            x = self.act_fn(v(l(x)))\n",
    "\n",
    "        x = self.vodes[-1](self.layers[-1](x))\n",
    "        if y is not None:\n",
    "            self.vodes[-1].set(\"h\", y)\n",
    "        return self.vodes[-1].get(\"u\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pxf.vmap(pxu.M(pxc.VodeParam | pxc.VodeParam.Cache).to((None, 0)), in_axes=(0, 0), out_axes=0)\n",
    "def forward(x, y, *, model: Model):\n",
    "    return model(x, y)\n",
    "\n",
    "\n",
    "@pxf.vmap(pxu.M(pxc.VodeParam | pxc.VodeParam.Cache).to((None, 0)), in_axes=(0,), out_axes=(None, 0), axis_name=\"batch\")\n",
    "def energy(x, *, model: Model):\n",
    "    y_ = model(x, None)\n",
    "    return jax.lax.psum(model.energy(), \"batch\"), y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pxf.jit(static_argnums=0)\n",
    "def train_on_batch(\n",
    "    T: int,\n",
    "    x: jax.Array,\n",
    "    y: jax.Array,\n",
    "    *,\n",
    "    model: Model,\n",
    "    optim_w: pxu.Optim,\n",
    "    optim_h: pxu.Optim\n",
    "):\n",
    "    def h_step(i, x, *, model, optim_h):\n",
    "        with pxu.step(model, clear_params=pxc.VodeParam.Cache):\n",
    "            (e, y_), g = pxf.value_and_grad(\n",
    "                pxu.M_hasnot(pxc.VodeParam, frozen=True).to([False, True]),\n",
    "                has_aux=True\n",
    "            )(energy)(x, model=model)\n",
    "        optim_h.step(model, g[\"model\"])\n",
    "        return x, None\n",
    "\n",
    "    model.train()\n",
    "        \n",
    "    # Init step\n",
    "    with pxu.step(model, (pxc.STATUS.INIT, None), clear_params=pxc.VodeParam.Cache):\n",
    "        forward(x, y, model=model)\n",
    "    optim_h.init(pxu.M_hasnot(pxc.VodeParam, frozen=True)(model))\n",
    "    \n",
    "    # Inference steps\n",
    "    pxf.scan(h_step, xs=jax.numpy.arange(T))(x, model=model, optim_h=optim_h)\n",
    "    \n",
    "    optim_h.clear()\n",
    "\n",
    "    # Learning step\n",
    "    with pxu.step(model, clear_params=pxc.VodeParam.Cache):\n",
    "        (e, y_), g = pxf.value_and_grad(pxu.M(pxnn.LayerParam).to([False, True]), has_aux=True)(energy)(x, model=model)\n",
    "    optim_w.step(model, g[\"model\"], scale_by=1.0/x.shape[0])\n",
    "\n",
    "\n",
    "def train(dl, T, *, model: Model, optim_w: pxu.Optim, optim_h: pxu.Optim):\n",
    "    model.vodes[-1].h.frozen = True\n",
    "    for x, y in tqdm(dl):\n",
    "        train_on_batch(T, x, y, model=model, optim_w=optim_w, optim_h=optim_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pxf.jit(static_argnums=0)\n",
    "def eval_on_batch(\n",
    "    T: int,\n",
    "    x: jax.Array, \n",
    "    *, \n",
    "    model: Model,\n",
    "    optim_h: pxu.Optim\n",
    "    ):\n",
    "    def h_step(i, x, *, model, optim_h):\n",
    "        with pxu.step(model, clear_params=pxc.VodeParam.Cache):\n",
    "            (e, y_), g = pxf.value_and_grad(\n",
    "                pxu.M_hasnot(pxc.VodeParam, frozen=True).to([False, True]),\n",
    "                has_aux=True\n",
    "            )(energy)(x, model=model)\n",
    "        optim_h.step(model, g[\"model\"])\n",
    "        return x, None\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    if model.vodes[-1].h.frozen:\n",
    "        print(\"vode[-1] should not be frozen! set frozen=False before calling eval function.\")\n",
    "        \n",
    "    # Init step\n",
    "    with pxu.step(model, (pxc.STATUS.INIT, None), clear_params=pxc.VodeParam.Cache):\n",
    "        forward(x, None, model=model)\n",
    "    optim_h.init(pxu.M(pxc.VodeParam)(model))\n",
    "    \n",
    "    # Inference steps\n",
    "    x, y_ = pxf.scan(h_step, xs=jax.numpy.arange(T))(x, model=model, optim_h=optim_h)\n",
    "    \n",
    "    optim_h.clear()\n",
    "\n",
    "\n",
    "# MCPC evaluation loop for 1D data\n",
    "def eval(dl, T, *, model: Model, optim_h: pxu.Optim):\n",
    "    model.vodes[-1].h.frozen = False\n",
    "    ys = []\n",
    "    ys_ = []\n",
    "    \n",
    "    for x, y in dl:\n",
    "        eval_on_batch(T, x, model=model, optim_h=optim_h)\n",
    "        ys.append(y)\n",
    "        ys_.append(model.vodes[-1].get(\"h\"))\n",
    "\n",
    "    ys = np.concatenate(ys, axis=0)\n",
    "    ys_ = np.concatenate(ys_, axis=0)\n",
    "\n",
    "    return wasserstein_distance(ys.squeeze(), ys_.squeeze()), ys_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "model = Model(\n",
    "    input_dim=1,\n",
    "    hidden_dim=1,\n",
    "    output_dim=1,\n",
    "    nm_layers=2,\n",
    "    act_fn= lambda x:x\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define noisy sgd optimiser for MCPC\n",
    "def sgdld(\n",
    "    learning_rate: base.ScalarOrSchedule,\n",
    "    momentum: Optional[float] = None,\n",
    "    h_var: float = 1.0,\n",
    "    gamma: float = 0.,\n",
    "    nesterov: bool = False,\n",
    "    accumulator_dtype: Optional[Any] = None,\n",
    "    seed: int = lambda: px.RKG(1)[0],\n",
    ") -> base.GradientTransformation:\n",
    "    def optim_fn():\n",
    "        eta = 2*h_var*(1-momentum)/learning_rate if momentum is not None else 2*h_var/learning_rate\n",
    "        s = seed()\n",
    "        return combine.chain(\n",
    "            transform.add_noise(eta, gamma, s),\n",
    "            (transform.trace(decay=momentum, nesterov=nesterov,\n",
    "                            accumulator_dtype=accumulator_dtype)\n",
    "            if momentum is not None else base.identity()),\n",
    "            transform.scale_by_learning_rate(learning_rate)\n",
    "        )\n",
    "    return optim_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_optimiser_fn = sgdld\n",
    "lr = 1e-1\n",
    "momentum = 0.5\n",
    "h_var = 1.0\n",
    "gamma = 0.\n",
    "lr_p = 1e-3\n",
    "\n",
    "mean = 1\n",
    "var = 5\n",
    "\n",
    "nm_elements = 10240\n",
    "X = np.zeros((batch_size * (nm_elements // batch_size), 1))\n",
    "y = np.random.randn(batch_size * (nm_elements // batch_size)).reshape(-1,1) * np.sqrt(var) + mean\n",
    "\n",
    "nm_elements_test = 1024\n",
    "X_test = np.zeros((batch_size * (nm_elements_test // batch_size), 1))\n",
    "y_test = np.random.randn(batch_size * (nm_elements // batch_size)).reshape(-1,1) * np.sqrt(var) + mean\n",
    "\n",
    "\n",
    "# we split the dataset in training batches and do the same for the generated test set.\n",
    "train_dl = list(zip(X.reshape(-1, batch_size, 1), y.reshape(-1, batch_size, 1)))\n",
    "test_dl = tuple(zip(X_test.reshape(-1, batch_size, 1), y_test.reshape(-1, batch_size, 1)))\n",
    "\n",
    "nm_epochs = 5120 // (nm_elements // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y, alpha = 0.5, density=True)\n",
    "plt.hist(y_test, alpha = 0.5, density=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "with pxu.step(model, pxc.STATUS.INIT, clear_params=pxc.VodeParam.Cache):\n",
    "    forward(jax.numpy.zeros((batch_size, 1)), None, model=model)\n",
    "    model.vodes[-1].h.frozen = True\n",
    "    optim_h = pxu.Optim(h_optimiser_fn(lr, momentum, h_var, gamma))\n",
    "    optim_w = pxu.Optim(lambda: optax.adam(lr_p), pxu.M(pxnn.LayerParam)(model))\n",
    "    # make optimiser that also optimises the activity of the model layer[-1]\n",
    "    model.vodes[-1].h.frozen = False\n",
    "\n",
    "\n",
    "T = 100\n",
    "T_eval = 100\n",
    "w, y_ = eval(test_dl, T = T_eval, model=model, optim_h=optim_h)\n",
    "print(f\"Epoch {0}/{nm_epochs} - Wasserstein distance: {w :.2f}\")\n",
    "for e in range(nm_epochs):\n",
    "    random.shuffle(train_dl)\n",
    "    train(train_dl, T=T, model=model, optim_w=optim_w, optim_h=optim_h)\n",
    "    if e %5 == 4 or e == nm_epochs - 1:\n",
    "        w, y_ = eval(test_dl, T = T_eval, model=model, optim_h=optim_h)\n",
    "        print(f\"Epoch {e + 1}/{nm_epochs} - Wasserstein distance: {w :.2f}\")\n",
    "\n",
    "print(f\"Learned data distribution has mean {y_.mean():.2f} and var {y_.var():.2f} \")\n",
    "print(f\"Learned parameters weight {model.layers[-1].nn.weight.get()[0,0] :.2f} and bias {model.layers[0].nn.bias.get()[0] :.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y, label = \"data\", density=True, alpha=0.5, bins=30)\n",
    "plt.hist(y_, label = \"learned\", density=True, alpha=0.5, bins=30)\n",
    "plt.ylabel(\"pdf\")\n",
    "plt.xlabel(\"y\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
