{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial #1: Analysis of PCNs\n",
    "\n",
    "We see how to analyse a PCN, studying its behaviour with respect to gradient updates.\n",
    "The code is an adaptation from tutorial #0. In particular, the code defining the model is unchanged, and only the training functions are expanded to include the analysis part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies not included in the base requirements.txt\n",
    "\n",
    "!pip install scikit-learn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "import jax\n",
    "import jax.tree_util as jtu\n",
    "import jax.numpy as jnp\n",
    "import equinox as eqx\n",
    "\n",
    "import pcx as px\n",
    "import pcx.predictive_coding as pxc\n",
    "import pcx.nn as pxnn\n",
    "import pcx.functional as pxf\n",
    "import pcx.utils as pxu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(pxc.EnergyModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        hidden_dim: int,\n",
    "        output_dim: int,\n",
    "        nm_layers: int,\n",
    "        act_fn: Callable[[jax.Array], jax.Array]\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.act_fn = px.static(act_fn)\n",
    "        \n",
    "        self.layers = [pxnn.Linear(input_dim, hidden_dim)] + [\n",
    "            pxnn.Linear(hidden_dim, hidden_dim) for _ in range(nm_layers - 2)\n",
    "        ] + [pxnn.Linear(hidden_dim, output_dim)]\n",
    "\n",
    "        self.vodes = [\n",
    "            pxc.Vode() for _ in range(nm_layers - 1)\n",
    "        ] + [pxc.Vode(pxc.ce_energy)]\n",
    "        \n",
    "        self.vodes[-1].h.frozen = True\n",
    "\n",
    "    def __call__(self, x, y):\n",
    "        for v, l in zip(self.vodes[:-1], self.layers[:-1]):\n",
    "            x = v(self.act_fn(l(x)))\n",
    "\n",
    "        x = self.vodes[-1](self.layers[-1](x))\n",
    "\n",
    "        if y is not None:\n",
    "            self.vodes[-1].set(\"h\", y)\n",
    "\n",
    "        return self.vodes[-1].get(\"u\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pxf.vmap(pxu.M(pxc.VodeParam | pxc.VodeParam.Cache).to((None, 0)), in_axes=(0, 0), out_axes=0)\n",
    "def forward(x, y, *, model: Model):\n",
    "    return model(x, y)\n",
    "\n",
    "@pxf.vmap(pxu.M(pxc.VodeParam | pxc.VodeParam.Cache).to((None, 0)), in_axes=(0,), out_axes=(None, 0), axis_name=\"batch\")\n",
    "def energy(x, *, model: Model):\n",
    "    y_ = model(x, None)\n",
    "    return jax.lax.psum(model.energy(), \"batch\"), y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pxf.jit(static_argnums=0)\n",
    "def train_on_batch(\n",
    "    T: int,\n",
    "    x: jax.Array,\n",
    "    y: jax.Array,\n",
    "    *,\n",
    "    model: Model,\n",
    "    optim_w: pxu.Optim,\n",
    "    optim_h: pxu.Optim\n",
    "):\n",
    "    model.train()\n",
    "\n",
    "    with pxu.step(model, pxc.STATUS.INIT, clear_params=pxc.VodeParam.Cache):\n",
    "        forward(x, y, model=model)\n",
    "\n",
    "    for i in range(T):\n",
    "        with pxu.step(model, clear_params=pxc.VodeParam.Cache):\n",
    "            (e, y_), g = pxf.value_and_grad(\n",
    "                pxu.M_hasnot(pxc.VodeParam, frozen=True).to([False, True]),\n",
    "                has_aux=True\n",
    "            )(energy)(x, model=model)\n",
    "        \n",
    "        optim_h.step(model, g[\"model\"])\n",
    "\n",
    "    with pxu.step(model, clear_params=pxc.VodeParam.Cache):\n",
    "        (e, y_), g = pxf.value_and_grad(pxu.M(pxnn.LayerParam).to([False, True]), has_aux=True)(energy)(x, model=model)\n",
    "    optim_w.step(model, g[\"model\"], scale_by=1.0/x.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "@pxf.jit()\n",
    "def eval_on_batch(x: jax.Array, y: jax.Array, *, model: Model):\n",
    "    model.eval()\n",
    "    \n",
    "    with pxu.step(model, pxc.STATUS.INIT, clear_params=pxc.VodeParam.Cache):\n",
    "        y_ = forward(x, None, model=model).argmax(axis=-1)\n",
    "    \n",
    "    return (y_ == y).mean(), y_\n",
    "\n",
    "\n",
    "def train(dl, T, *, model: Model, optim_w: pxu.Optim, optim_h: pxu.Optim):\n",
    "    for x, y in dl:\n",
    "        train_on_batch(T, x, jax.nn.one_hot(y, 2), model=model, optim_w=optim_w, optim_h=optim_h)\n",
    "\n",
    "\n",
    "def eval(dl, *, model: Model):\n",
    "    acc = []\n",
    "    ys_ = []\n",
    "    \n",
    "    for x, y in dl:\n",
    "        a, y_ = eval_on_batch(x, y, model=model)\n",
    "        acc.append(a)\n",
    "        ys_.append(y_)\n",
    "    \n",
    "    return np.mean(acc), np.concatenate(ys_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "nm_layers = 4\n",
    "\n",
    "\n",
    "# we merge initialisation in a single function to be able to create multiple models.\n",
    "def init(\n",
    "    batch_size: int,\n",
    "    hidden_dim: int,\n",
    "    h_lr: float = 1e-2,\n",
    "    nm_layers: int = 4,\n",
    "    act_fn: Callable[[jax.Array], jax.Array] = jax.nn.leaky_relu\n",
    "):\n",
    "    model = Model(\n",
    "        input_dim=2,\n",
    "        hidden_dim=hidden_dim,\n",
    "        output_dim=2,\n",
    "        nm_layers=nm_layers,\n",
    "        act_fn=act_fn\n",
    "    )\n",
    "    \n",
    "    # here, we use a state optimizer without internal state (i.e., no momentum), so we\n",
    "    # initialise it once without the need to init/clear it for every batch.\n",
    "    # we also perform a dummy forward pass so to initialise the `Vodes` before the first\n",
    "    # `train_on_batch` call to avoid recompilation.\n",
    "    with pxu.step(model, pxc.STATUS.INIT, clear_params=pxc.VodeParam.Cache):\n",
    "        forward(jax.numpy.zeros((batch_size, 2)), None, model=model)\n",
    "        \n",
    "        optim_h = pxu.Optim(lambda: optax.sgd(h_lr), pxu.M_hasnot(pxc.VodeParam, frozen=True)(model))\n",
    "        optim_w = pxu.Optim(lambda: optax.adamw(1e-2), pxu.M(pxnn.LayerParam)(model))\n",
    "    \n",
    "    return model, optim_h, optim_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "\n",
    "nm_elements = 512\n",
    "X, y = make_moons(n_samples=batch_size * (nm_elements // batch_size), noise=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = list(zip(X.reshape(-1, batch_size, 2), y.reshape(-1, batch_size)))\n",
    "\n",
    "X_test, y_test = make_moons(n_samples=batch_size * (nm_elements // batch_size) // 2, noise=0.2, random_state=0)\n",
    "test_dl = tuple(zip(X_test.reshape(-1, batch_size, 2), y_test.reshape(-1, batch_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "Instead of training a single PCN, we iterate over several hidden widths and for each resulting network we compute the best h learning rate.\n",
    "Note that we do not change the number of layers (set to 4) or the number of inference steps (set to 8).\n",
    "\n",
    "Ideally hidden width and h learning rate should be independent values, as the h learning rate should only affect how the error propagates through multiple layers, and be uneffected by the properties of each single layer (also because this would make networks with layers of different sizes inheritely difficult to train)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "nm_epochs = 32 // (nm_elements // batch_size)\n",
    "\n",
    "# We test over the following values\n",
    "h_dims = [16, 32, 64, 128, 256, 512, 1024, 2048]\n",
    "h_lrs = [1e-3, 3e-3, 5e-3, 1e-2, 3e-2, 5e-2, 1e-1, 3e-1, 5e-1]\n",
    "\n",
    "def run(\n",
    "    nm_epochs,\n",
    "    model,\n",
    "    optim_h,\n",
    "    optim_w,\n",
    "    train_dl,\n",
    "    test_dl,\n",
    "    T = 8\n",
    "):\n",
    "    for _ in range(nm_epochs):\n",
    "        random.shuffle(train_dl)\n",
    "        train(train_dl, T=T, model=model, optim_w=optim_w, optim_h=optim_h)\n",
    "    a, _ = eval(test_dl, model=model)\n",
    "    \n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_by_dim = {}\n",
    "\n",
    "for h_dim in h_dims:\n",
    "    acc_by_dim[h_dim] = []\n",
    "    for h_lr in h_lrs:\n",
    "        print(f\"Running h_dim={h_dim} and h_lr={h_lr}.\", end=\" \")\n",
    "        model, optim_h, optim_w = init(batch_size, h_dim, h_lr, act_fn=jax.nn.leaky_relu)\n",
    "        a = run(nm_epochs, model, optim_h, optim_w, train_dl, test_dl, 8)\n",
    "        acc_by_dim[h_dim].append(a)\n",
    "        print(f\"Accuracy: {(a*100.0):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Plot the dataset\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for i, (h_dim, accs) in enumerate(acc_by_dim.items()):\n",
    "    color = plt.get_cmap('viridis')(i / (len(acc_by_dim) - 1))\n",
    "    plt.plot(h_lrs, accs, marker=\"o\", color=color, label=f\"h={h_dim}\")\n",
    "    \n",
    "    max_acc = max(accs)\n",
    "    i_max = accs.index(max_acc)\n",
    "    plt.scatter(h_lrs[i_max], max_acc, s=500, color=color, alpha=0.2)\n",
    "\n",
    "plt.margins(x = 0.02)\n",
    "plt.xticks(h_lrs, h_lrs, rotation=45)\n",
    "plt.xscale(\"log\")\n",
    "plt.legend()\n",
    "plt.title(\"Two Moons Dataset\")\n",
    "plt.xlabel(\"h learning rate\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our hypothesis is wrong! And the best learning rate changes drastically for different hidden sizes. This is true for different choices of activation function, and even by using a SE energy instead of the standard CE energy (so it is unrelated to it, note that SE energy performs drastically worse than CE, as expected). We chose to use leaky_relu as default activation function to show the most evident correlation (note that based on the seed it not be perfect).\n",
    "\n",
    "What we see is that high hidden dim values prefer small learning rates and are unstable for larger values. This does not apply to BP, which is unaffected by the width of the network. Something is wrong, could it be the default weight initialisation being not suitable for PC (to this day, we haven't solved this, but we are working on it)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In \"Understanding the difficulty of training deep feedforward neural networks\", the authors suggest that the variance of the gradients should be costant through the network, as we want information that doesn't vanish or explode throughout the layers (eqs. 8,9, 14). To measure this, we check the ratio between the gradients of different layers. To do so, we modify the train function to return such information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pxf.jit(static_argnums=0)\n",
    "def train_on_batch_with_ratio(\n",
    "    T: int,\n",
    "    x: jax.Array,\n",
    "    y: jax.Array,\n",
    "    *,\n",
    "    model: Model,\n",
    "    optim_w: pxu.Optim,\n",
    "    optim_h: pxu.Optim\n",
    "):\n",
    "    model.train()\n",
    "\n",
    "    with pxu.step(model, pxc.STATUS.INIT, clear_params=pxc.VodeParam.Cache):\n",
    "        forward(x, y, model=model)\n",
    "\n",
    "    for i in range(T):\n",
    "        with pxu.step(model, clear_params=pxc.VodeParam.Cache):\n",
    "            (e, y_), g = pxf.value_and_grad(\n",
    "                pxu.M_hasnot(pxc.VodeParam, frozen=True).to([False, True]),\n",
    "                has_aux=True\n",
    "            )(energy)(x, model=model)\n",
    "        \n",
    "        optim_h.step(model, g[\"model\"])\n",
    "\n",
    "    with pxu.step(model, clear_params=pxc.VodeParam.Cache):\n",
    "        (e, y_), g = pxf.value_and_grad(pxu.M(pxnn.LayerParam).to([False, True]), has_aux=True)(energy)(x, model=model)\n",
    "    optim_w.step(model, g[\"model\"], scale_by=1.0/x.shape[0])\n",
    "    \n",
    "    # We measure the variance of the gradients for the second and third layer\n",
    "    # Note that by having 4 layers in total we guaranteed that the two layers we compare have the same shape.\n",
    "    assert g[\"model\"].layers[1].nn.weight.get().shape == g[\"model\"].layers[2].nn.weight.get().shape, \"Shapes can't be different\"\n",
    "    var_g_w1 = jnp.var(g[\"model\"].layers[1].nn.weight.get())\n",
    "    var_g_w2 = jnp.var(g[\"model\"].layers[2].nn.weight.get())\n",
    "    \n",
    "    return var_g_w1 / var_g_w2\n",
    "\n",
    "\n",
    "def train_with_ratio(dl, T, *, model: Model, optim_w: pxu.Optim, optim_h: pxu.Optim):\n",
    "    ratios = []\n",
    "    for x, y in dl:\n",
    "        ratio = train_on_batch_with_ratio(T, x, jax.nn.one_hot(y, 2), model=model, optim_w=optim_w, optim_h=optim_h)\n",
    "        ratios.append(ratio)\n",
    "        \n",
    "    return np.mean(ratios)\n",
    "\n",
    "def run_with_ratio(\n",
    "    nm_epochs,\n",
    "    model,\n",
    "    optim_h,\n",
    "    optim_w,\n",
    "    train_dl,\n",
    "    test_dl,\n",
    "    T = 16\n",
    "):\n",
    "    for i in range(nm_epochs):\n",
    "        random.shuffle(train_dl)\n",
    "        ratio = train_with_ratio(train_dl, T=T, model=model, optim_w=optim_w, optim_h=optim_h)\n",
    "        print(f\"\\tAvg. gradient ratio for epoch {i + 1}: {ratio}\")\n",
    "    a, _ = eval(test_dl, model=model)\n",
    "    \n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_dims = [50, 200, 800, 1600]\n",
    "# we use a learning rate that works for all the chosen dims.\n",
    "h_lr = 3e-3\n",
    "\n",
    "for h_dim in h_dims:\n",
    "    print(f\"Running h_dim={h_dim}\")\n",
    "    model, optim_h, optim_w = init(batch_size, h_dim, h_lr, act_fn=jax.nn.leaky_relu)\n",
    "    a = run_with_ratio(nm_epochs, model, optim_h, optim_w, train_dl, test_dl, 16)\n",
    "    print(f\"Accuracy: {(a*100.0):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the ratio is not stable, as it should instead be. Note that by increasing the h learning rate or the number T of inference steps, it can be brought closer to 1 for the smaller architectures. However, larger h learning rates make big architectures unstable, making the difficult to train (as you can only play around with T and increase the total computational cost)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
