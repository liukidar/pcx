{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial #4: CIFAR10 via AlexNet\n",
    "\n",
    "In this notebook we will see how to code a PCN based on AlexNet and train it on CIFAR10.\n",
    "We will use PyTorch to handle the dataset and the dataloading, so make sure it is installed in the local environment. We only need the CPU version which, currently, can be installed via `pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "# Core dependencies\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# pcax\n",
    "import pcax as px\n",
    "import pcax.predictive_coding as pxc\n",
    "import pcax.nn as pxnn\n",
    "import pcax.utils as pxu\n",
    "\n",
    "\n",
    "class AlexNet(pxc.EnergyModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        nm_classes: int,\n",
    "        act_fn: Callable[[jax.Array], jax.Array]\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.nm_classes = nm_classes\n",
    "        \n",
    "        # Note we use a custom activation function and not exclusively ReLU since\n",
    "        # it does not seem to perform as well as in backpropagation\n",
    "        self.act_fn = px.static(act_fn)\n",
    "\n",
    "        # We define the convolutional layers. We organise them in blocks just for clarity.\n",
    "        # Ideally, pcax will soon support a \"pxnn.Sequential\" module to ease the definition\n",
    "        # of such blocks. Layers are based on equinox.nn, so check their documentation for\n",
    "        # more information.\n",
    "        self.feature_layers = [\n",
    "            (\n",
    "                pxnn.Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)),\n",
    "                self.act_fn,\n",
    "                pxnn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            ),\n",
    "            (\n",
    "                pxnn.Conv2d(64, 192, kernel_size=(3), padding=(1, 1)),\n",
    "                self.act_fn,\n",
    "                pxnn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            ),\n",
    "            (\n",
    "                pxnn.Conv2d(192, 384, kernel_size=(3, 3), padding=(1, 1)),\n",
    "                self.act_fn\n",
    "            ),\n",
    "            (\n",
    "                pxnn.Conv2d(384, 256, kernel_size=(3, 3), padding=(1, 1)),\n",
    "                self.act_fn\n",
    "            ),\n",
    "            (\n",
    "                pxnn.Conv2d(256, 256, kernel_size=(3, 3), padding=(1, 1)),\n",
    "                self.act_fn,\n",
    "                pxnn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            )\n",
    "        ]\n",
    "        # We define the classifier layers. We organise them in blocks just for clarity.\n",
    "        self.classifier_layers = [\n",
    "            (\n",
    "                pxnn.Linear(256 * 2 * 2, 4096),\n",
    "                self.act_fn\n",
    "            ),\n",
    "            (\n",
    "                pxnn.Linear(4096, 4096),\n",
    "                self.act_fn\n",
    "            ),\n",
    "            (\n",
    "                pxnn.Linear(4096, self.nm_classes),\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        # We define the Vode modules. Note that currently each vode requires its shape\n",
    "        # to be manually specified. This will be improved in the near future as lazy\n",
    "        # initialisation should be possible.\n",
    "        self.vodes = [\n",
    "            pxc.Vode() for _ in self.feature_layers\n",
    "        ] + [\n",
    "            pxc.Vode() for _ in self.classifier_layers[:-1]\n",
    "        ] + [pxc.Vode(pxc.ce_energy)]\n",
    "\n",
    "        # Remember 'frozen' is a user specified attribute used later in the gradient function\n",
    "        self.vodes[-1].h.frozen = True\n",
    "\n",
    "    def __call__(self, x: jax.Array, y: jax.Array):\n",
    "        # Nothing new here: we just define the forward pass of the network by iterating\n",
    "        # through the blocks and vodes. Each block is followed by a vode, to split the\n",
    "        # computation in indpendent chunks. \n",
    "        for block, node in zip(self.feature_layers, self.vodes[:len(self.feature_layers)]):\n",
    "            for layer in block:\n",
    "                x = layer(x)\n",
    "            x = node(x)\n",
    "\n",
    "        x = x.flatten()\n",
    "        for block, node in zip(self.classifier_layers, self.vodes[len(self.feature_layers):]):\n",
    "            for layer in block:\n",
    "                x = layer(x)\n",
    "            x = node(x)\n",
    "\n",
    "        if y is not None:\n",
    "            self.vodes[-1].set(\"h\", y)\n",
    "\n",
    "        return self.vodes[-1].get(\"u\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# This is a simple collate function that stacks numpy arrays used to interface\n",
    "# the PyTorch dataloader with JAX. In the future we hope to provide custom dataloaders\n",
    "# that are independent of PyTorch.\n",
    "\n",
    "def numpy_collate(batch):\n",
    "    if isinstance(batch[0], np.ndarray):\n",
    "        return np.stack(batch)\n",
    "    elif isinstance(batch[0], (tuple, list)):\n",
    "        transposed = zip(*batch)\n",
    "        return [numpy_collate(samples) for samples in transposed]\n",
    "    else:\n",
    "        return np.array(batch)\n",
    "\n",
    "\n",
    "# The dataloader assumes cuda is being used, as such it sets 'pin_memory = True' and\n",
    "# 'prefetch_factor = 2'. Note that the batch size should be constant during training, so\n",
    "# we set 'drop_last = True' to avoid having to deal with variable batch sizes. \n",
    "class TorchDataloader(torch.utils.data.DataLoader):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset,\n",
    "        batch_size=1,\n",
    "        shuffle=None,\n",
    "        sampler=None,\n",
    "        batch_sampler=None,\n",
    "        num_workers=1,\n",
    "        pin_memory=True,\n",
    "        timeout=0,\n",
    "        worker_init_fn=None,\n",
    "        persistent_workers=True,\n",
    "        prefetch_factor=2,\n",
    "    ):\n",
    "        super(self.__class__, self).__init__(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            sampler=sampler,\n",
    "            batch_sampler=batch_sampler,\n",
    "            num_workers=num_workers,\n",
    "            collate_fn=numpy_collate,\n",
    "            pin_memory=pin_memory,\n",
    "            drop_last=True if batch_sampler is None else None,\n",
    "            timeout=timeout,\n",
    "            worker_init_fn=worker_init_fn,\n",
    "            persistent_workers=persistent_workers,\n",
    "            prefetch_factor=prefetch_factor,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "def get_dataloaders(batch_size: int):\n",
    "    t = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        # These are normalisation factors found online.\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "        transforms.Lambda(lambda x: x.numpy())\n",
    "    ])\n",
    "\n",
    "    train_dataset = torchvision.datasets.CIFAR10(\n",
    "        \"~/tmp/cifar10/\",\n",
    "        transform=t,\n",
    "        download=True,\n",
    "        train=True,\n",
    "    )\n",
    "\n",
    "    train_dataloader = TorchDataloader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "    )\n",
    "\n",
    "    test_dataset = torchvision.datasets.CIFAR10(\n",
    "        \"~/tmp/cifar10/\",\n",
    "        transform=t,\n",
    "        download=True,\n",
    "        train=False,\n",
    "    )\n",
    "        \n",
    "    test_dataloader = TorchDataloader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "    )\n",
    "\n",
    "    return train_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pcax.functional as pxf\n",
    "\n",
    "# Training functions are identical to tutorial #0, we only change the model definition.\n",
    "\n",
    "@pxf.vmap(pxu.M(pxc.VodeParam | pxc.VodeParam.Cache).to((None, 0)), in_axes=(0, 0), out_axes=0)\n",
    "def forward(x, y, *, model: AlexNet):\n",
    "    return model(x, y)\n",
    "\n",
    "@pxf.vmap(pxu.M(pxc.VodeParam | pxc.VodeParam.Cache).to((None, 0)), in_axes=(0,), out_axes=(None, 0), axis_name=\"batch\")\n",
    "def energy(x, *, model: AlexNet):\n",
    "    y_ = model(x, None)\n",
    "    return jax.lax.psum(model.energy(), \"batch\"), y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pxf.jit(static_argnums=0)\n",
    "def train_on_batch(\n",
    "    T: int,\n",
    "    x: jax.Array,\n",
    "    y: jax.Array,\n",
    "    *,\n",
    "    model: AlexNet,\n",
    "    optim_w: pxu.Optim,\n",
    "    optim_h: pxu.Optim\n",
    "):\n",
    "    model.train()\n",
    "\n",
    "    # Init step\n",
    "    with pxu.step(model, pxc.STATUS.INIT, clear_params=pxc.VodeParam.Cache):\n",
    "        forward(x, y, model=model)\n",
    "    \n",
    "    optim_h.init(pxu.M_hasnot(pxc.VodeParam, frozen=True)(model))\n",
    "    \n",
    "    # Inference steps\n",
    "    for _ in range(T):\n",
    "        with pxu.step(model, clear_params=pxc.VodeParam.Cache):\n",
    "            _, g = pxf.value_and_grad(\n",
    "                pxu.M_hasnot(pxc.VodeParam, frozen=True).to([False, True]),\n",
    "                has_aux=True\n",
    "            )(energy)(x, model=model)\n",
    "        \n",
    "        optim_h.step(model, g[\"model\"])\n",
    "    \n",
    "    optim_h.clear()\n",
    "\n",
    "    # Learning step\n",
    "    with pxu.step(model, clear_params=pxc.VodeParam.Cache):\n",
    "        _, g = pxf.value_and_grad(pxu.M(pxnn.LayerParam).to([False, True]), has_aux=True)(energy)(x, model=model)\n",
    "    optim_w.step(model, g[\"model\"], scale_by=1.0/x.shape[0])\n",
    "\n",
    "\n",
    "@pxf.jit()\n",
    "def eval_on_batch(x: jax.Array, y: jax.Array, *, model: AlexNet):\n",
    "    model.eval()\n",
    "    \n",
    "    with pxu.step(model, pxc.STATUS.INIT, clear_params=pxc.VodeParam.Cache):\n",
    "        y_ = forward(x, None, model=model).argmax(axis=-1)\n",
    "    \n",
    "    return (y_ == y).mean(), y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dl, T, *, model: AlexNet, optim_w: pxu.Optim, optim_h: pxu.Optim):\n",
    "    for x, y in dl:\n",
    "        train_on_batch(T, x, jax.nn.one_hot(y, model.nm_classes), model=model, optim_w=optim_w, optim_h=optim_h)\n",
    "\n",
    "\n",
    "def eval(dl, *, model: AlexNet):\n",
    "    acc = []\n",
    "    ys_ = []\n",
    "    \n",
    "    for x, y in dl:\n",
    "        a, y_ = eval_on_batch(x, y, model=model)\n",
    "        acc.append(a)\n",
    "        ys_.append(y_)\n",
    "    \n",
    "    return np.mean(acc), np.concatenate(ys_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "\n",
    "batch_size = 128\n",
    "nm_epochs = 24\n",
    "\n",
    "model = AlexNet(\n",
    "    nm_classes=10,\n",
    "    act_fn=jax.nn.gelu\n",
    ")\n",
    "    \n",
    "optim_h = pxu.Optim(optax.sgd(5e-2, momentum=0.5, nesterov=True))\n",
    "optim_w = pxu.Optim(optax.adamw(1e-4), pxu.M(pxnn.LayerParam)(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, test_dataloader = get_dataloaders(batch_size)\n",
    "\n",
    "for e in range(nm_epochs):\n",
    "    train(train_dataloader, T=13, model=model, optim_w=optim_w, optim_h=optim_h)\n",
    "    a, y = eval(test_dataloader, model=model)\n",
    "    print(f\"Epoch # {e + 1}/{nm_epochs} - Test Accuracy: {a * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to switch the training method to iPC, we can simply perform weight updates at every step, instead that only after T. In general, hybrid approaches are possible and can be described in terms of 3 values:\n",
    "- **T**: number of total iterations;\n",
    "- **T_0**: number of warmup steps;\n",
    "- **T_i**: number of inference steps between weight updates.\n",
    "\n",
    "In terms of these variables, *inference learning* would be (T, 0, T) and *iPC* (T, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pxf.jit(static_argnums=(0, 1, 2))\n",
    "def train_on_batch_hybrid(\n",
    "    T: int,\n",
    "    T_0: int,\n",
    "    T_i: int,\n",
    "    x: jax.Array,\n",
    "    y: jax.Array,\n",
    "    *,\n",
    "    model: AlexNet,\n",
    "    optim_w: pxu.Optim,\n",
    "    optim_h: pxu.Optim\n",
    "):\n",
    "    model.train()\n",
    "\n",
    "    # Init step\n",
    "    with pxu.step(model, pxc.STATUS.INIT, clear_params=pxc.VodeParam.Cache):\n",
    "        forward(x, y, model=model)\n",
    "    \n",
    "    optim_h.init(pxu.M_hasnot(pxc.VodeParam, frozen=True)(model))\n",
    "    \n",
    "    # Hybrid steps\n",
    "    for i in range(T):\n",
    "        with pxu.step(model, clear_params=pxc.VodeParam.Cache):\n",
    "            if i >= T_0 and (T_i == 0 or ((i - T_0) % T_i == 0)):\n",
    "                # gradient with respect of state h and weights\n",
    "                _, g = pxf.value_and_grad(\n",
    "                    (pxu.M_hasnot(pxc.VodeParam, frozen=True) | pxu.M(pxnn.LayerParam)).to([False, True]),\n",
    "                    has_aux=True\n",
    "                )(energy)(x, model=model)\n",
    "                \n",
    "                optim_w.step(model, g[\"model\"])\n",
    "            else:\n",
    "                # gradient only with respect of state h\n",
    "                _, g = pxf.value_and_grad(\n",
    "                    pxu.M_hasnot(pxc.VodeParam, frozen=True).to([False, True]),\n",
    "                    has_aux=True\n",
    "                )(energy)(x, model=model)\n",
    "        \n",
    "            optim_h.step(model, g[\"model\"])\n",
    "        \n",
    "    optim_h.clear()\n",
    "\n",
    "    # we always do a final weight update to avoid wasting any x updates.\n",
    "    with pxu.step(model, clear_params=pxc.VodeParam.Cache):\n",
    "        _, g = pxf.value_and_grad(pxu.M(pxnn.LayerParam).to([False, True]), has_aux=True)(energy)(x, model=model)\n",
    "    optim_w.step(model, g[\"model\"], scale_by=1.0/x.shape[0])\n",
    "\n",
    "\n",
    "def train_hybrid(dl, T, T_0, T_i, *, model: AlexNet, optim_w: pxu.Optim, optim_h: pxu.Optim):\n",
    "    for x, y in dl:\n",
    "        train_on_batch_hybrid(T, T_0, T_i, x, jax.nn.one_hot(y, model.nm_classes), model=model, optim_w=optim_w, optim_h=optim_h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the hyperparameters below don't work :( as training is not stable, but it's very fast in the first epochs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AlexNet(\n",
    "    nm_classes=10,\n",
    "    act_fn=jax.nn.leaky_relu\n",
    ")\n",
    "\n",
    "with pxu.step(model, pxc.STATUS.INIT, clear_params=pxc.VodeParam.Cache):\n",
    "    forward(jnp.zeros((batch_size, 3, 32, 32)), None, model=model)\n",
    "    \n",
    "    optim_h = pxu.Optim(optax.sgd(3e-1))\n",
    "    optim_w = pxu.Optim(optax.adamw(5e-5), pxu.M(pxnn.LayerParam)(model))\n",
    "\n",
    "train_dataloader, test_dataloader = get_dataloaders(batch_size)\n",
    "\n",
    "for e in range(nm_epochs):\n",
    "    train_hybrid(train_dataloader, T=11, T_0=1, T_i=0, model=model, optim_w=optim_w, optim_h=optim_h)\n",
    "    a, y = eval(test_dataloader, model=model)\n",
    "    print(f\"Epoch {e + 1}/{nm_epochs} - Test Accuracy: {a * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pcax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
