{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial #0: Predictive Coding Networks (PCNs)\n",
    "\n",
    "In this notebook we will see how to create and train a simple PCN to classify the two moons dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "# These are the default import names used in tutorials and documentation.\n",
    "import jax\n",
    "import jax.tree_util as jtu\n",
    "import jax.numpy as jnp\n",
    "import equinox as eqx\n",
    "\n",
    "import pcax as px\n",
    "import pcax.predictive_coding as pxc\n",
    "import pcax.nn as pxnn\n",
    "import pcax.functional as pxf\n",
    "import pcax.utils as pxu\n",
    "\n",
    "# px.RKG is the default key generator used in pcax, which is used as default\n",
    "# source of randomness within pcax. Here we set its seed to 0 for more reproducibility.\n",
    "# By default it is initialised with the system time.\n",
    "px.RKG.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create our model, which inherits from pxc.EnergyModule, so to have access to the notion\n",
    "# energy. The constructor takes in input all the hyperparameters of the model. Being static\n",
    "# values, if we intend to save them withing the model we must wrap them into a 'StaticParam'.\n",
    "class Model(pxc.EnergyModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        hidden_dim: int,\n",
    "        output_dim: int,\n",
    "        nm_layers: int,\n",
    "        act_fn: Callable[[jax.Array], jax.Array]\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.act_fn = px.static(act_fn)\n",
    "        \n",
    "        self.layers = [pxnn.Linear(input_dim, hidden_dim)] + [\n",
    "            pxnn.Linear(hidden_dim, hidden_dim) for _ in range(nm_layers - 2)\n",
    "        ] + [pxnn.Linear(hidden_dim, output_dim)]\n",
    "\n",
    "        # the default ruleset for a Vode is: `{\"STATUS.INIT\": (\"h, u <- u\",),}` which means:\n",
    "        # \"if the status is set to 'STATUS.INIT', everytime I set 'u', save that value not only\n",
    "        # in 'u', but also in 'x', which is exactly the behvaiour of a forward pass.\n",
    "        # By default if not specified, the behaviour is '* <- *', i.e., save everything passed\n",
    "        # to the vode via __call__ (remember vode(a) equals to vode.set(\"u\", a)).\n",
    "        #\n",
    "        # Since we are doing classification, we replace the last energy with the equivalent of\n",
    "        # cross entropy loss for predictive coding.\n",
    "        self.vodes = [\n",
    "            pxc.Vode((hidden_dim,)) for _ in range(nm_layers - 1)\n",
    "        ] + [pxc.Vode((output_dim,), pxc.ce_energy)]\n",
    "        \n",
    "        # 'frozen' is not a magic word, we define it here and use it later to distinguish between\n",
    "        # vodes we want to differentiate or not.\n",
    "        # NOTE: any attribute of a Param (except its value) is treated automatically as static,\n",
    "        # no need to specify it (but it's possible if you like more consistency,\n",
    "        # i.e., ...frozen = px.static(True)).\n",
    "        self.vodes[-1].h.frozen = True\n",
    "\n",
    "    def __call__(self, x, y):\n",
    "        for v, l in zip(self.vodes[:-1], self.layers[:-1]):\n",
    "            # remember 'x = v(a)' corresponds to v.set(\"u\", a); x = v.get(\"x\")\n",
    "            #\n",
    "            # note that 'self.act_fn' is a StaticParam, so to access it we would have to do\n",
    "            # self.act_fn.get()(...), however, all standard methods such as __call__ and\n",
    "            # __getitem__ are overloaded such that 'self.act_fn.__***__' becomes\n",
    "            # 'self.act_fn.get().__***__'\n",
    "            x = v(self.act_fn(l(x)))\n",
    "\n",
    "        x = self.vodes[-1](self.layers[-1](x))\n",
    "\n",
    "        if y is not None:\n",
    "            # if the target label is provided (e.g., during training), we save it to the last\n",
    "            # vode. Given that the 'froze' it, its value will not be upadated during inference,\n",
    "            # so we need to fix it only once for each new sample, usually during the init step.\n",
    "            self.vodes[-1].set(\"h\", y)\n",
    "\n",
    "        # at least with this architecture, the input activation of the last vode is the actual\n",
    "        # output of the model ('h' is fixed to the label during training or 'h = u' during eval)\n",
    "        return self.vodes[-1].get(\"u\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vmap is used to specify the batch dimension of the input data. Remember jax doesn't handle it\n",
    "# implicitly but relies on the user to explicitly tell it over which dimension to parallelise the\n",
    "# computation. That is, we always define a computational graph on a single sample, and then batch\n",
    "# the computation over the given mini-batch. We use the jax syntax for in_axes, out_axes, axis_name,\n",
    "# and the introduce a new parameter, kwargs_mask, to specify the batch information over the kwargs\n",
    "# (which, just as a reminder, have the property of being automatically tracked by pcax).\n",
    "# pxu.utils.mask has an in-depth explanation about how masking work. Here, we simply use the Mask\n",
    "# object, which, in this case, replaces every parameter that matches any of the given types with '0',\n",
    "# meaning that their value is batched over the 0th dimension (which is the case for the vode values\n",
    "# and caches), and with 'None' the non matching ones (such as the weights, which are shared across\n",
    "# different samples).\n",
    "# Both positional input arguments and output are batched over the 0th dimension, so we specify it.\n",
    "@pxf.vmap(pxu.Mask(pxc.VodeParam | pxc.VodeParam.Cache, (None, 0)), in_axes=(0, 0), out_axes=0)\n",
    "def forward(x, y, *, model: Model):\n",
    "    return model(x, y)\n",
    "\n",
    "# Similarly here, we specify 'out_axes=(None, 0)' since the function returns two values, the first\n",
    "# a single float storing the total energy of the model (not batched, but summed over the batch\n",
    "# dimension; this is a requirement of the gradient transformation, which jax requires taking a\n",
    "# scalar function in input and so a single scalar output). To follow on this, 'axis_name' is specified\n",
    "# so that we can return the sum over the batch dimension as required (this is standard jax syntax).\n",
    "@pxf.vmap(pxu.Mask(pxc.VodeParam | pxc.VodeParam.Cache, (None, 0)), in_axes=(0,), out_axes=(None, 0), axis_name=\"batch\")\n",
    "def energy(x, *, model: Model):\n",
    "    y_ = model(x, None)\n",
    "    return jax.lax.pmean(model.energy().sum(), \"batch\"), y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JIT is Just In Time compilation, which effectively compiles our code for fast CPU/GPU executioning\n",
    "# removing all python overhead.\n",
    "# 'T' is an hyperparameter that determines the number of inferences steps (and therefore the computational flow).\n",
    "# A such, it must be a static value. We can either specify it using 'static_argnums' (which however is only available\n",
    "# when using 'jit'), or pass it as a static parameter, in which case we would to 'train_on_batch(px.static(T), ...)'.\n",
    "#\n",
    "# Remember that pcax distinguishes between positional and keyword arguments, tracking only the parameters in latter ones.\n",
    "# Since we don't care about tracking of 'x' and 'y', we pass them as simple jax.Arrays as positional arguments. On the\n",
    "# other hand, both the model and the optimizers, may have parameters that are going to change and we want to track, so\n",
    "# we pass them as keyword arguments.\n",
    "@pxf.jit(static_argnums=0)\n",
    "def train_on_batch(\n",
    "    T: int,\n",
    "    x: jax.Array,\n",
    "    y: jax.Array,\n",
    "    *,\n",
    "    model: Model,\n",
    "    optim_w: pxu.Optim,\n",
    "    optim_h: pxu.Optim\n",
    "):\n",
    "    print(\"Training!\")  # this will come in handy later\n",
    "\n",
    "    # This only sets an internal flag to be \"train\" (instead of \"eval\")\n",
    "    model.train()\n",
    "    \n",
    "    # 'pxu.step' is an utility function that does two things:\n",
    "    # - sets the status to the provided one (default is 'None')\n",
    "    #   (and resets it to 'None' afterwards);\n",
    "    # - clears the target parameters if clear_params is specified\n",
    "    # (normally we want to clear the vode cache, such as activation and energy,\n",
    "    # after each step).\n",
    "    #\n",
    "    # pxc.STATUS.INIT triggers the only default vode ruleset defined, as\n",
    "    # previously explained.\n",
    "\n",
    "    # Init step\n",
    "    with pxu.step(model, pxc.STATUS.INIT, clear_params=pxc.VodeParam.Cache):\n",
    "        forward(x, y, model=model)\n",
    "    \n",
    "    # Inference steps\n",
    "    for _ in range(T):\n",
    "        with pxu.step(model, clear_params=pxc.VodeParam.Cache):\n",
    "            # 'm' is a masking object with a couple of useful methods to create complex masking functions.\n",
    "            # Here, we use it to target all VodeParams that are not forzen (again, frozen is a totally custom\n",
    "            # attribute we, as users, we decided to use above in the model and here).\n",
    "            #\n",
    "            # As jax expects, we distinguish between Parameters to differentiate ('True') and the rest ('False')\n",
    "            #\n",
    "            # 'e', 'y_' are the values returned by the 'energy' function defined above\n",
    "            (e, y_), g = pxf.value_and_grad(\n",
    "                pxu.Mask(pxu.m(pxc.VodeParam).has_not(frozen=True), [False, True]),\n",
    "                has_aux=True\n",
    "            )(energy)(x, model=model)\n",
    "        \n",
    "        # the returned gradient has the same structure of the function input. In this case, since we didn't use\n",
    "        # 'argnums' (jax argument of 'value_and_grad'), we only return the gradient with respect to the keyword\n",
    "        # arguments, that can be accessed as a dictionary. If we also had positional arguments gradients, we\n",
    "        # would have 'g = (positional_grad, keyword_grad)', so that, for example, the gradient of 'model' would\n",
    "        # be at 'g[1][\"model\"]'.\n",
    "        optim_h.step(model, g[\"model\"], True)\n",
    "\n",
    "    # Weight update step\n",
    "    with pxu.step(model, clear_params=pxc.VodeParam.Cache):\n",
    "        (e, y_), g = pxf.value_and_grad(pxu.Mask(pxnn.LayerParam, [False, True]), has_aux=True)(energy)(x, model=model)\n",
    "    optim_w.step(model, g[\"model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Not much to say here: we usa a single forward pass to compute the output of the model.\n",
    "# If we were to use a different initialisation, or a more complex architecture, we would have\n",
    "# to run inference to converge to some output value.\n",
    "@pxf.jit()\n",
    "def eval_on_batch(x: jax.Array, y: jax.Array, *, model: Model):\n",
    "    model.eval()\n",
    "    \n",
    "    with pxu.step(model, pxc.STATUS.INIT, clear_params=pxc.VodeParam.Cache):\n",
    "        y_ = forward(x, None, model=model).argmax(axis=-1)\n",
    "    \n",
    "    return (y_ == y).mean(), y_\n",
    "\n",
    "\n",
    "# Standard training loop\n",
    "def train(dl, T, *, model: Model, optim_w: pxu.Optim, optim_h: pxu.Optim):\n",
    "    for x, y in dl:\n",
    "        train_on_batch(T, x, jax.nn.one_hot(y, 2), model=model, optim_w=optim_w, optim_h=optim_h)\n",
    "\n",
    "# Standard evaluation loop\n",
    "def eval(dl, *, model: Model):\n",
    "    acc = []\n",
    "    ys_ = []\n",
    "    \n",
    "    for x, y in dl:\n",
    "        a, y_ = eval_on_batch(x, y, model=model)\n",
    "        acc.append(a)\n",
    "        ys_.append(y_)\n",
    "    \n",
    "    return np.mean(acc), np.concatenate(ys_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "model = Model(\n",
    "    input_dim=2,\n",
    "    hidden_dim=32,\n",
    "    output_dim=2,\n",
    "    nm_layers=3,\n",
    "    act_fn=jax.nn.leaky_relu\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only thing to note here is how optimizers are created. In particular,\n",
    "# we first want all the parameters of the model to exist, so that the optimizers\n",
    "# can capture them for optimization. This requires performing a dummy forward pass.\n",
    "# Note that the batch_size is an hyperparameter of the model and determines, among\n",
    "# other things, the shape of the Vode parameters, and thus must be kept as much\n",
    "# constant as possible (each change would trigger ricompilation of the jitted functions).\n",
    "with pxu.step(model, pxc.STATUS.INIT, clear_params=pxc.VodeParam.Cache):\n",
    "    forward(jax.numpy.zeros((batch_size, 2)), None, model=model)\n",
    "    \n",
    "    # 'pxu.Optim' accepts a optax optimizer and the parameters pytree in input. pxu.Mask\n",
    "    # can be used to partition between target parameters and not: when no 'map_to' is\n",
    "    # provided, such as here, it acts as 'eqx.partition', using pxc.VodeParam as filter.\n",
    "    optim_h = pxu.Optim(optax.sgd(1e-2 * batch_size), pxu.Mask(pxc.VodeParam)(model))\n",
    "    optim_w = pxu.Optim(optax.adamw(1e-2), pxu.Mask(pxnn.LayerParam)(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# this is unrelated to pcax: we generate and display the training set.\n",
    "nm_elements = 1024\n",
    "X, y = make_moons(n_samples=batch_size * (nm_elements // batch_size), noise=0.2, random_state=42)\n",
    "\n",
    "# Plot the dataset\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', edgecolor='k')\n",
    "plt.title(\"Two Moons Dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we split the dataset in training batches and do the same for the generated test set.\n",
    "train_dl = list(zip(X.reshape(-1, batch_size, 2), y.reshape(-1, batch_size)))\n",
    "\n",
    "X_test, y_test = make_moons(n_samples=batch_size * (nm_elements // batch_size) // 2, noise=0.2, random_state=0)\n",
    "test_dl = tuple(zip(X_test.reshape(-1, batch_size, 2), y_test.reshape(-1, batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "nm_epochs = 256 // (nm_elements // batch_size)\n",
    "\n",
    "# Note how the text \"Training!\" appears only once. This is because 'train_on_batch' is executed only once,\n",
    "# and then its compiled equivalent is instead used (which only cares about what happens to jax.Arrays and\n",
    "# discards all python code).\n",
    "\n",
    "for e in range(nm_epochs):\n",
    "    random.shuffle(train_dl)\n",
    "    train(train_dl, T=8, model=model, optim_w=optim_w, optim_h=optim_h)\n",
    "    a, y = eval(test_dl, model=model)\n",
    "    \n",
    "    # We print the average shift of the first vode during the inference steps. Note that it does not depend on\n",
    "    # the choice for the batch_size (feel free to play around with it, remember to reset the notebook if you\n",
    "    # you change it). This is because we multiply the learning rate of 'optim_h' by the batch_size. This is \n",
    "    # because the total energy is averaged over the batch dimension (as required for the weight updates),\n",
    "    # so we need to scale the learning rate accordingly for the vode updates.\n",
    "    print(f\"Epoch {e + 1}/{nm_epochs} - Test Accuracy: {a * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pcax.utils contains a couple of useful functions to save and load the parameters of a model.\n",
    "# They allow to define which subset of the parameters to save, and to load them back into the model.\n",
    "# The default behaviour is to save all the weights (i.e., values contained in 'pxnn.LayerParam') of\n",
    "# the model and ignore any 'Vode' value.\n",
    "\n",
    "import os\n",
    "\n",
    "# We check what is inside the model.\n",
    "print(model)\n",
    "\n",
    "# save/load the model\n",
    "pxu.save_params(model, \"model\")\n",
    "pxu.load_params(model, \"model\")\n",
    "\n",
    "# Remove the saved model file\n",
    "os.remove(\"model.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we evaluate all the grid set as a single batch with 96^2 elements. If we were to directly\n",
    "# call 'forward' we would get an error as the size of the batch dimension do not agree:\n",
    "# 'model' contains VodeParams whose batch size is 32 as previously defined, while the function\n",
    "# input X_grid would have a batch size of 96^2.\n",
    "# In order to solve the problem, we first clear all VodeParams (replacing them with None, which\n",
    "# is ignored by jax) so that jax sees a single size for the batch dimension (i.e., 96^2) and works\n",
    "# without any problem.\n",
    "\n",
    "model.clear_params(pxc.VodeParam)\n",
    "\n",
    "# Test the model on the grid of points in the range [-2.5, 2.5]x[-2.5, 2.5]\n",
    "X_grid = jax.numpy.stack(np.meshgrid(np.linspace(-2.5, 2.5, 96), np.linspace(-2.0, 2.0, 96))).reshape(2, -1).T\n",
    "with pxu.step(model, pxc.STATUS.INIT, clear_params=pxc.VodeParam.Cache):\n",
    "    y_grid = forward(X_grid, None, model=model).argmax(axis=-1)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.scatter(X_grid[:, 0], X_grid[:, 1], c=y_grid, cmap='viridis', s=14, marker='o', linewidths=0, alpha=0.2)\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], c=y, cmap='viridis', edgecolor='k')\n",
    "plt.title(\"Prediction on Two Moons Dataset\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
