{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial #5: Decoder-only PCN\n",
    "\n",
    "In this notebook we will see how to code a decoder-only PCN and train it to generate FashionMNIST images.\n",
    "We will use PyTorch to handle the dataset and the dataloading, so make sure it is installed in the local environment. We only need the CPU version which, currently, can be installed via `pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use this tutorial to also introduce the concept of *ruleset*. In PCX, to configure the behaviour of a Vode is not always necessary to inherit from the base class, as a lot can be configured via the class constructur. In particular, a ruleset allows us to specify the node behaviour for input and output values (i.e., what transformations to apply to incoming activation and outgoing state values). A detailed explanation is provided within the library, as a comment of the ruleset class, so please refer to that.\n",
    "In summary, we have the following:\n",
    "- **input rules** are rules that follow the pattern `t1, t2, ... <- v:f1:f2:...`. Every time the activation `v` is set it is tranformed by all `f_i` and then saved to all `t_j` in the cache. The default behavior is clearly `v <- v` (i.e., a received activation is cached as it is). By default, each node uses the following ruleset: `STATUS.INIT: (\"h, u <- u\",)` which specifies that, if the status is set to `pxc.STATUS.INIT` the incoming activation `u` is not only saved to `u` but also to `h` (setting `h = u` which is forward initialisation)\n",
    "- **output rules** are rules that follow the pattern `k -> t:f1:f2:...`. Similarly to input rules, every time the state value `k` is queried, the result of applying each `f_i` to the state value `t` is instead returned. Note that the result is cached, so subsequent calls to the same `k` will reuse the already computed value, unless the cache is cleared (this is why we specify `clear_params=pxc.VodeParam.Cache` in `pxu.step`). This is convenient because, for example, the energy is cached as `\"E\"` (which is thus a reserved value) and can be queried outside the loss function without performing new computations.\n",
    "\n",
    "In this example, we use input rules to override the forward initialisation and initialise nodes to 0s instead (as we do not have a value to forward). Similarly, we define a new status `STATUS_FORWARD` that allows us to perform a forward pass by defining the output rule `h -> u` according to which every time we query `h` we get `u` instead (basically ignoring the Vode altogether).\n",
    "Each set of rules is associated to a status (or multiple of them) via its key (such as `STATUS_FORWARD: ...`). The key can be a regex to match multiple statuses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies not included in the base requirements.txt\n",
    "\n",
    "# linux\n",
    "# !pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu\n",
    "\n",
    "# windows\n",
    "!pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "# Core dependencies\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# pcx\n",
    "import pcx as px\n",
    "import pcx.predictive_coding as pxc\n",
    "import pcx.nn as pxnn\n",
    "import pcx.utils as pxu\n",
    "\n",
    "STATUS_FORWARD = \"forward\"\n",
    "\n",
    "\n",
    "class Decoder(pxc.EnergyModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        hidden_dim: int,\n",
    "        output_dim: int,\n",
    "        nm_layers: int,\n",
    "        act_fn: Callable[[jax.Array], jax.Array],\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.act_fn = px.static(act_fn)\n",
    "\n",
    "        self.layers = (\n",
    "            [pxnn.Linear(input_dim, hidden_dim)]\n",
    "            + [pxnn.Linear(hidden_dim, hidden_dim) for _ in range(nm_layers - 2)]\n",
    "            + [pxnn.Linear(hidden_dim, output_dim)]\n",
    "        )\n",
    "\n",
    "        # We initialise the first node to zero.\n",
    "        # We use 'zero_energy' as we do not want any prior on the first layer.\n",
    "        self.vodes = (\n",
    "            [\n",
    "                pxc.Vode(\n",
    "                    energy_fn=None,\n",
    "                    ruleset={pxc.STATUS.INIT: (\"h, u <- u:to_zero\",)},\n",
    "                    tforms={\"to_zero\": lambda n, k, v, rkg: jnp.zeros((input_dim,))},\n",
    "                )\n",
    "            ]\n",
    "            + [\n",
    "                # we stick with default forward initialisation for now for the remaining nodes,\n",
    "                # however we enable a \"forward mode\" where we forward the incoming activation instead\n",
    "                # of the node state; this is used during evaluation to generate the encoded output.\n",
    "                pxc.Vode(\n",
    "                    ruleset={\n",
    "                        pxc.STATUS.INIT: (\"h, u <- u:to_zero\",),\n",
    "                        STATUS_FORWARD: (\"h -> u\",)\n",
    "                    },\n",
    "                    tforms={\"to_zero\": lambda n, k, v, rkg: jnp.zeros_like(v)},\n",
    "                )\n",
    "                for _ in range(nm_layers - 1)\n",
    "            ]\n",
    "            + [pxc.Vode()]\n",
    "        )\n",
    "        self.vodes[-1].h.frozen = True\n",
    "\n",
    "    def __call__(self, y: jax.Array | None):\n",
    "        # The defined ruleset for the first vode is to set the hidden state to zero,\n",
    "        # independent of the input, so we always pass '-1' (as None would skip the computation).\n",
    "        x = self.vodes[0](jnp.empty(()))\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            act_fn = self.act_fn if i != len(self.layers) - 1 else lambda x: x\n",
    "            x = act_fn(layer(x))\n",
    "            x = self.vodes[i + 1](x)\n",
    "\n",
    "        if y is not None:\n",
    "            self.vodes[-1].set(\"h\", y.flatten())\n",
    "\n",
    "        return self.vodes[-1].get(\"u\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# The dataloader assumes cuda is being used, as such it sets 'pin_memory = True' and\n",
    "# 'prefetch_factor = 2'. Note that the batch size should be constant during training, so\n",
    "# we set 'drop_last = True' to avoid having to deal with variable batch sizes.\n",
    "class TorchDataloader(torch.utils.data.DataLoader):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset,\n",
    "        batch_size=1,\n",
    "        shuffle=None,\n",
    "        sampler=None,\n",
    "        batch_sampler=None,\n",
    "        num_workers=1,\n",
    "        pin_memory=True,\n",
    "        timeout=0,\n",
    "        worker_init_fn=None,\n",
    "        persistent_workers=True,\n",
    "        prefetch_factor=2,\n",
    "    ):\n",
    "        super(self.__class__, self).__init__(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            sampler=sampler,\n",
    "            batch_sampler=batch_sampler,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=pin_memory,\n",
    "            drop_last=True if batch_sampler is None else None,\n",
    "            timeout=timeout,\n",
    "            worker_init_fn=worker_init_fn,\n",
    "            persistent_workers=persistent_workers,\n",
    "            prefetch_factor=prefetch_factor,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "def get_dataloaders(batch_size: int):\n",
    "    t = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor()\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    train_dataset = torchvision.datasets.FashionMNIST(\n",
    "        \"~/tmp/fashion-mnist/\",\n",
    "        transform=t,\n",
    "        download=True,\n",
    "        train=True,\n",
    "    )\n",
    "\n",
    "    train_dataloader = TorchDataloader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "    )\n",
    "\n",
    "    test_dataset = torchvision.datasets.FashionMNIST(\n",
    "        \"~/tmp/fashion-mnist/\",\n",
    "        transform=t,\n",
    "        download=True,\n",
    "        train=False,\n",
    "    )\n",
    "\n",
    "    test_dataloader = TorchDataloader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "    )\n",
    "\n",
    "    return train_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pcx.functional as pxf\n",
    "\n",
    "\n",
    "@pxf.vmap(pxu.M(pxc.VodeParam | pxc.VodeParam.Cache).to((None, 0)), in_axes=0, out_axes=0)\n",
    "def forward(x, *, model: Decoder):\n",
    "    return model(x)\n",
    "\n",
    "\n",
    "@pxf.vmap(pxu.M(pxc.VodeParam | pxc.VodeParam.Cache).to((None, 0)), out_axes=(None, 0), axis_name=\"batch\")\n",
    "def energy(*, model: Decoder):\n",
    "    y_ = model(None)\n",
    "    return jax.lax.psum(model.energy(), \"batch\"), y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pxf.jit(static_argnums=0)\n",
    "def train_on_batch(T: int, x: jax.Array, *, model: Decoder, optim_w: pxu.Optim, optim_h: pxu.Optim):\n",
    "    model.train()\n",
    "\n",
    "    inference_step = pxf.value_and_grad(pxu.M_hasnot(pxc.VodeParam, frozen=True).to([False, True]), has_aux=True)(\n",
    "        energy\n",
    "    )\n",
    "\n",
    "    learning_step = pxf.value_and_grad(pxu.M_hasnot(pxnn.LayerParam).to([False, True]), has_aux=True)(energy)\n",
    "\n",
    "    # Init step\n",
    "    with pxu.step(model, pxc.STATUS.INIT, clear_params=pxc.VodeParam.Cache):\n",
    "        forward(x, model=model)\n",
    "\n",
    "    optim_h.init(pxu.M_hasnot(pxc.VodeParam, frozen=True)(model))\n",
    "\n",
    "    # Inference steps\n",
    "    for _ in range(T):\n",
    "        with pxu.step(model, clear_params=pxc.VodeParam.Cache):\n",
    "            _, g = inference_step(model=model)\n",
    "\n",
    "        optim_h.step(model, g[\"model\"])\n",
    "    \n",
    "    optim_h.clear()\n",
    "\n",
    "    # Learning step\n",
    "    with pxu.step(model, clear_params=pxc.VodeParam.Cache):\n",
    "        _, g = learning_step(model=model)\n",
    "    optim_w.step(model, g[\"model\"], scale_by=1.0/x.shape[0])\n",
    "\n",
    "\n",
    "@pxf.jit(static_argnums=0)\n",
    "def eval_on_batch(T: int, x: jax.Array, *, model: Decoder, optim_h: pxu.Optim):\n",
    "    model.eval()\n",
    "\n",
    "    inference_step = pxf.value_and_grad(pxu.M_hasnot(pxc.VodeParam, frozen=True).to([False, True]), has_aux=True)(\n",
    "        energy\n",
    "    )\n",
    "\n",
    "    # Init step\n",
    "    with pxu.step(model, pxc.STATUS.INIT, clear_params=pxc.VodeParam.Cache):\n",
    "        forward(x, model=model)\n",
    "    \n",
    "    optim_h.init(pxu.M_hasnot(pxc.VodeParam, frozen=True)(model))\n",
    "\n",
    "    # Inference steps\n",
    "    for _ in range(T):\n",
    "        with pxu.step(model, clear_params=pxc.VodeParam.Cache):\n",
    "            _, g = inference_step(model=model)\n",
    "\n",
    "        optim_h.step(model, g[\"model\"])\n",
    "    \n",
    "    optim_h.clear()\n",
    "\n",
    "\n",
    "    with pxu.step(model, STATUS_FORWARD, clear_params=pxc.VodeParam.Cache):\n",
    "        x_hat = forward(None, model=model)\n",
    "\n",
    "    loss = jnp.square(jnp.clip(x_hat.flatten(), 0.0, 1.0) - x.flatten()).mean()\n",
    "\n",
    "    return loss, x_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dl, T, *, model: Decoder, optim_w: pxu.Optim, optim_h: pxu.Optim):\n",
    "    for x, y in dl:\n",
    "        train_on_batch(T, x.numpy(), model=model, optim_w=optim_w, optim_h=optim_h)\n",
    "\n",
    "\n",
    "def eval(dl, T, *, model: Decoder, optim_h: pxu.Optim):\n",
    "    losses = []\n",
    "\n",
    "    for x, y in dl:\n",
    "        e, y_hat = eval_on_batch(T, x.numpy(), model=model, optim_h=optim_h)\n",
    "        losses.append(e)\n",
    "\n",
    "    return np.mean(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "\n",
    "batch_size = 128\n",
    "nm_epochs = 24\n",
    "\n",
    "model = Decoder(input_dim=64, hidden_dim=256, output_dim=28 * 28, nm_layers=4, act_fn=jax.nn.tanh)\n",
    "\n",
    "optim_h = pxu.Optim(lambda: optax.sgd(5e-2, momentum=0.1))\n",
    "optim_w = pxu.Optim(lambda: optax.adamw(1e-4), pxu.M(pxnn.LayerParam)(model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, test_dataloader = get_dataloaders(batch_size)\n",
    "\n",
    "for e in range(nm_epochs):\n",
    "    train(train_dataloader, T=20, model=model, optim_w=optim_w, optim_h=optim_h)\n",
    "    l = eval(test_dataloader, T=20, model=model, optim_h=optim_h)\n",
    "    print(f\"Epoch {e + 1}/{nm_epochs} - Test Loss: {l:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pcax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
