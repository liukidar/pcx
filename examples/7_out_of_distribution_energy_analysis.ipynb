{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we analyse how the energy in a MLP neural network (trained with PC) could be used to assess if \n",
    "test data belongs to in-distribution or out-of-distribution. We will use the energy of the network as a measure of\n",
    "surprise.\n",
    "\n",
    "1. Prepare the in-distribution (MNIST) and out-of-distribution (FashionMNIST) datasets.\n",
    "    - note that the interpolation threshold for regular trainset size of (n=60000) would be (with K=10): I_th = n*K = 600 000 samples,\n",
    "    one would need a 3-layer-MLP with 800 hidden nodes (h=800) to have approx. 1.2x10^6 parameters and thus be twice as big as the interpolation threshold \n",
    "    in order to guarantee not fitting a model which might below the interpolation threshold I_th or in the \"classical\" under-parameterized regime but \n",
    "    instead ensure to fit a model in the \"modern\" over-parameterized regime. We achieve this by a) training for long enough and b) ensuring that the model \n",
    "    has the appropriate capacity (i.e., number of parameters) which we control with h (i.e., the number of hidden nodes in each layer).\n",
    "    - given that the complete training data (n=60000) size would require unnecessary big neural networks (increased compute time due to increased \n",
    "    hidden size h) we will use a subset of the complete training data (n_subset_size=) to make it sufficient to train networks with half the amount of \n",
    "    samples which will give use a reduce interpolation threshold of I_th (= 300 000 samples). Given fixed K (K=10 classes) we can use a \n",
    "    training subset_size n_subset_size of 30 000 samples. In this case, the required hidden size h to achieve a 3-layer-MLP with at least twice as many \n",
    "    parameters as the chosen training set size would be approx. h≈512 - which would give use a neural network with approx. 6.5x10^5 (650 000) parameters \n",
    "    (so roughly half, 6x10^5 parameters, of what we required above).\n",
    "2. Training phase: Train a MLP neural network with PC on MNIST.\n",
    "3. Testing phase: Test the network with MNIST and FashionMNIST.\n",
    "4. Plots and tables:\n",
    "    1. Plot the energy distribution histograms to visualize the distribution and spread of the energy for in-distribution and out-of-distribution \n",
    "    data, use separate histograms for each dataset and overlay them for comparison.\n",
    "    2. Create box plots to show the median, quartiles, and outliers of the energies to summarize the central tendency and variability of energies.\n",
    "    3. Generate a Receiver Operating Characteristic (ROC) curve based on the energies to assess the model's ability to distinguish between \n",
    "    in-distribution and out-of-distribution data. Plot true positive rate (sensitivity) against false positive rate (1-specificity) for different \n",
    "    threshold values of energies.\n",
    "    4. Create a table summarizing key statistics (mean, median, standard deviation) of energies for MNIST and FashionMNIST. Include columns for \n",
    "    MNIST and FashionMNIST with rows for each statistic.\n",
    "    5. Create a scatter plot of energies versus prediction confidence (max softmax value) for both MNIST and FashionMNIST. Goal is to investigate \n",
    "    the relationship/correlation between model confidence and energy value. Use different colors or markers for MNIST and FashionMNIST data points.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# built-in\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "# choose the GPU to use\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "# disable preallocation of memory\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\n",
    "\n",
    "# 3rd party\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# own\n",
    "from helpers import get_dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################### start of model related code ##########################################\n",
    "\n",
    "from typing import Callable\n",
    "\n",
    "# Core dependencies\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import equinox as eqx\n",
    "import optax\n",
    "print(jax.default_backend())  # print backend type\n",
    "\n",
    "# pcax\n",
    "import pcax as px\n",
    "import pcax.predictive_coding as pxc\n",
    "import pcax.nn as pxnn\n",
    "import pcax.functional as pxf\n",
    "import pcax.utils as pxu\n",
    "\n",
    "\n",
    "# Model definition\n",
    "class Model(pxc.EnergyModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        hidden_dim: int,\n",
    "        output_dim: int,\n",
    "        nm_layers: int,\n",
    "        act_fn: Callable[[jax.Array], jax.Array]\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self._dim_input = input_dim\n",
    "        self._dim_output = output_dim\n",
    "        \n",
    "        self.act_fn = px.static(act_fn)\n",
    "\n",
    "        self.layers = [pxnn.Linear(input_dim, hidden_dim)] + [\n",
    "            pxnn.Linear(hidden_dim, hidden_dim) for _ in range(nm_layers - 2)\n",
    "        ] + [pxnn.Linear(hidden_dim, output_dim)]\n",
    "        \n",
    "        self.vodes = [\n",
    "            pxc.Vode((hidden_dim,)) for _ in range(nm_layers - 1)\n",
    "        ] + [pxc.Vode((output_dim,), pxc.ce_energy)]\n",
    "\n",
    "        self.vodes[-1].h.frozen = True\n",
    "\n",
    "    def num_parameters(self):\n",
    "        \"\"\"\n",
    "        Calculate the total number of parameters in the model.\n",
    "        Args:\n",
    "            model: The model object containing layers with weights and biases.\n",
    "        Returns:\n",
    "            int: Total number of parameters in the model.\n",
    "        \"\"\"\n",
    "        return sum(layer.nn.weight.size + layer.nn.bias.size for layer in self.layers)\n",
    "\n",
    "    def save(self, file_name):\n",
    "        pxu.save_params(self, file_name)\n",
    "\n",
    "\n",
    "    def __call__(self, x, y):\n",
    "        for v, l in zip(self.vodes[:-1], self.layers[:-1]):\n",
    "            x = v(self.act_fn(l(x)))\n",
    "        x = self.vodes[-1](self.layers[-1](x))\n",
    "        \n",
    "        if y is not None:\n",
    "            self.vodes[-1].set(\"h\", y)\n",
    "        return self.vodes[-1].get(\"u\")\n",
    "\n",
    "\n",
    "# Training and evaluation functions\n",
    "@pxf.vmap(pxu.Mask(pxc.VodeParam | pxc.VodeParam.Cache, (None, 0)), in_axes=(0, 0), out_axes=0)\n",
    "def forward(x, y, *, model: Model):\n",
    "    return model(x, y)\n",
    "\n",
    "@pxf.vmap(pxu.Mask(pxc.VodeParam | pxc.VodeParam.Cache, (None, 0)), in_axes=(0,), out_axes=(None, 0), axis_name=\"batch\")\n",
    "def energy(x, *, model: Model):\n",
    "    y_ = model(x, None)\n",
    "    return jax.lax.pmean(model.energy().sum(), \"batch\"), y_\n",
    "\n",
    "@pxf.jit(static_argnums=0)\n",
    "def train_on_batch(T: int, x: jax.Array, y: jax.Array, *, model: Model, optim_w: pxu.Optim, optim_h: pxu.Optim):\n",
    "    model.train()\n",
    "    with pxu.step(model, pxc.STATUS.INIT, clear_params=pxc.VodeParam.Cache):\n",
    "        forward(x, y, model=model)\n",
    "\n",
    "    for _ in range(T):\n",
    "        with pxu.step(model, clear_params=pxc.VodeParam.Cache):\n",
    "            _, g = pxf.value_and_grad(pxu.Mask(pxu.m(pxc.VodeParam).has_not(frozen=True), [False, True]), has_aux=True)(energy)(x, model=model)\n",
    "        optim_h.step(model, g[\"model\"], True)\n",
    "\n",
    "    with pxu.step(model, clear_params=pxc.VodeParam.Cache):\n",
    "        _, g = pxf.value_and_grad(pxu.Mask(pxnn.LayerParam, [False, True]), has_aux=True)(energy)(x, model=model)\n",
    "    optim_w.step(model, g[\"model\"])\n",
    "\n",
    "@pxf.jit()\n",
    "def eval_on_batch(x: jax.Array, y: jax.Array, *, model: Model):\n",
    "    model.eval()\n",
    "    with pxu.step(model, pxc.STATUS.INIT, clear_params=pxc.VodeParam.Cache):\n",
    "        y_ = forward(x, jax.nn.one_hot(y, 10), model=model).argmax(axis=-1)\n",
    "        e = model.vodes[-1].energy()\n",
    "    return (y_ == y).mean(), y_, e.mean()\n",
    "\n",
    "@pxf.jit()\n",
    "def predict_with_logits_on_batch(x: jax.Array, *, model: Model):\n",
    "    model.eval()\n",
    "    with pxu.step(model, pxc.STATUS.INIT, clear_params=pxc.VodeParam.Cache):\n",
    "        logits_ = forward(x, None, model=model)\n",
    "        y_ = logits_.argmax(axis=-1)\n",
    "    # print the shape of the logits and the y_ to debug\n",
    "    print(f\"logits shape: {logits_.shape}, y_ shape: {y_.shape}\")  # Debugging line\n",
    "    return y_, logits_\n",
    "\n",
    "def train(dl, T, *, model: Model, optim_w: pxu.Optim, optim_h: pxu.Optim):\n",
    "    for x, y in dl:\n",
    "        train_on_batch(T, x, jax.nn.one_hot(y, 10), model=model, optim_w=optim_w, optim_h=optim_h)\n",
    "    print(f\"x shape: {x.shape}, y shape: {y.shape}\")  # Debugging line\n",
    "      \n",
    "def eval(dl, *, model: Model):\n",
    "    acc = []\n",
    "    es = []\n",
    "    ys_ = []\n",
    "    for x, y in dl:\n",
    "        a, y_, e = eval_on_batch(x, y, model=model)\n",
    "        acc.append(a)\n",
    "        es.append(e)\n",
    "        ys_.append(y_)\n",
    "    return float(np.mean(acc)), np.concatenate(ys_), float(np.mean(es))\n",
    "\n",
    "def predict_with_logits(dl, *, model: Model):\n",
    "    ys_ = []\n",
    "    logits_ = []\n",
    "    for x, _ in dl:\n",
    "        y_, logits = predict_with_logits_on_batch(x, model=model)\n",
    "        ys_.append(y_)\n",
    "        logits_.append(logits)\n",
    "    return np.concatenate(ys_), np.concatenate(logits_)\n",
    "\n",
    "###################################### end of model related code ##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"mnist\"\n",
    "train_subset_size = 30000\n",
    "batch_size = 128\n",
    "noise_level = 0.0 # this means that the validation set will be 20% of the train_subset_size (6000 samples)\n",
    "# get the dataloaders\n",
    "dataset = get_dataloaders(dataset_name, train_subset_size, batch_size, noise_level)\n",
    "# Check the sizes of the datasets\n",
    "print(f\"Training set: {len(dataset.train_loader.sampler)} samples\")\n",
    "print(f\"Validation set: {len(dataset.val_loader.sampler)} samples\")\n",
    "print(f\"Test set: {len(dataset.test_loader.dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_learning_rate = 1e-2\n",
    "h_learning_rate = 1e-2\n",
    "T = 10\n",
    "\n",
    "# create a dummy model to see if the model is working\n",
    "model = Model(\n",
    "    input_dim=784,\n",
    "    hidden_dim=512,\n",
    "    output_dim=10,\n",
    "    nm_layers=3,\n",
    "    act_fn=jax.nn.gelu\n",
    ")\n",
    "\n",
    "# print the model\n",
    "print(model)\n",
    "# now print number of parameters in the model\n",
    "print(f\"Number of parameters: {model.num_parameters()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nm_epochs = 5\n",
    "\n",
    "# Initialize the model and optimizers\n",
    "with pxu.step(model, pxc.STATUS.INIT, clear_params=pxc.VodeParam.Cache):\n",
    "    forward(jax.numpy.zeros((batch_size, model._dim_input)), None, model=model)\n",
    "    optim_h = pxu.Optim(optax.sgd(h_learning_rate), pxu.Mask(pxc.VodeParam)(model))\n",
    "    optim_w = pxu.Optim(optax.sgd(w_learning_rate, momentum=0.95), pxu.Mask(pxnn.LayerParam)(model))\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "for e in range(nm_epochs):\n",
    "\n",
    "    # train the model\n",
    "    train(dataset.train_loader, T=T, model=model, optim_w=optim_w, optim_h=optim_h)\n",
    "    \n",
    "    # evaluate the model and get accuracies and losses\n",
    "    a_train, ys_train, e_train = eval(dataset.train_loader, model=model)\n",
    "    a_val, ys_val, e_val = eval(dataset.val_loader, model=model)\n",
    "\n",
    "    train_losses.append(e_train)\n",
    "    val_losses.append(e_val)\n",
    "    train_accuracies.append(a_train)\n",
    "    val_accuracies.append(a_val)\n",
    "\n",
    "    print(f\"Epoch {e+1}/{nm_epochs} - Train Acc: {a_train:.4f} - Val Acc: {a_val:.4f} - Train Loss: {e_train:.4f} - Val Loss: {e_val:.4f}\")\n",
    "\n",
    "# clear VodeParams\n",
    "model.clear_params(pxc.VodeParam) # we need to do this because the batch_size of test set is different than the batch_size of the training set, i.e., batch_size of test set is entire test set (10000 samples)\n",
    "\n",
    "a_test, ys_test, e_test = eval(dataset.test_loader, model=model)\n",
    "print(f\"Test Acc: {a_test:.4f} - Test Loss: {e_test:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming train_losses, val_losses, train_accuracies, val_accuracies are defined\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "# Plot training and validation losses\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('Epoch', fontsize=16)\n",
    "ax1.set_ylabel('Loss', fontsize=16, color=color)\n",
    "ax1.plot(train_losses, color=color, label=\"Train Loss\", linewidth=2)\n",
    "ax1.plot(val_losses, color=\"orange\", label=\"Val Loss\", linewidth=2)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "ax1.legend(loc=\"upper left\", fontsize=14)\n",
    "ax1.grid(True)\n",
    "\n",
    "# Create a twin Axes sharing the x-axis for accuracies\n",
    "ax2 = ax1.twinx()\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('Accuracy', fontsize=16, color=color)\n",
    "ax2.plot(train_accuracies, color=color, linestyle=\"--\", label=\"Train Acc\", linewidth=2)\n",
    "ax2.plot(val_accuracies, color=\"green\", linestyle=\"--\", label=\"Val Acc\", linewidth=2)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "ax2.legend(loc=\"upper right\", fontsize=14)\n",
    "\n",
    "# Title and layout adjustments\n",
    "plt.title('Training and Validation Losses and Accuracies', fontsize=20)\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear VodeParams\n",
    "model.clear_params(pxc.VodeParam) # we need to do this because the batch_size of test set is different than the batch_size of the training set, i.e., batch_size of test set is entire test set (10000 samples)\n",
    "\n",
    "# now compute the prediction and logits for the test set\n",
    "ys_test, logits_test = predict_with_logits(dataset.test_loader, model=model)\n",
    "\n",
    "# now print the shape of the logits_test and the ys_test\n",
    "print(f\"logits_test shape: {logits_test.shape}, ys_test shape: {ys_test.shape}\")\n",
    "\n",
    "# show the first row of the logits_test and the ys_test\n",
    "print(f\"logits_test[0]: {logits_test[0]}\")\n",
    "print()\n",
    "print(f\"ys_test[0]: {ys_test[0]}\")\n",
    "\n",
    "# now compute the softmax of the logits_test for the first row\n",
    "logits_test_softmax = jax.nn.softmax(logits_test[0])\n",
    "print(f\"logits_test_softmax[0]: {logits_test_softmax}\")\n",
    "\n",
    "# now compute the sum of the logits_test_softmax\n",
    "print(f\"sum of the logits_test_softmax[0]: {logits_test_softmax.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming `dataset.test_loader` is your DataLoader\n",
    "test_loader = dataset.test_loader\n",
    "\n",
    "# Initialize lists to collect data\n",
    "X_test_list = []\n",
    "y_test_list = []\n",
    "\n",
    "# Iterate over the DataLoader\n",
    "for batch in test_loader:\n",
    "    # Assuming each batch is a tuple (inputs, targets)\n",
    "    inputs, targets = batch\n",
    "    \n",
    "    # Directly append the numpy arrays to the lists\n",
    "    X_test_list.append(inputs)\n",
    "    y_test_list.append(targets)\n",
    "\n",
    "# Concatenate all batches to form the complete test set\n",
    "X_test_dl = np.concatenate(X_test_list, axis=0)\n",
    "y_test_dl = np.concatenate(y_test_list, axis=0)\n",
    "\n",
    "# Print shapes to verify\n",
    "print(f\"X_test_dl shape: {X_test_dl.shape}\")\n",
    "print(f\"y_test_dl shape: {y_test_dl.shape}\")\n",
    "\n",
    "# compare the difference between first sample from test_loader and first sample from X_test_dl by computing the mean and variance for each image\n",
    "print(f\"mean of the first image from test_loader: {dataset.test_loader.dataset.data[0].numpy().mean()}\")\n",
    "print(f\"variance of the first image from test_loader: {dataset.test_loader.dataset.data[0].numpy().var()}\")\n",
    "print()\n",
    "print(f\"mean of the first image from X_test_dl: {X_test_dl[0].mean()}\")\n",
    "print(f\"variance of the first image from X_test_dl: {X_test_dl[0].var()}\")\n",
    "\n",
    "# we see that the mean and variance of the first image from test_loader and X_test_dl are different because dataset.test_loader.dataset.data returns the original data without any transformation, but X_test_dl is the transformed data (i.e., the data is normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pcax24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
