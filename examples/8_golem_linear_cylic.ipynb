{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/amine.mcharrak/miniconda3/envs/pcax24/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final sys.path:\n",
      "/home/amine.mcharrak/frp/dglearn\n",
      "/home/amine.mcharrak\n",
      "/home/amine.mcharrak/causality-lab\n",
      "/home/amine.mcharrak/dodiscover\n",
      "/home/amine.mcharrak/cosmo\n",
      "/home/amine.mcharrak/cadimulc\n",
      "/share/amine.mcharrak/miniconda3/envs/pcax24/lib/python310.zip\n",
      "/share/amine.mcharrak/miniconda3/envs/pcax24/lib/python3.10\n",
      "/share/amine.mcharrak/miniconda3/envs/pcax24/lib/python3.10/lib-dynload\n",
      "\n",
      "/share/amine.mcharrak/miniconda3/envs/pcax24/lib/python3.10/site-packages\n",
      "/home/amine.mcharrak/pcax\n",
      "CyclicManager is imported from: /home/amine.mcharrak/frp/frp_dglearn/learning/cyclic_manager.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting CUDA device(s) : [1]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# choose the GPU to use\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "# disable preallocation of memory\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\n",
    "\n",
    "# pcx\n",
    "import pcx as px\n",
    "import pcx.predictive_coding as pxc\n",
    "import pcx.nn as pxnn\n",
    "import pcx.functional as pxf\n",
    "import pcx.utils as pxu\n",
    "\n",
    "# 3rd party\n",
    "import jax\n",
    "from jax import jit\n",
    "import jax.numpy as jnp\n",
    "import jax.numpy.linalg as jax_la\n",
    "import jax.random as random\n",
    "import optax\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.api.types import is_float_dtype\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import timeit\n",
    "\n",
    "# own\n",
    "import causal_helpers\n",
    "from causal_helpers import simulate_dag, simulate_parameter, simulate_linear_sem, simulate_linear_sem_cyclic\n",
    "from causal_helpers import load_adjacency_matrix, set_random_seed, plot_adjacency_matrices\n",
    "from causal_helpers import load_graph, load_adjacency_matrix\n",
    "from causal_metrics import compute_F1_directed, compute_F1_skeleton, compute_AUPRC, compute_AUROC, compute_model_fit\n",
    "from causal_metrics import compute_cycle_F1, compute_cycle_SHD, compute_cycle_KLD, compute_CSS  # CSS: Cyclic Structure Score\n",
    "from connectome_cyclic_data_generator import sample_cyclic_data\n",
    "\n",
    "#################### NODAGS #########################\n",
    "\n",
    "from nodags_flows.models.resblock_trainer import resflow_train_test_wrapper\n",
    "\n",
    "##################### DGLearn ############################\n",
    "\n",
    "from dglearn.dglearn.dg.adjacency_structure import AdjacencyStucture\n",
    "from dglearn.dglearn.dg.graph_equivalence_search import GraphEquivalenceSearch\n",
    "from dglearn.dglearn.dg.converter import binary2array\n",
    "from dglearn.dglearn.learning.cyclic_manager import CyclicManager\n",
    "from dglearn.dglearn.dg.reduction import reduce_support\n",
    "from dglearn.dglearn.learning.search.virtual import virtual_refine\n",
    "from dglearn.dglearn.learning.search.tabu import tabu_search\n",
    "\n",
    "###################### LiNGD #########################\n",
    "\n",
    "from lingd import LiNGD\n",
    "\n",
    "##################### FRP ###########################\n",
    "\n",
    "from frp.run_causal_discovery import  run_filter_rank_prune, run_dglearn\n",
    "\n",
    "#####################################################\n",
    "\n",
    "# Set random seed\n",
    "#seed = 44 # main seed for reproducibility\n",
    "seed = 33\n",
    "set_random_seed(seed)\n",
    "\n",
    "# causal libraries\n",
    "import cdt, castle\n",
    "from castle.algorithms import GOLEM\n",
    "from castle.algorithms import Notears\n",
    "\n",
    "# causal metrics\n",
    "from cdt.metrics import precision_recall, SHD, SID\n",
    "from castle.metrics import MetricsDAG\n",
    "from castle.common import GraphDAG\n",
    "from causallearn.graph.SHD import SHD as SHD_causallearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of equivalence class of true graph for /share/amine.mcharrak/cyclic_data/10ER41_linear_cyclic_GAUSS-EV_seed_2_n_samples_5000/: 42\n",
      "Number of cycles in G_true for /share/amine.mcharrak/cyclic_data/10ER41_linear_cyclic_GAUSS-EV_seed_2_n_samples_5000/: 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "#   # fetch dataset \n",
    "# abalone = fetch_ucirepo(id=1) \n",
    "  \n",
    "# # data (as pandas dataframes) \n",
    "# X = abalone.data.features\n",
    "# y = abalone.data.targets\n",
    "  \n",
    "# # metadata \n",
    "# #print(abalone.metadata) \n",
    "\n",
    "# # now merge X and y to create a single dataframe and give the columns the correct names using abalone.variables.name.values\n",
    "# df = pd.concat([X, y], axis=1)\n",
    "# df.columns = abalone.variables.name.values.tolist()\n",
    "# print(df.head())\n",
    "# print()\n",
    "# # show # of unique values in each column\n",
    "# print(df.nunique())\n",
    "# # finally convert the Rings variable to a binary variable by setting the threshold to mean(rings)\n",
    "# df['Rings'] = df['Rings'] > df['Rings'].mean()\n",
    "# # then convert to integer\n",
    "# df['Rings'] = df['Rings'].astype(int)\n",
    "# # also replace the values in Sex with integers\n",
    "# df['Sex'] = df['Sex'].map({'M': 0, 'F': 1, 'I': 2})\n",
    "\n",
    "# # now show the first 5 rows of the dataframe\n",
    "# print(df.head())\n",
    "\n",
    "# # Create a boolean list for continuous variables (any float dtype)\n",
    "# is_cont_node = df.dtypes.map(is_float_dtype).tolist()\n",
    "\n",
    "# # Print the result\n",
    "# print(is_cont_node)\n",
    "\n",
    "# # plot the distribution of all variables in the dataframe\n",
    "# df.hist(figsize=(15, 10))\n",
    "# plt.show()\n",
    "\n",
    "path = '/share/amine.mcharrak/cyclic_data/10ER41_linear_cyclic_GAUSS-EV_seed_2_n_samples_5000/'\n",
    "\n",
    "# Load adjacency matrix and data as pandas dataframe\n",
    "adj_matrix = pd.read_csv(os.path.join(path, 'adj_matrix.csv'), header=None)\n",
    "data = pd.read_csv(os.path.join(path, 'train.csv'), header=None)\n",
    "weighted_adj_matrix = pd.read_csv(os.path.join(path, 'W_adj_matrix.csv'), header=None)\n",
    "prec_matrix_path = os.path.join(path, 'prec_matrix.csv')\n",
    "\n",
    "if os.path.exists(prec_matrix_path):\n",
    "    prec_matrix = pd.read_csv(prec_matrix_path, header=None)\n",
    "\n",
    "n_vars = data.shape[1]\n",
    "\n",
    "B_true = adj_matrix.values\n",
    "X = data.values\n",
    "W_true = weighted_adj_matrix.values\n",
    "\n",
    "# Get edge list from B_true using networkx\n",
    "G_true = nx.DiGraph(B_true)\n",
    "edges_list = list(G_true.edges())\n",
    "\n",
    "# Compute performance metric: SHD\n",
    "true_graph = AdjacencyStucture(n_vars=n_vars, edge_list=edges_list)\n",
    "search = GraphEquivalenceSearch(true_graph)\n",
    "search.search_dfs()\n",
    "\n",
    "B_true_EC = [binary2array(bstr) for bstr in search.visited_graphs]\n",
    "print(f\"Size of equivalence class of true graph for {path}: {len(B_true_EC)}\")\n",
    "\n",
    "# Check if G_true is cyclic by listing all cycles\n",
    "print(f\"Number of cycles in G_true for {path}: {len(list(nx.simple_cycles(G_true)))}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    5000\n",
      "1    5000\n",
      "2    5000\n",
      "3    5000\n",
      "4    5000\n",
      "5    5000\n",
      "6    5000\n",
      "7    5000\n",
      "8    5000\n",
      "9    5000\n",
      "dtype: int64\n",
      "[True, True, True, True, True, True, True, True, True, True]\n"
     ]
    }
   ],
   "source": [
    "data.head()\n",
    "# show unique values in each column\n",
    "print(data.nunique())\n",
    "# Determine if each variable is continuous or discrete based on the number of unique values\n",
    "is_cont_node = np.array([True if data[col].nunique() > 2 else False for col in data.columns])\n",
    "is_cont_node = is_cont_node.tolist()\n",
    "print(is_cont_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ## Load the actual connectome data\n",
    "\n",
    "# # %%\n",
    "# # load the weighted adjacency matrices for ER and connectome\n",
    "\n",
    "# # Specify the folder where the adjacency matrices were saved\n",
    "# folder = '../data/'\n",
    "\n",
    "# # Specify the folder where the acyclic positive integer weighted connectome data was saved\n",
    "# folder_cyclic = '/home/amine.mcharrak/connectome/data/'\n",
    "# # Specify the folder where the acyclic positive integer weighted connectome data was saved\n",
    "# folder_acyclic = '/home/amine.mcharrak/connectome/data/'\n",
    "\n",
    "# # Example usage to load the saved adjacency matrices\n",
    "# # G_A_init_t_ordered_adj_matrix = load_adjacency_matrix(os.path.join(folder, 'G_A_init_t_ordered_adj_matrix.npy'))\n",
    "# # G_A_init_t_ordered_dag_adj_matrix = load_adjacency_matrix(os.path.join(folder, 'G_A_init_t_ordered_dag_adj_matrix.npy'))\n",
    "# # ER = load_adjacency_matrix(os.path.join(folder, 'ER_adj_matrix.npy'))\n",
    "# # ER_dag = load_adjacency_matrix(os.path.join(folder, 'ER_dag_adj_matrix.npy'))\n",
    "\n",
    "# # Change name of the connectome adjacency matrix to C and C_dag\n",
    "# # C = G_A_init_t_ordered_adj_matrix\n",
    "# # C_dag = G_A_init_t_ordered_dag_adj_matrix\n",
    "\n",
    "# # Now ensure that both DAG adjacency matrices are binary, if they aren't already\n",
    "# # ER_dag_bin = (ER_dag != 0).astype(int)\n",
    "# # C_dag_bin = (C_dag != 0).astype(int)\n",
    "\n",
    "# # ER_true = ER_dag_bin\n",
    "# # C_true = C_dag_bin\n",
    "\n",
    "# # %% [markdown]\n",
    "# # ## Create data to debug and implement the pcax version of NOTEARS\n",
    "\n",
    "# # %%\n",
    "# # actual data\n",
    "# B_true = simulate_dag(d=10, s0=20, graph_type='ER') # ER2\n",
    "# # B_true = simulate_dag(d=100, s0=400, graph_type='ER') # ER4\n",
    "# # debugging data\n",
    "# # B_true = simulate_dag(d=10, s0=20, graph_type='ER') # ER2\n",
    "\n",
    "\n",
    "# # B_true = C_dag_bin # if you want to use the connectome-based DAG # best\n",
    "# #B_true = ER_dag_bin # if you want to use the ER-based DAG\n",
    "\n",
    "# #B_true = simulate_dag(d=5, s0=10, graph_type='ER') # ER2\n",
    "# #B_true = simulate_dag(d=10, s0=20, graph_type='ER') # ER2\n",
    "# #B_true = simulate_dag(d=50, s0=100, graph_type='ER') # ER2\n",
    "# #B_true = simulate_dag(d=100, s0=200, graph_type='ER') # ER2\n",
    "# #B_true = simulate_dag(d=279, s0=558, graph_type='ER') # ER2\n",
    "\n",
    "# # create SF2 graph and SF4 graph with d=10 nodes\n",
    "# #B_true = simulate_dag(d=10, s0=20, graph_type='SF') # SF2\n",
    "# #B_true = simulate_dag(d=10, s0=40, graph_type='SF') # SF4\n",
    "# #B_true = simulate_dag(d=100, s0=400, graph_type='SF') # SF4\n",
    "\n",
    "# # create ER2 and ER4 graphs with d=100 nodes\n",
    "# #B_true = simulate_dag(d=100, s0=200, graph_type='ER') # ER2\n",
    "# #B_true = simulate_dag(d=100, s0=400, graph_type='ER') # ER4\n",
    "\n",
    "# # create equivalent ER4 and ER6 graphs\n",
    "# #B_true = simulate_dag(d=279, s0=1116, graph_type='ER') # ER4\n",
    "# #B_true = simulate_dag(d=279, s0=1674, graph_type='ER') # ER6\n",
    "\n",
    "# # create equivalent SF4 and SF6 graphs\n",
    "# #B_true = simulate_dag(d=100, s0=600, graph_type='SF') # SF6\n",
    "# #B_true = simulate_dag(d=279, s0=1116, graph_type='SF') # SF4\n",
    "# #B_true = simulate_dag(d=279, s0=1674, graph_type='SF') # SF6\n",
    "\n",
    "# # create simple data using simulate_dag method from causal_helpers with expected number of edges (s0) and number of nodes (d)\n",
    "# #B_true = simulate_dag(d=100, s0=199, graph_type='ER') # we use p≈0.040226 for the connectome-based ER_dag graph. This means that the expected number of edges is 0.040226 * d * (d-1) / 2\n",
    "# # examples: d=50 -> s0=49 (works), d=100 -> s0=199, d=200 -> s0=800\n",
    "\n",
    "# # create the weighted adjacency matrix based on the binary adjacency matrix\n",
    "# #W_true = simulate_parameter(B_true, connectome=True)\n",
    "# #W_true = simulate_parameter(B_true)\n",
    "\n",
    "# # sample data from the linear SEM\n",
    "# # actual data\n",
    "# #X = simulate_linear_sem(W_true, n=10000, sem_type='gauss')\n",
    "# #X = simulate_linear_sem(W_true, n=10000, sem_type='uniform')\n",
    "# # for debugging\n",
    "# #X = simulate_linear_sem(W_true, n=1000, sem_type='gauss')\n",
    "# #X = simulate_linear_sem(W_true, n=2500, sem_type='gauss')\n",
    "# #X = simulate_linear_sem(W_true, n=6250, sem_type='gauss')\n",
    "# #X = simulate_linear_sem(W_true, n=50000, sem_type='gauss')\n",
    "# #X = simulate_linear_sem(W_true, n=100000, sem_type='gauss') # 1000*(279**2)/(20**2) = 194602\n",
    "\n",
    "# # load the cyclic integer weighted connectome data adjacency matrix\n",
    "# #B_true_weighted = load_adjacency_matrix(os.path.join(folder_cyclic, 'A_init_t_ordered_adj_matrix_with_cycles.npy'))\n",
    "# #X, W_true = sample_cyclic_data(B_true_weighted, n_samples=10000, noise_type='non-gaussian')\n",
    "# #B_true = (W_true != 0).astype(int)\n",
    "\n",
    "# # load the acyclic integer weighted connectome data adjacency matrix\n",
    "# # B_true_weighted = load_adjacency_matrix(os.path.join(folder_acyclic, 'A_init_t_ordered_adj_matrix_no_cycles.npy'))\n",
    "# # print(\"B_true_weighted:\\n\", np.array_str(B_true_weighted, precision=4, suppress_small=True))\n",
    "\n",
    "# # A: use this for regular DAGs\n",
    "# W_true = simulate_parameter(B_true)\n",
    "\n",
    "# # B: use this for connectome-based DAGs\n",
    "# #W_true = simulate_parameter(B_true_weighted, connectome=True)\n",
    "# #B_true = (W_true != 0).astype(int)\n",
    "\n",
    "# # some print statements to check the values of W_true\n",
    "# print(\"W_true:\\n\", np.array_str(W_true, precision=4, suppress_small=True))\n",
    "# print(\"Mean of W_true:\", np.mean(W_true))\n",
    "# print(\"Variance of W_true:\", np.var(W_true))\n",
    "# print(\"Max value in W_true:\", np.max(W_true))\n",
    "# print(\"Min value in W_true:\", np.min(W_true))\n",
    "\n",
    "# # sample data from the linear SEM\n",
    "# X = simulate_linear_sem(W_true, n=1000, sem_type='gauss')\n",
    "\n",
    "# # now standardized data, where each variable is normalized to unit variance\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# scaler = StandardScaler()\n",
    "# X_std = scaler.fit_transform(X)\n",
    "\n",
    "# # NOTE: you may not write positional arguments after keyword arguments. \n",
    "# # That is, the values that you are passing positionally have to come first!\n",
    "\n",
    "# # create a dataset using the simulated data\n",
    "# # NOTE: NOTEARS paper uses n=1000 for graph with d=20.\n",
    "# # NOTE: d... number of nodes, p=d^2... number of parameters, n... number of samples. Then: comparing p1=d1^2 vs p2=d2^2 we have that: n1/p1 must be equal to n2/p2\n",
    "# # Thus we have n2 = n1 * p2 / p1. For the case of d2=100 we have that n2 = (n1*p2)/p1 = 1000*(100^2)/(20^2) = 25000 \n",
    "# # we should expect to use that many samples actually to be able to learn the graph in a comparable way.\n",
    "# #dataset = IIDSimulation(W=W_true, n=25000, method='linear', sem_type='gauss')\n",
    "# #true_dag, X = dataset.B, dataset.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print how many non-zero entries are in the true DAG\n",
    "print(f\"Number of non-zero entries in the true DAG: {np.count_nonzero(B_true)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility and evaluation functions\n",
    "@jit\n",
    "def MAE(W_true, W):\n",
    "    \"\"\"This function returns the Mean Absolute Error for the difference between the true weighted adjacency matrix W_true and th estimated one, W.\"\"\"\n",
    "    MAE_ = jnp.mean(jnp.abs(W - W_true))\n",
    "    return MAE_\n",
    "\n",
    "@jax.jit\n",
    "def compute_h_reg(W):\n",
    "    \"\"\"\n",
    "    Compute the DAG constraint using the exponential trace-based acyclicity constraint.\n",
    "\n",
    "    This function calculates the value of the acyclicity constraint for a given\n",
    "    adjacency matrix using the formulation:\n",
    "    h_reg = trace(exp(W ⊙ W)) - d\n",
    "    where ⊙ represents the Hadamard (element-wise) product.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    W : jnp.ndarray\n",
    "        (d, d) adjacency matrix.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    h_reg : float\n",
    "        The value of the DAG constraint.\n",
    "    \"\"\"\n",
    "    # Dimensions of W\n",
    "    d = W.shape[0]\n",
    "\n",
    "    # Compute h_reg using the trace of the matrix exponential\n",
    "    h_reg = jnp.trace(jax_la.expm(jnp.multiply(W, W))) - d\n",
    "\n",
    "    return h_reg\n",
    "\n",
    "@jax.jit\n",
    "def notears_dag_constraint(W):\n",
    "    \"\"\"\n",
    "    Compute the NOTEARS DAG constraint using the exponential trace-based acyclicity constraint.\n",
    "\n",
    "    This function calculates the value of the acyclicity constraint for a given\n",
    "    adjacency matrix using the formulation:\n",
    "    h_reg = trace(exp(W ⊙ W)) - d\n",
    "    where ⊙ represents the Hadamard (element-wise) product.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    W : jnp.ndarray\n",
    "        (d, d) adjacency matrix.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    h_reg : float\n",
    "        The value of the DAG constraint.\n",
    "    \"\"\"\n",
    "    # Dimensions of W\n",
    "    d = W.shape[0]\n",
    "\n",
    "    # Compute h_reg using the trace of the matrix exponential\n",
    "    h_reg = jnp.trace(jax_la.expm(jnp.multiply(W, W))) - d\n",
    "\n",
    "    return h_reg\n",
    "\n",
    "@jax.jit\n",
    "def dagma_dag_constraint(W, s=1.0):\n",
    "    \"\"\"\n",
    "    Compute the DAG constraint using the logdet acyclicity constraint from DAGMA.\n",
    "    This function is JAX-jitted for improved performance.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    W : jnp.ndarray\n",
    "        (d, d) adjacency matrix.\n",
    "    s : float, optional\n",
    "        Controls the domain of M-matrices. Defaults to 1.0.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    h_reg : float\n",
    "        The value of the DAG constraint.\n",
    "    \"\"\"\n",
    "    # Dimensions of W\n",
    "    d = W.shape[0]\n",
    "\n",
    "    # Compute M-matrix for the logdet constraint\n",
    "    M = s * jnp.eye(d) - jnp.multiply(W, W)\n",
    "\n",
    "    # Compute the value of the logdet DAG constraint\n",
    "    h_reg = -jax_la.slogdet(M)[1] + d * jnp.log(s)\n",
    "\n",
    "    return h_reg\n",
    "\n",
    "\n",
    "# Define fucntion to compute h_reg based W with h_reg = jnp.trace(jax.scipy.linalg.expm(W * W)) - d, here * denotes the hadamard product\n",
    "def compute_h_reg(W):\n",
    "    \"\"\"This function computes the h_reg term based on the matrix W.\"\"\"\n",
    "    h_reg = jnp.trace(jax.scipy.linalg.expm(W * W)) - W.shape[0]\n",
    "    return h_reg\n",
    "\n",
    "def compute_binary_adjacency(W, threshold=0.3):\n",
    "    \"\"\"\n",
    "    Compute the binary adjacency matrix by thresholding the input matrix.\n",
    "\n",
    "    Args:\n",
    "    - W (array-like): The weighted adjacency matrix (can be a JAX array or a NumPy array).\n",
    "    - threshold (float): The threshold value to determine the binary matrix. Default is 0.3.\n",
    "\n",
    "    Returns:\n",
    "    - B_est (np.ndarray): The binary adjacency matrix where each element is True if the corresponding \n",
    "                          element in W is greater than the threshold, otherwise False.\n",
    "    \"\"\"\n",
    "    # Convert JAX array to NumPy array if necessary\n",
    "    if isinstance(W, jnp.ndarray):\n",
    "        W = np.array(W)\n",
    "\n",
    "    # Compute the binary adjacency matrix\n",
    "    B_est = np.array(np.abs(W) > threshold)\n",
    "    \n",
    "    return B_est\n",
    "\n",
    "def ensure_DAG(W):\n",
    "    \"\"\"\n",
    "    Ensure that the weighted adjacency matrix corresponds to a DAG.\n",
    "\n",
    "    Inputs:\n",
    "        W: numpy.ndarray - a weighted adjacency matrix representing a directed graph\n",
    "\n",
    "    Outputs:\n",
    "        W: numpy.ndarray - a weighted adjacency matrix without cycles (DAG)\n",
    "    \"\"\"\n",
    "    # Convert the adjacency matrix to a directed graph\n",
    "    g = nx.DiGraph(W)\n",
    "\n",
    "    # Make a copy of the graph to modify\n",
    "    gg = g.copy()\n",
    "\n",
    "    # Remove cycles by removing edges\n",
    "    while not nx.is_directed_acyclic_graph(gg):\n",
    "        h = gg.copy()\n",
    "\n",
    "        # Remove all the sources and sinks\n",
    "        while True:\n",
    "            finished = True\n",
    "\n",
    "            for node, in_degree in nx.in_degree_centrality(h).items():\n",
    "                if in_degree == 0:\n",
    "                    h.remove_node(node)\n",
    "                    finished = False\n",
    "\n",
    "            for node, out_degree in nx.out_degree_centrality(h).items():\n",
    "                if out_degree == 0:\n",
    "                    h.remove_node(node)\n",
    "                    finished = False\n",
    "\n",
    "            if finished:\n",
    "                break\n",
    "\n",
    "        # Find a cycle with a random walk starting at a random node\n",
    "        node = list(h.nodes)[0]\n",
    "        cycle = [node]\n",
    "        while True:\n",
    "            edges = list(h.out_edges(node))\n",
    "            _, node = edges[np.random.choice(len(edges))]\n",
    "\n",
    "            if node in cycle:\n",
    "                break\n",
    "\n",
    "            cycle.append(node)\n",
    "\n",
    "        # Extract the cycle path and adjust it to start at the first occurrence of the repeated node\n",
    "        cycle = np.array(cycle)\n",
    "        i = np.argwhere(cycle == node)[0][0]\n",
    "        cycle = cycle[i:]\n",
    "        cycle = cycle.tolist() + [node]\n",
    "\n",
    "        # Find edges in that cycle\n",
    "        edges = list(zip(cycle[:-1], cycle[1:]))\n",
    "\n",
    "        # Randomly pick an edge to remove\n",
    "        edge = edges[np.random.choice(len(edges))]\n",
    "        gg.remove_edge(*edge)\n",
    "\n",
    "    # Convert the modified graph back to a weighted adjacency matrix\n",
    "    W_acyclic = nx.to_numpy_array(gg)\n",
    "\n",
    "    return W_acyclic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Complete_Graph(pxc.EnergyModule):\n",
    "    def __init__(self, input_dim: int, n_nodes: int, has_bias: bool = False, is_cont_node: list = None) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = px.static(input_dim)  # Ensure input_dim is static\n",
    "        self.n_nodes = px.static(n_nodes)  # Keep n_nodes as a static value\n",
    "        self.has_bias = has_bias\n",
    "        self.is_cont_node = is_cont_node\n",
    "\n",
    "        # Initialize a single linear layer for the weights and wrap it in a list\n",
    "        self.layers = [pxnn.Linear(n_nodes, n_nodes, bias=has_bias)] # vanilla initialization is uniform(-stdv, stdv) with stdv = 1/sqrt(n_nodes), here n_nodes = 12, thus stdv = 1/sqrt(12) = 0.2887\n",
    "        \n",
    "        #stddev = jnp.sqrt(0.01) # this equals 0.1 (default would have been 0.2887)\n",
    "        stddev = 1/n_nodes\n",
    "        key = random.PRNGKey(0)\n",
    "        #new_weight_matrix = random.normal(key, shape=(n_nodes, n_nodes)) * stddev # option 1 using normal distribution\n",
    "        new_weight_matrix = random.uniform(key, shape=(n_nodes, n_nodes), minval=-stddev, maxval=stddev) # option 2 using uniform distribution\n",
    "\n",
    "        # Step 3: Replace diagonal elements with 0\n",
    "        for i in range(n_nodes):\n",
    "            new_weight_matrix = new_weight_matrix.at[i, i].set(0.0)\n",
    "\n",
    "        # Step 5: Update the weight matrix\n",
    "        self.layers[0].nn.weight.set(new_weight_matrix)\n",
    "\n",
    "        # Initialize vodes based on is_cont_node\n",
    "        if is_cont_node is None:\n",
    "            is_cont_node = [True] * n_nodes  # Default to all continuous nodes if not provided\n",
    "\n",
    "        self.vodes = []\n",
    "        for is_cont in is_cont_node:\n",
    "            if is_cont:\n",
    "                self.vodes.append(pxc.Vode())\n",
    "            else:\n",
    "                #self.vodes.append(pxc.Vode(pxc.ce_energy))\n",
    "                self.vodes.append(pxc.Vode(pxc.bce_energy))\n",
    "\n",
    "    def freeze_nodes(self, freeze=True):\n",
    "        \"\"\"Freeze or unfreeze all vodes in the model.\"\"\"\n",
    "        for vode in self.vodes:\n",
    "            vode.h.frozen = freeze\n",
    "\n",
    "    def are_vodes_frozen(self):\n",
    "        \"\"\"Check if all vodes in the model are frozen.\"\"\"\n",
    "        return all(hasattr(vode.h, 'frozen') and vode.h.frozen for vode in self.vodes)\n",
    "    \n",
    "    def get_W(self):\n",
    "        \"\"\"This function returns the weighted adjacency matrix based on the linear layer in the model.\"\"\"\n",
    "        W = self.layers[0].nn.weight.get()\n",
    "        W_T = W.T\n",
    "        return W_T\n",
    "\n",
    "    def __call__(self, x=None):\n",
    "        n_nodes = self.n_nodes.get()\n",
    "\n",
    "        if x is not None:\n",
    "            # Initialize nodes with given data\n",
    "            reshaped_x = x.reshape(n_nodes, -1)  # Infer input_dim from x\n",
    "            \n",
    "            # Print the shape of reshaped_x[0] when x is not None\n",
    "            print(\"The shape of reshaped_x[0] when x is not None is: \", reshaped_x[0].shape)\n",
    "\n",
    "            for i in range(n_nodes):\n",
    "                self.vodes[i](reshaped_x[i])\n",
    "\n",
    "        else:\n",
    "            # Stack current state of vodes into a matrix of shape (n_nodes, input_dim)\n",
    "            x_ = jnp.vstack([vode.get('h') for vode in self.vodes])\n",
    "\n",
    "            # Print the shape of x_ when x is None\n",
    "            print(\"The shape of x_ when x is None is: \", x_.shape)\n",
    "\n",
    "            # Apply the linear transformation\n",
    "            output = self.layers[0](x_)\n",
    "\n",
    "            # Print the shape of output when x is None\n",
    "            print(\"The shape of output when x is None is: \", output.shape)\n",
    "\n",
    "            # Update the vodes with the output\n",
    "            for i in range(n_nodes):\n",
    "                self.vodes[i](output[i])\n",
    "\n",
    "        # Stack the final state of vodes for output\n",
    "        output = jnp.vstack([vode.get('h') for vode in self.vodes])\n",
    "\n",
    "        # Print the shape of the output\n",
    "        print(\"The shape of the output is: \", output.shape)\n",
    "\n",
    "        return output\n",
    "\n",
    "# Usage\n",
    "input_dim = 1\n",
    "n_nodes = X.shape[1]\n",
    "#model = Complete_Graph(input_dim, n_nodes, has_bias=False)\n",
    "#model = Complete_Graph(input_dim, n_nodes, has_bias=True)\n",
    "model = Complete_Graph(input_dim, n_nodes, has_bias=True, is_cont_node=is_cont_node)\n",
    "# Get weighted adjacency matrix\n",
    "W = model.get_W()\n",
    "print(\"This is the weighted adjacency matrix:\\n\", W)\n",
    "print()\n",
    "print(\"The shape of the weighted adjacency matrix is: \", W.shape)\n",
    "print()\n",
    "#print(model)\n",
    "print()\n",
    "\n",
    "# Check if all nodes are frozen initially\n",
    "print(\"Initially, are all nodes frozen?:\", model.are_vodes_frozen())\n",
    "print()\n",
    "\n",
    "# Freezing all nodes\n",
    "print(\"Freezing all nodes...\")\n",
    "model.freeze_nodes(freeze=True)\n",
    "print()\n",
    "\n",
    "# Check if all nodes are frozen after freezing\n",
    "print(\"After freezing, are all nodes frozen?:\", model.are_vodes_frozen())\n",
    "print()\n",
    "\n",
    "# Unfreezing all nodes\n",
    "print(\"Unfreezing all nodes...\")\n",
    "model.freeze_nodes(freeze=False)\n",
    "print()\n",
    "\n",
    "# Check if all nodes are frozen after unfreezing\n",
    "print(\"After unfreezing, are all nodes frozen?:\", model.are_vodes_frozen())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.is_cont_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: make the below params global or input to the functions in which it is used.\n",
    "w_learning_rate = 1e-3\n",
    "#w_learning_rate = 5e-3\n",
    "h_learning_rate = 1e-4\n",
    "T = 1\n",
    "\n",
    "nm_epochs = 2000 # not much happens after 2000 epochs\n",
    "every_n_epochs = nm_epochs // 20 # Print every 5% of the epochs\n",
    "batch_size = 256\n",
    "\n",
    "#lam_h = 1e-1\n",
    "lam_h = 1e0\n",
    "lam_l1 = 1e-2\n",
    "\n",
    "# SHD cyclic and cycle F1 using our method: 45, 0.4838709677419355\n",
    "#lam_h = 2e0\n",
    "#lam_l1 = 4e-2\n",
    "\n",
    "# SHD cyclic and cycle F1 using our method: 44, 0.4838709677419355\n",
    "#lam_h = 1.7e0\n",
    "#lam_l1 = 4e-2\n",
    "\n",
    "# SHD cyclic and cycle F1 using our method: 54, 0.5538461538461538\n",
    "#lam_h = 0.8e0\n",
    "#lam_l1 = 4e-2\n",
    "\n",
    "# SHD cyclic and cycle F1 using our method: 45, 0.507936507936508\n",
    "#lam_h = 1.5e0\n",
    "#lam_l1 = 4e-2\n",
    "\n",
    "# Create a file name string for the hyperparameters\n",
    "exp_name = f\"bs_{batch_size}_lrw_{w_learning_rate}_lrh_{h_learning_rate}_lamh_{lam_h}_laml1_{lam_l1}_epochs_{nm_epochs}\"\n",
    "print(\"Name of the experiment: \", exp_name)\n",
    "\n",
    "# Training an1d evaluation functions\n",
    "@pxf.vmap(pxu.M(pxc.VodeParam | pxc.VodeParam.Cache).to((None, 0)), in_axes=(0,), out_axes=0)\n",
    "def forward(x, *, model: Complete_Graph):\n",
    "    return model(x)\n",
    "\n",
    "# @pxf.vmap(pxu.M(pxc.VodeParam | pxc.VodeParam.Cache).to((None, 0)), out_axes=(None, 0), axis_name=\"batch\") # if only one output\n",
    "@pxf.vmap(pxu.M(pxc.VodeParam | pxc.VodeParam.Cache).to((None, 0)), out_axes=(None, None, None, None, 0), axis_name=\"batch\") # if multiple outputs\n",
    "def energy(*, model: Complete_Graph):\n",
    "\n",
    "    print(\"Energy: Starting computation\")\n",
    "    x_ = model(None)\n",
    "    print(\"Energy: Got model output\")\n",
    "    \n",
    "    W = model.get_W()\n",
    "    # Dimensions of W\n",
    "    d = W.shape[0]\n",
    "    print(f\"Energy: Got W (shape: {W.shape}) and d: {d}\")\n",
    "\n",
    "    # PC energy term\n",
    "    pc_energy = jax.lax.pmean(model.energy(), axis_name=\"batch\")\n",
    "    print(f\"Energy: PC energy term: {pc_energy}\")\n",
    "\n",
    "    # L1 regularization using adjacency matrix (scaled by Frobenius norm)\n",
    "    l1_reg = jnp.sum(jnp.abs(W)) / (jnp.linalg.norm(W, ord='fro') + 1e-8)\n",
    "    #l1_reg = jnp.sum(jnp.abs(W)) / d\n",
    "    print(f\"Energy: L1 reg term: {l1_reg}\")\n",
    "\n",
    "    # DAG constraint (stable logarithmic form)\n",
    "    #h_reg = notears_dag_constraint(W)\n",
    "    #h_reg = notears_dag_constraint(W)/ (jnp.sqrt(d) + 1e-8)\n",
    "    #h_reg = notears_dag_constraint(W) / d  # with normalization\n",
    "\n",
    "    #h_reg = dagma_dag_constraint(W)\n",
    "    h_reg = dagma_dag_constraint(W) / (jnp.sqrt(d) + 1e-8)\n",
    "    #h_reg = dagma_dag_constraint(W) / d  # with normalization\n",
    "    print(f\"Energy: DAG constraint term: {h_reg}\")\n",
    "        \n",
    "    # Combined loss\n",
    "    obj = pc_energy + lam_h * h_reg + lam_l1 * l1_reg\n",
    "    print(f\"Energy: Final objective: {obj}\")\n",
    "\n",
    "    # Ensure obj is a scalar, not a (1,) array because JAX's grad and value_and_grad functions are designed\n",
    "    # to compute gradients of *scalar-output functions*\n",
    "    obj = obj.squeeze()  # explicitly converte the (1,) array (obj) to a scalar of shape ()  \n",
    "    \n",
    "    return obj, pc_energy, h_reg, l1_reg, x_\n",
    "\n",
    "@pxf.jit(static_argnums=0)\n",
    "def train_on_batch(T: int, x: jax.Array, *, model: Complete_Graph, optim_w: pxu.Optim, optim_h: pxu.Optim):\n",
    "    print(\"1. Starting train_on_batch\")  \n",
    "\n",
    "    model.train()\n",
    "    print(\"2. Model set to train mode\")\n",
    "\n",
    "    model.freeze_nodes(freeze=True)\n",
    "    print(\"3. Nodes frozen\")\n",
    "\n",
    "    # init step\n",
    "    with pxu.step(model, pxc.STATUS.INIT, clear_params=pxc.VodeParam.Cache):\n",
    "        print(\"4. Doing forward for initialization\")\n",
    "        forward(x, model=model)\n",
    "        print(\"5. After forward for initialization\")\n",
    "\n",
    "    \"\"\"\n",
    "    # The following code might not be needed as we are keeping the vodes frozen at all times\n",
    "    # Reinitialize the optimizer state between different batches\n",
    "    optim_h.init(pxu.M(pxc.VodeParam)(model))\n",
    "\n",
    "    for _ in range(T):\n",
    "        with pxu.step(model, clear_params=pxc.VodeParam.Cache):\n",
    "            (e, x_), g = pxf.value_and_grad(\n",
    "                pxu.M(pxu.m(pxc.VodeParam).has_not(frozen=True), [False, True]),\n",
    "                has_aux=True\n",
    "            )(energy)(model=model)\n",
    "        optim_h.step(model, g[\"model\"], True)\n",
    "    \"\"\"\n",
    "\n",
    "    with pxu.step(model, clear_params=pxc.VodeParam.Cache):\n",
    "        print(\"6. Before computing gradients\")\n",
    "        (obj, (pc_energy, h_reg, l1_reg, x_)), g = pxf.value_and_grad(\n",
    "            pxu.M(pxnn.LayerParam).to([False, True]), \n",
    "            has_aux=True\n",
    "        )(energy)(model=model) # pxf.value_and_grad returns a tuple structured as ((value, aux), grad), not as six separate outputs.\n",
    "        \n",
    "        print(\"7. After computing gradients\")\n",
    "        #print(\"Gradient structure:\", g)\n",
    "\n",
    "        print(\"8. Before zeroing out the diagonal gradients\")\n",
    "        # Zero out the diagonal gradients using jax.numpy.fill_diagonal\n",
    "        weight_grads = g[\"model\"].layers[0].nn.weight.get()\n",
    "        weight_grads = jax.numpy.fill_diagonal(weight_grads, 0.0, inplace=False)\n",
    "        # print the grad values using the syntax jax.debug.print(\"🤯 {x} 🤯\", x=x)\n",
    "        #jax.debug.print(\"{weight_grads}\", weight_grads=weight_grads)\n",
    "        g[\"model\"].layers[0].nn.weight.set(weight_grads)\n",
    "        print(\"9. After zeroing out the diagonal gradients\")\n",
    "\n",
    "        \n",
    "    print(\"10. Before optimizer step\")\n",
    "    optim_w.step(model, g[\"model\"])\n",
    "    #optim_w.step(model, g[\"model\"], scale_by=1.0/x.shape[0])\n",
    "    print(\"11. After optimizer step\")\n",
    "\n",
    "    with pxu.step(model, clear_params=pxc.VodeParam.Cache):\n",
    "        print(\"12. Before final forward\")\n",
    "        forward(None, model=model)\n",
    "        e_avg_per_sample = model.energy()\n",
    "        print(\"13. After final forward\")\n",
    "\n",
    "    model.freeze_nodes(freeze=False)\n",
    "    print(\"14. Nodes unfrozen\")\n",
    "\n",
    "    return pc_energy, l1_reg, h_reg, obj\n",
    "\n",
    "def train(dl, T, *, model: Complete_Graph, optim_w: pxu.Optim, optim_h: pxu.Optim):\n",
    "    batch_pc_energies = []\n",
    "    batch_l1_regs = []\n",
    "    batch_h_regs = []\n",
    "    batch_objs = []\n",
    "    \n",
    "    for batch in dl:\n",
    "        pc_energy, l1_reg, h_reg, obj = train_on_batch(\n",
    "            T, batch, model=model, optim_w=optim_w, optim_h=optim_h\n",
    "        )\n",
    "        batch_pc_energies.append(pc_energy)\n",
    "        batch_l1_regs.append(l1_reg)\n",
    "        batch_h_regs.append(h_reg)\n",
    "        batch_objs.append(obj)\n",
    "\n",
    "    W = model.get_W()\n",
    "\n",
    "    # Compute epoch averages\n",
    "    epoch_pc_energy = jnp.mean(jnp.array(batch_pc_energies))\n",
    "    epoch_l1_reg = jnp.mean(jnp.array(batch_l1_regs))\n",
    "    epoch_h_reg = jnp.mean(jnp.array(batch_h_regs))\n",
    "    epoch_obj = jnp.mean(jnp.array(batch_objs))\n",
    "    \n",
    "    return W, epoch_pc_energy, epoch_l1_reg, epoch_h_reg, epoch_obj\n",
    "\n",
    "\n",
    "# %%\n",
    "# for reference compute the MAE, SID, and SHD between the true adjacency matrix and an all-zero matrix and then print it\n",
    "# this acts as a baseline for the MAE, SID, and SHD similar to how 1/K accuracy acts as a baseline for classification tasks where K is the number of classes\n",
    "\n",
    "W_zero = np.zeros_like(W_true)\n",
    "print(\"MAE between the true adjacency matrix and an all-zero matrix: \", MAE(W_true, W_zero))\n",
    "print(\"SHD between the true adjacency matrix and an all-zero matrix: \", SHD(B_true, compute_binary_adjacency(W_zero)))\n",
    "#print(\"SID between the true adjacency matrix and an all-zero matrix: \", SID(W_true, W_zero))\n",
    "\n",
    "# %%\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X):\n",
    "        self.X = X\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx]\n",
    "\n",
    "# This is a simple collate function that stacks numpy arrays used to interface\n",
    "# the PyTorch dataloader with JAX. In the future we hope to provide custom dataloaders\n",
    "# that are independent of PyTorch.\n",
    "def numpy_collate(batch):\n",
    "    if isinstance(batch[0], np.ndarray):\n",
    "        return np.stack(batch)\n",
    "    elif isinstance(batch[0], (tuple, list)):\n",
    "        transposed = zip(*batch)\n",
    "        return [numpy_collate(samples) for samples in transposed]\n",
    "    else:\n",
    "        return np.array(batch)\n",
    "\n",
    "# The dataloader assumes cuda is being used, as such it sets 'pin_memory = True' and\n",
    "# 'prefetch_factor = 2'. Note that the batch size should be constant during training, so\n",
    "# we set 'drop_last = True' to avoid having to deal with variable batch sizes.\n",
    "class TorchDataloader(torch.utils.data.DataLoader):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset,\n",
    "        batch_size=1,\n",
    "        shuffle=None,\n",
    "        sampler=None,\n",
    "        batch_sampler=None,\n",
    "        num_workers=1,\n",
    "        pin_memory=True,\n",
    "        timeout=0,\n",
    "        worker_init_fn=None,\n",
    "        persistent_workers=True,\n",
    "        prefetch_factor=2,\n",
    "    ):\n",
    "        super(self.__class__, self).__init__(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            sampler=sampler,\n",
    "            batch_sampler=batch_sampler,\n",
    "            num_workers=num_workers,\n",
    "            collate_fn=numpy_collate,\n",
    "            pin_memory=pin_memory,\n",
    "            drop_last=True if batch_sampler is None else None,\n",
    "            timeout=timeout,\n",
    "            worker_init_fn=worker_init_fn,\n",
    "            persistent_workers=persistent_workers,\n",
    "            prefetch_factor=prefetch_factor,\n",
    "        )\n",
    "\n",
    "\n",
    "# Create the custom dataset\n",
    "dataset = CustomDataset(X)\n",
    "\n",
    "# Create the custom dataset with standardized data\n",
    "#dataset_std = CustomDataset(X_std)\n",
    "\n",
    "# Create the dataloader\n",
    "dl = TorchDataloader(dataset, batch_size=batch_size, shuffle=True)\n",
    "######## OR ########\n",
    "#dl = TorchDataloader(dataset_std, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# %%\n",
    "# Initialize the model and optimizers\n",
    "with pxu.step(model, pxc.STATUS.INIT, clear_params=pxc.VodeParam.Cache):\n",
    "    forward(jnp.zeros((batch_size, model.n_nodes.get())), model=model)\n",
    "    optim_h = pxu.Optim(lambda: optax.sgd(h_learning_rate))\n",
    "\n",
    "    \"\"\"\n",
    "    optim_w = pxu.Optim(\n",
    "    optax.chain(\n",
    "        optax.clip_by_global_norm(clip_value),  # Clip gradients by global norm\n",
    "        optax.sgd(w_learning_rate)  # Apply SGD optimizer\n",
    "    ),\n",
    "    pxu.M(pxnn.LayerParam)(model)  # Masking the parameters of the model\n",
    ")\n",
    "    \"\"\"\n",
    "    #optim_w = pxu.Optim(lambda: optax.adam(w_learning_rate), pxu.M(pxnn.LayerParam)(model))\n",
    "    optim_w = pxu.Optim(lambda: optax.adamw(w_learning_rate, nesterov=True), pxu.M(pxnn.LayerParam)(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store differences and energies\n",
    "MAEs = []\n",
    "SHDs = []\n",
    "SHDs_cyclic = []\n",
    "F1s_cycles = []\n",
    "F1s = []\n",
    "pc_energies = []\n",
    "l1_regs = []\n",
    "h_regs = []\n",
    "objs = []\n",
    "\n",
    "# Calculate the initial MAE, SID, and SHD\n",
    "\n",
    "W_init = model.get_W()\n",
    "B_init = compute_binary_adjacency(W_init)\n",
    "\n",
    "MAE_init = MAE(W_true, W_init)\n",
    "print(f\"Start difference (cont.) between W_true and W_init: {MAE_init:.4f}\")\n",
    "\n",
    "SHD_init = SHD(B_true, B_init, double_for_anticausal=False)\n",
    "print(f\"Start SHD between B_true and B_init: {SHD_init:.4f}\")\n",
    "\n",
    "SHD_cyclic_init = compute_cycle_SHD(B_true_EC, B_init)\n",
    "print(f\"Start SHD (cyclic) between B_true and B_init: {SHD_cyclic_init:.4f}\")\n",
    "\n",
    "F1_init = compute_F1_directed(B_true, B_init)\n",
    "print(f\"Start F1 between B_true and B_init: {F1_init:.4f}\")\n",
    "\n",
    "cycle_f1_init = compute_cycle_F1(B_true, B_init)\n",
    "print(f\"Start cycle accuracy between B_true and B_init: {cycle_f1_init:.4f}\")\n",
    "\n",
    "# print the values of the diagonal of the initial W\n",
    "print(\"The diagonal of the initial W: \", jnp.diag(W_init))\n",
    "\n",
    "# Start timing\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "# Training loop\n",
    "with tqdm(range(nm_epochs), position=0, leave=True) as pbar:\n",
    "    for epoch in pbar:\n",
    "        # Train for one epoch using the dataloader\n",
    "        W, epoch_pc_energy, epoch_l1_reg, epoch_h_reg, epoch_obj = train(dl, T=T, model=model, optim_w=optim_w, optim_h=optim_h)\n",
    "        \n",
    "        # Extract the weighted adjacency matrix W and compute the binary adjacency matrix B\n",
    "        W = np.array(W)\n",
    "        B = compute_binary_adjacency(W)\n",
    "\n",
    "        # Compute metrics every 100 epochs\n",
    "        if (epoch + 1) % every_n_epochs == 0 or epoch == 0:\n",
    "            MAEs.append(float(MAE(W_true, W)))\n",
    "            SHDs.append(float(SHD(B_true, compute_binary_adjacency(W), double_for_anticausal=False)))\n",
    "            SHDs_cyclic.append(float(compute_cycle_SHD(B_true_EC, compute_binary_adjacency(W))))\n",
    "            F1s.append(float(compute_F1_directed(B_true, B)))\n",
    "            F1s_cycles.append(float(compute_cycle_F1(B_true, B)))\n",
    "            pc_energies.append(float(epoch_pc_energy))\n",
    "            l1_regs.append(float(epoch_l1_reg))\n",
    "            epoch_h_reg_raw = compute_h_reg(W)\n",
    "            h_regs.append(float(epoch_h_reg_raw))\n",
    "            objs.append(float(epoch_obj))\n",
    "\n",
    "            # Update progress bar with the current status\n",
    "            pbar.set_description(f\"MAE: {MAEs[-1]:.4f}, Cycle F1: {F1s_cycles[-1]:.4f}, F1: {F1s[-1]:.4f}, SHD: {SHDs[-1]:.4f}, Cycle SHD: {SHDs_cyclic[-1]:.4f} || PC Energy: {pc_energies[-1]:.4f}, L1 Reg: {l1_regs[-1]:.4f}, H Reg: {h_regs[-1]:.4f}, Obj: {objs[-1]:.4f}\")\n",
    "\n",
    "# End timing\n",
    "end_time = timeit.default_timer()\n",
    "\n",
    "# Print the average time per epoch\n",
    "average_time_per_epoch = (end_time - start_time) / nm_epochs\n",
    "print(f\"An epoch (with compiling and testing) took on average: {average_time_per_epoch:.4f} seconds\")\n",
    "# print the values of the diagonal of the final W\n",
    "print(\"The diagonal of the final W: \", jnp.diag(model.get_W()))\n",
    "\n",
    "\n",
    "# print in big that training is done\n",
    "print(\"\\n\\n ###########################  Training is done  ########################### \\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define experiment name\n",
    "exp_name = f\"bs_{batch_size}_lrw_{w_learning_rate}_lrh_{h_learning_rate}_lamh_{lam_h}_laml1_{lam_l1}_epochs_{nm_epochs}\"\n",
    "\n",
    "# Create subdirectory in linear folder with the name stored in exp_name\n",
    "save_path = os.path.join('plots/linear_cyclic', exp_name)\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "# Reset to default style and set seaborn style\n",
    "plt.style.use('default')\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Update matplotlib parameters\n",
    "plt.rcParams.update({\n",
    "    'figure.dpi': 300,\n",
    "    'savefig.dpi': 300,\n",
    "    'font.size': 12,\n",
    "    'axes.titlesize': 14,\n",
    "    'axes.labelsize': 12,\n",
    "    'xtick.labelsize': 10,\n",
    "    'ytick.labelsize': 10,\n",
    "    'legend.fontsize': 10,\n",
    "    'figure.titlesize': 16,\n",
    "    'axes.grid': True,\n",
    "    'grid.alpha': 0.4,\n",
    "    'axes.spines.top': False,\n",
    "    'axes.spines.right': False\n",
    "})\n",
    "\n",
    "# Create a figure and subplots using GridSpec with 3x3 layout\n",
    "fig = plt.figure(figsize=(22, 12))\n",
    "gs = fig.add_gridspec(3, 3, height_ratios=[1, 1, 1], width_ratios=[1, 1, 1])\n",
    "\n",
    "# Adjust layout to make room for titles and subtitles\n",
    "plt.subplots_adjust(top=0.92, hspace=0.4, wspace=0.3)\n",
    "\n",
    "# Create axes\n",
    "axes = [fig.add_subplot(gs[i, j]) for i in range(3) for j in range(3)]\n",
    "\n",
    "# Updated plot configurations\n",
    "plot_configs = [\n",
    "    {'metric': MAEs, 'title': 'MAE', 'ylabel': 'MAE', 'color': '#2ecc71', 'ax': axes[0]},\n",
    "    {'metric': SHDs, 'title': 'SHD', 'ylabel': 'SHD', 'color': '#e74c3c', 'ax': axes[1]},\n",
    "    {'metric': SHDs_cyclic, 'title': 'SHD Cyclic', 'ylabel': 'SHD Cyclic', 'color': '#8e44ad', 'ax': axes[2]},\n",
    "    {'metric': F1s, 'title': 'F1 Score', 'ylabel': 'F1 Structure', 'color': '#3498db', 'ax': axes[3]},\n",
    "    {'metric': l1_regs, 'title': 'L1 Regularization', 'ylabel': 'L1', 'color': '#f1c40f', 'ax': axes[4]},\n",
    "    {'metric': h_regs, 'title': 'DAG Constraint', 'ylabel': 'h(W)', 'color': '#e67e22', 'ax': axes[5]},\n",
    "    {'metric': pc_energies, 'title': 'PC Energy', 'ylabel': 'PC Energy', 'color': '#9b59b6', 'ax': axes[6]},\n",
    "    {'metric': objs, 'title': 'Total Objective', 'ylabel': 'Total Loss', 'color': '#1abc9c', 'ax': axes[7]},\n",
    "    {'metric': F1s_cycles, 'title': 'F1 Cyclic', 'ylabel': 'F1 Cyclic', 'color': '#16a085', 'ax': axes[8]}\n",
    "]\n",
    "\n",
    "# Create all subplots\n",
    "for config in plot_configs:\n",
    "    ax = config['ax']\n",
    "\n",
    "    epochs = range(0, len(config['metric']) * every_n_epochs, every_n_epochs)\n",
    "    \n",
    "    # Determine if we should use log scale and/or scaling factor\n",
    "    use_log_scale = config['title'] in ['PC Energy', 'Total Objective']\n",
    "    #scale_factor = 1e4 if config['title'] == 'DAG Constraint' else 1\n",
    "    scale_factor = 1 if config['title'] == 'DAG Constraint' else 1\n",
    "    \n",
    "    # Apply scaling and/or log transform to the metric\n",
    "    metric_values = np.array(config['metric'])\n",
    "    if use_log_scale:\n",
    "        # Add small constant to avoid log(0)\n",
    "        metric_values = np.log10(np.abs(metric_values) + 1e-10)\n",
    "    metric_values = metric_values * scale_factor\n",
    "\n",
    "    # Plot raw data\n",
    "    ax.plot(epochs, metric_values, alpha=0.3, color=config['color'], label='Raw')\n",
    "\n",
    "    # Calculate rolling average\n",
    "    window_size = max(1, len(metric_values) // 50)  # Dynamic window size based on number of epochs\n",
    "    if len(metric_values) > window_size:\n",
    "        rolling_mean = np.convolve(metric_values, np.ones(window_size) / window_size, mode='valid')\n",
    "        ax.plot(epochs[window_size - 1:], rolling_mean, color=config['color'], linewidth=2, label='Rolling Avg')\n",
    "\n",
    "    # Customize subplot\n",
    "    ax.set_xlabel('Epoch', labelpad=10)\n",
    "    # Adjust ylabel based on transformations\n",
    "    ylabel = config['ylabel']\n",
    "    if use_log_scale:\n",
    "        ylabel = f'log10({ylabel})'\n",
    "    if scale_factor != 1:\n",
    "        ylabel = f'{ylabel} (×{int(scale_factor)})'\n",
    "    ax.set_ylabel(ylabel, labelpad=10)\n",
    "    \n",
    "    ax.grid(True, linestyle='--', alpha=0.7)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "\n",
    "    # Add legend for Total Objective plot\n",
    "    if config['title'] == 'Total Objective':\n",
    "        ax.legend(loc='upper right')\n",
    "\n",
    "    # Add note about scaling if applicable\n",
    "    if use_log_scale or scale_factor != 1:\n",
    "        transform_text = []\n",
    "        if use_log_scale:\n",
    "            transform_text.append('log scale')\n",
    "        if scale_factor != 1:\n",
    "            transform_text.append(f'×{int(scale_factor)}')\n",
    "        ax.text(0.02, 0.98, f\"({', '.join(transform_text)})\", \n",
    "                transform=ax.transAxes, \n",
    "                fontsize=8, \n",
    "                verticalalignment='top',\n",
    "                bbox=dict(facecolor='white', alpha=0.8, edgecolor='none'))        \n",
    "\n",
    "# Add overall title and subtitle with adjusted positions\n",
    "fig.suptitle('Training Metrics Over Time', \n",
    "             fontsize=16, \n",
    "             weight='bold', \n",
    "             y=0.95)  # Lower the main title slightly\n",
    "\n",
    "subtitle = f'λ_h = {lam_h}, λ_l1 = {lam_l1}, Weights Learning Rate = {w_learning_rate}'\n",
    "fig.text(0.5, 0.90,  # Adjusted y position for the subtitle\n",
    "         subtitle, \n",
    "         horizontalalignment='center',\n",
    "         fontsize=12,\n",
    "         style='italic')\n",
    "\n",
    "# Adjust layout to make more room for title and subtitle\n",
    "plt.subplots_adjust(top=0.88, hspace=0.4, wspace=0.3)\n",
    "\n",
    "\n",
    "# Save and show the figure as a .pdf file\n",
    "plt.savefig(os.path.join(save_path, 'training_metrics.pdf'), bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "########################## Plotting the DAG comparison  ##########################\n",
    "\n",
    "# Create a separate figure for the adjacency matrices comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Use a better colormap - options:\n",
    "# 'YlOrBr' - Yellow-Orange-Brown (good for sparse matrices)\n",
    "# 'viridis' - Perceptually uniform, colorblind-friendly\n",
    "# 'Greys' - Black and white, professional\n",
    "# 'YlGnBu' - Yellow-Green-Blue, professional\n",
    "cmap = 'YlOrBr'  # Choose one of the above\n",
    "\n",
    "# Plot estimated adjacency matrix (now on the left)\n",
    "im1 = ax1.imshow(compute_binary_adjacency(W), cmap=cmap, interpolation='nearest')\n",
    "ax1.set_title('Estimated DAG', pad=10)\n",
    "plt.colorbar(im1, ax=ax1, fraction=0.046, pad=0.04)\n",
    "ax1.set_xlabel('Node', labelpad=10)\n",
    "ax1.set_ylabel('Node', labelpad=10)\n",
    "\n",
    "# Plot true adjacency matrix (now on the right)\n",
    "im2 = ax2.imshow(B_true, cmap=cmap, interpolation='nearest')\n",
    "ax2.set_title('True DAG', pad=10)\n",
    "plt.colorbar(im2, ax=ax2, fraction=0.046, pad=0.04)\n",
    "ax2.set_xlabel('Node', labelpad=10)\n",
    "ax2.set_ylabel('Node', labelpad=10)\n",
    "\n",
    "# Add overall title\n",
    "fig.suptitle('Estimated vs True DAG Structure', \n",
    "             fontsize=16, \n",
    "             weight='bold', \n",
    "             y=1.05)\n",
    "\n",
    "# Add grid lines to better separate the nodes\n",
    "for ax in [ax1, ax2]:\n",
    "    ax.set_xticks(np.arange(-.5, B_true.shape[0], 1), minor=True)\n",
    "    ax.set_yticks(np.arange(-.5, B_true.shape[0], 1), minor=True)\n",
    "    ax.grid(which=\"minor\", color=\"w\", linestyle='-', linewidth=0.3)\n",
    "    ax.tick_params(which=\"minor\", bottom=False, left=False)\n",
    "\n",
    "# Improve layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the comparison plot as a .pdf file at the specified location\n",
    "plt.savefig(os.path.join(save_path, 'dag_comparison.pdf'), \n",
    "            bbox_inches='tight', \n",
    "            dpi=300,\n",
    "            facecolor='white',\n",
    "            edgecolor='none')\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# Now use a threshold of 0.3 to binarize the weighted adjacency matrix W\n",
    "W_est = np.array(model.get_W())\n",
    "B_est = compute_binary_adjacency(W_est, threshold=0.3)\n",
    "\n",
    "# %%\n",
    "# Check if B_est is indeed a DAG\n",
    "def is_dag(adjacency_matrix):\n",
    "    \"\"\"\n",
    "    Check if a given adjacency matrix represents a Directed Acyclic Graph (DAG).\n",
    "    \n",
    "    Parameters:\n",
    "        adjacency_matrix (numpy.ndarray): A square matrix representing the adjacency of a directed graph.\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if the graph is a DAG, False otherwise.\n",
    "    \"\"\"\n",
    "    # Create a directed graph from the adjacency matrix\n",
    "    graph = nx.DiGraph(adjacency_matrix)\n",
    "    \n",
    "    # Check if the graph is a DAG\n",
    "    return nx.is_directed_acyclic_graph(graph)\n",
    "\n",
    "# Check if the estimated binary adjacency matrix B_est is a DAG\n",
    "is_dag_B_est = is_dag(B_est)\n",
    "print(f\"Is the estimated binary adjacency matrix a DAG? {is_dag_B_est}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Compute the h_reg term for the true weighted adjacency matrix W_true\n",
    "h_reg_true = compute_h_reg(W_true)\n",
    "print(f\"The h_reg term for the TRUE weighted adjacency matrix W_true is: {h_reg_true:.4f}\")\n",
    "# Compute the h_reg term for the estimated weighted adjacency matrix W_est\n",
    "h_reg_est = compute_h_reg(W_est)\n",
    "print(f\"The h_reg term for the EST weighted adjacency matrix W_est is: {h_reg_est:.4f}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# print the number of edges in the true graph and the estimated graph\n",
    "print(f\"The number of edges in the TRUE graph: {np.sum(B_true)}\")\n",
    "print(f\"The number of edges in the EST graph: {np.sum(B_est)}\")\n",
    "\n",
    "# print first 5 rows and columsn of W_est and round values to 4 decimal places and show as non-scientific notation\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"The first 5 rows and columns of the estimated weighted adjacency matrix W_est\\n{}\".format(W_est[:5, :5]))\n",
    "\n",
    "# now show the adjacency matrix of the true graph and the estimated graph side by side\n",
    "plot_adjacency_matrices(true_matrix=B_true, est_matrix=B_est, save_path=os.path.join(save_path, 'adjacency_matrices.png'))\n",
    "\n",
    "\n",
    "# %%\n",
    "# plot est_dag and true_dag\n",
    "GraphDAG(B_est, B_true, save_name=os.path.join(save_path, 'est_dag_true_dag.png'))\n",
    "# calculate accuracy\n",
    "met_pcx = MetricsDAG(B_est, B_true) # expects first arg to be the predicted labels and the second arg to be the true labels\n",
    "print(met_pcx.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the style and color palette\n",
    "sns.set(style=\"whitegrid\")\n",
    "palette = sns.color_palette(\"tab10\")\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(18, 5))  # Adjusting layout to 1 row and 3 columns\n",
    "fig.suptitle('Performance Metrics Over Epochs', fontsize=16, weight='bold')\n",
    "\n",
    "# Plot the Cyclic F1 score\n",
    "sns.lineplot(x=range(len(F1s_cycles)), y=F1s_cycles, ax=axs[0], color=palette[0])\n",
    "axs[0].set_xlabel(\"Epoch\", fontsize=12)\n",
    "axs[0].set_ylabel(\"F1 Cyclic\", fontsize=12)\n",
    "axs[0].grid(True)\n",
    "\n",
    "# Plot the Cyclic SHD\n",
    "sns.lineplot(x=range(len(SHDs)), y=SHDs_cyclic, ax=axs[1], color=palette[2])\n",
    "axs[1].set_xlabel(\"Epoch\", fontsize=12)\n",
    "axs[1].set_ylabel(\"SHD Cyclic\", fontsize=12)\n",
    "axs[1].grid(True)\n",
    "\n",
    "# Plot the vanilla SHD\n",
    "sns.lineplot(x=range(len(SHDs)), y=SHDs, ax=axs[2], color=palette[1])\n",
    "axs[2].set_xlabel(\"Epoch\", fontsize=12)\n",
    "axs[2].set_ylabel(\"SHD Structure\", fontsize=12)\n",
    "axs[2].grid(True)\n",
    "\n",
    "# Improve layout and show the plot\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjust layout to fit the suptitle\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print first 5 rows and columsn of W_est and round values to 4 decimal places and show as non-scientific notation\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "print(\"The first 4 rows and columns of the estimated weighted adjacency matrix W_est\\n{}\".format(W_est[:11, :11]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now show the adjacency matrix of the true graph and the estimated graph side by side\n",
    "plot_adjacency_matrices(B_true, B_est)\n",
    "\n",
    "# print the number of edges in the true graph and the estimated graph\n",
    "print(f\"The number of edges in the estimated graph: {np.sum(B_est)}\")\n",
    "print(f\"The number of edges in the true graph: {np.sum(B_true)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot est_dag and true_dag\n",
    "GraphDAG(B_est, B_true)\n",
    "# calculate accuracy\n",
    "met_pcax = MetricsDAG(B_est, B_true)\n",
    "print(met_pcax.metrics)\n",
    "\n",
    "# print experiment name\n",
    "print(exp_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the below setup is taken from the nodags-flows/validation /run_synthetic_benchmark.py file which runs on linear data\n",
    "\n",
    "# Initialize the NODAGS model with linear functions\n",
    "resblock = resflow_train_test_wrapper(\n",
    "    n_nodes=X.shape[1],\n",
    "    batch_size=128,\n",
    "    l1_reg=False,\n",
    "    fun_type='lin-mlp',  # Linear version\n",
    "    act_fun='none', # No activation function\n",
    "    lr=1e-3,\n",
    "    lip_const=0.99,\n",
    "    epochs=100,\n",
    "    optim='adam',\n",
    "    n_hidden=0,\n",
    "    thresh_val=0.05,\n",
    "    centered=False,\n",
    "    v=False,\n",
    "    inline=False,\n",
    "    upd_lip=False,\n",
    "    full_input=False,\n",
    ")\n",
    "\n",
    "# Train the model with only observational data\n",
    "resblock.train([X], [[]], batch_size=64)\n",
    "\n",
    "# get the weighted adjacency matrix\n",
    "nodags_W_est = resblock.get_adjacency()\n",
    "# get the binary adjacency matrix\n",
    "nodags_B_est = (nodags_W_est > 0.3)\n",
    "\n",
    "# plot est_dag and true_dag\n",
    "GraphDAG(nodags_B_est, B_true)\n",
    "# calculate accuracy\n",
    "met_nodags = MetricsDAG(nodags_B_est, B_true) # expects first arg to be the predicted labels and the second arg to be the true labels\n",
    "print(met_nodags.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lingd import LiNGD\n",
    "\n",
    "# benchmark model 1 LiNG\n",
    "ling = LiNGD(k=1)\n",
    "ling.fit(X)\n",
    "W_ling = ling._adjacency_matrices[0].T # 0 because we are using k=1, Transpose to make each row correspond to parents\n",
    "ling_B_est = compute_binary_adjacency(W_ling)\n",
    "\n",
    "# plot est_dag and true_dag\n",
    "GraphDAG(ling_B_est, B_true)\n",
    "# calculate accuracy\n",
    "met_ling = MetricsDAG(ling_B_est, B_true)\n",
    "print(met_ling.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# benchmark model 2 dglearn\n",
    "from dglearn import *\n",
    "\n",
    "# learn structure using tabu search, plot learned structure\n",
    "tabu_length = 4\n",
    "patience = 4\n",
    "max_iter = 10\n",
    "\n",
    "manager = CyclicManager(X, bic_coef=0.5)\n",
    "dglearn_B_est, best_score, log = tabu_search(manager, tabu_length, patience, max_iter=max_iter, first_ascent=False, verbose=1) # returns a binary matrix as learned support\n",
    "\n",
    "# perform virtual edge correction\n",
    "print(\"virtual edge correction...\")\n",
    "dglearn_B_est = virtual_refine(manager, dglearn_B_est, patience=0, max_iter=max_iter, max_path_len=6, verbose=1)\n",
    "\n",
    "# remove any reducible edges\n",
    "dglearn_B_est = reduce_support(dglearn_B_est, fill_diagonal=False)\n",
    "# convert dglearn_B_est to boolean matrix in case it is not\n",
    "dglearn_B_est = dglearn_B_est.astype(bool).T # Transpose to make each row correspond to parents\n",
    "\n",
    "# plot est_dag and true_dag\n",
    "GraphDAG(dglearn_B_est, B_true)\n",
    "# calculate accuracy\n",
    "met_dglearn = MetricsDAG(dglearn_B_est, B_true)\n",
    "print(met_dglearn.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# benchmark model 3 FRP\n",
    "\n",
    "edge_penalty = 0.5 * np.log(X.shape[0]) / X.shape[0] # BIC choice\n",
    "# Run FRP\n",
    "frp_result = run_filter_rank_prune(\n",
    "        X, \n",
    "        loss_type=\"kld\",\n",
    "        reg_type=\"scad\",\n",
    "        reg_params={\"lam\": edge_penalty, \"gamma\": 3.7},\n",
    "\t\tedge_penalty=edge_penalty,\n",
    "        n_inits=5,\n",
    "        n_threads=200,\n",
    "        parcorr_thrs=0.1 * (X.shape[0] / 1000)**(-1/4),\n",
    "        use_loss_cache=True,\n",
    "        seed=seed,\n",
    "        verbose=True,\n",
    ")\n",
    "\n",
    "frp_B_est = frp_result[\"learned_support\"] # Transpose to make each row correspond to parents\n",
    "\n",
    "# plot est_dag and true_dag\n",
    "GraphDAG(frp_B_est, B_true)\n",
    "# calculate accuracy\n",
    "met_frp = MetricsDAG(frp_B_est, B_true)\n",
    "print(met_frp.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from castle.algorithms import Notears\n",
    "\n",
    "# notears learn\n",
    "nt = Notears(lambda1=1e-2, h_tol=1e-5, max_iter=1000, no_dag_constraint=True) # default loss_type is 'l2', F1 of 27%\n",
    "#nt = Notears(lambda1=1e-2, h_tol=1e-5, max_iter=10000, loss_type='logistic')\n",
    "#nt = Notears(lambda1=1e-2, h_tol=1e-5, max_iter=10000, loss_type='poisson')\n",
    "nt.learn(X)\n",
    "\n",
    "# plot est_dag and true_dag\n",
    "nt_W_est = nt.weight_causal_matrix\n",
    "nt_B_est = nt.causal_matrix\n",
    "GraphDAG(nt_B_est, B_true)\n",
    "\n",
    "# calculate accuracy\n",
    "met_nt = MetricsDAG(nt_B_est, B_true)\n",
    "print(met_nt.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from castle.algorithms import GOLEM\n",
    "#gol = GOLEM(num_iter=2e4, lambda_2=1e4/X.shape[1], lambda_1=1e-2)\n",
    "#gol = GOLEM(num_iter=2e4, lambda_2=0.0)  setting h_reg to 0 still allows model to be fit (no error thrown)\n",
    "gol = GOLEM(num_iter=15e3, non_equal_variances=True, lambda_1=1e-2, lambda_2=0.0, device_type='gpu') # we deactivate DAG constraint by setting lambda_2=0.0 as done in FRP paper\n",
    "#g = GOLEM(num_iter=2e4, non_equal_variances=False) # F1 of 68%, default non_equal_variances=True\n",
    "gol.learn(X)\n",
    "\n",
    "# plot est_dag and true_dag\n",
    "gol_B_est = gol.causal_matrix\n",
    "GraphDAG(gol_B_est, B_true)\n",
    "\n",
    "# calculate accuracy\n",
    "met_gol = MetricsDAG(gol_B_est, B_true)\n",
    "print(met_gol.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: update metrics for newly added models, then create a utility function to print the metrics of all models\n",
    "\n",
    "# Show metrics of all models (ours, LiNG, dglearn, FRP, Notears, GOLEM, NODAGS)\n",
    "\n",
    "# use the function compute_model_fit to get the BIC and AIC scores for our method\n",
    "bic_our, aic_our = compute_model_fit(B_est, X)\n",
    "bic_ling, aic_ling = compute_model_fit(ling_B_est, X) # use W_ling otherwise we get -inf for bic and aic\n",
    "bic_dglearn, aic_dglearn = compute_model_fit(dglearn_B_est, X)\n",
    "bic_frp, aic_frp = compute_model_fit(frp_B_est, X)\n",
    "bic_nt, aic_nt = compute_model_fit(nt_B_est, X)\n",
    "bic_gol, aic_gol = compute_model_fit(gol_B_est, X)\n",
    "bic_nodags, aic_nodags = compute_model_fit(nodags_B_est, X)\n",
    "\n",
    "print(f\"BIC and AIC scores for our method: {bic_our:.3f}, {aic_our:.3f}\")\n",
    "print(f\"BIC and AIC scores for LinG method: {bic_ling:.3f}, {aic_ling:.3f}\")\n",
    "print(f\"BIC and AIC scores for dglearn method: {bic_dglearn:.3f}, {aic_dglearn:.3f}\")\n",
    "print(f\"BIC and AIC scores for FRP method: {bic_frp:.3f}, {aic_frp:.3f}\")\n",
    "print(f\"BIC and AIC scores for Notears method: {bic_nt:.3f}, {aic_nt:.3f}\")\n",
    "print(f\"BIC and AIC scores for GOLEM method: {bic_gol:.3f}, {aic_gol:.3f}\")\n",
    "print(f\"BIC and AIC scores for NODAGS method: {bic_nodags:.3f}, {aic_nodags:.3f}\")\n",
    "\n",
    "# Compute SHD and cycle F1 for all methods\n",
    "cycle_shd_our = compute_cycle_SHD(B_true_EC, B_est)\n",
    "cycle_shd_ling = compute_cycle_SHD(B_true_EC, ling_B_est)  # LinG method\n",
    "cycle_shd_dglearn = compute_cycle_SHD(B_true_EC, dglearn_B_est)  # dglearn method\n",
    "cycle_shd_frp = compute_cycle_SHD(B_true_EC, frp_B_est)  # FRP method\n",
    "cycle_shd_nt = compute_cycle_SHD(B_true_EC, nt_B_est)  # Notears method\n",
    "cycle_shd_gol = compute_cycle_SHD(B_true_EC, gol_B_est)  # GOLEM method\n",
    "cycle_shd_nodags = compute_cycle_SHD(B_true_EC, nodags_B_est)  # NODAGS method\n",
    "\n",
    "# B or B.T can be used for cycle F1, it doesn't matter\n",
    "cycle_f1_our = compute_cycle_F1(B_true, B_est)\n",
    "cycle_f1_ling = compute_cycle_F1(B_true, ling_B_est)\n",
    "cycle_f1_dglearn = compute_cycle_F1(B_true, dglearn_B_est)\n",
    "cycle_f1_frp = compute_cycle_F1(B_true, frp_B_est)\n",
    "cycle_f1_nt = compute_cycle_F1(B_true, nt_B_est)\n",
    "cycle_f1_gol = compute_cycle_F1(B_true, gol_B_est)\n",
    "cycle_f1_nodags = compute_cycle_F1(B_true, nodags_B_est)\n",
    "\n",
    "# Print SHD and cycle F1 scores\n",
    "print(f\"SHD cyclic and cycle F1 using our method: {cycle_shd_our}, {cycle_f1_our}\")\n",
    "print(f\"SHD cyclic and cycle F1 using LinG method: {cycle_shd_ling}, {cycle_f1_ling}\")\n",
    "print(f\"SHD cyclic and cycle F1 using dglearn method: {cycle_shd_dglearn}, {cycle_f1_dglearn}\")\n",
    "print(f\"SHD cyclic and cycle F1 using FRP method: {cycle_shd_frp}, {cycle_f1_frp}\")\n",
    "print(f\"SHD cyclic and cycle F1 using Notears method: {cycle_shd_nt}, {cycle_f1_nt}\")\n",
    "print(f\"SHD cyclic and cycle F1 using GOLEM method: {cycle_shd_gol}, {cycle_f1_gol}\")\n",
    "print(f\"SHD cyclic and cycle F1 using NODAGS method: {cycle_shd_nodags}, {cycle_f1_nodags}\")\n",
    "\n",
    "# compute_CSS using compute_CSS(shd_cyclic, cycle_f1, epsilon=1e-8)\n",
    "css_our = compute_CSS(cycle_shd_our, cycle_f1_our, epsilon=1e-8)\n",
    "css_ling = compute_CSS(cycle_shd_ling, cycle_f1_ling, epsilon=1e-8)\n",
    "css_dglearn = compute_CSS(cycle_shd_dglearn, cycle_f1_dglearn, epsilon=1e-8)\n",
    "css_frp = compute_CSS(cycle_shd_frp, cycle_f1_frp, epsilon=1e-8)\n",
    "css_nt = compute_CSS(cycle_shd_nt, cycle_f1_nt, epsilon=1e-8)\n",
    "css_gol = compute_CSS(cycle_shd_gol, cycle_f1_gol, epsilon=1e-8)\n",
    "css_nodags = compute_CSS(cycle_shd_nodags, cycle_f1_nodags, epsilon=1e-8)\n",
    "\n",
    "# compute cycle KLD using compute_cycle_KLD\n",
    "cycle_kld_our = compute_cycle_KLD(prec_matrix, B_est)[0]\n",
    "cycle_kld_ling = compute_cycle_KLD(prec_matrix, ling_B_est)[0]\n",
    "cycle_kld_dglearn = compute_cycle_KLD(prec_matrix, dglearn_B_est)[0]\n",
    "cycle_kld_frp = compute_cycle_KLD(prec_matrix, frp_B_est)[0]\n",
    "cycle_kld_nt = compute_cycle_KLD(prec_matrix, nt_B_est)[0]\n",
    "cycle_kld_gol = compute_cycle_KLD(prec_matrix, gol_B_est)[0]\n",
    "cycle_kld_nodags = compute_cycle_KLD(prec_matrix, nodags_B_est)[0]\n",
    "\n",
    "# print the cycle KLD scores and CSS scores\n",
    "print(f\"Cycle KLD and CSS using our method: {cycle_kld_our}, {css_our}\")\n",
    "print(f\"Cycle KLD and CSS using LinG method: {cycle_kld_ling}, {css_ling}\")\n",
    "print(f\"Cycle KLD and CSS using dglearn method: {cycle_kld_dglearn}, {css_dglearn}\")\n",
    "print(f\"Cycle KLD and CSS using FRP method: {cycle_kld_frp}, {css_frp}\")\n",
    "print(f\"Cycle KLD and CSS using Notears method: {cycle_kld_nt}, {css_nt}\")\n",
    "print(f\"Cycle KLD and CSS using GOLEM method: {cycle_kld_gol}, {css_gol}\")\n",
    "print(f\"Cycle KLD and CSS using NODAGS method: {cycle_kld_nodags}, {css_nodags}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################ MISC NON CYCLIC MODELS ################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_KLD(B_true, B_est):\n",
    "    \"\"\"\n",
    "        Computes the KL-Divergence between two multivariate normal\n",
    "        distributions with given precision matrices,\n",
    "        assuming that they are both zero mean\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute the precision matrices\n",
    "    def precision_matrix_simplified(B):\n",
    "        \"\"\"\n",
    "        Given a B matrix, compute precision matrix assuming equal variances.\n",
    "        \"\"\"\n",
    "        dim = B.shape[0]\n",
    "        Q = np.eye(dim) - B\n",
    "        return Q @ Q.T  # Ignoring logvars factor since variances are constant in equal variances setup\n",
    "\n",
    "    prec_true = precision_matrix_simplified(B_true)\n",
    "    prec_est = precision_matrix_simplified(B_est)\n",
    "\n",
    "    assert np.all(prec_true.shape == prec_est.shape)\n",
    "    assert prec_true.shape[0] == prec_true.shape[1]\n",
    "    dim = prec_true.shape[0]\n",
    "\n",
    "    (sign1, logdet1) = np.linalg.slogdet(prec_true)\n",
    "    (sign2, logdet2) = np.linalg.slogdet(prec_est)\n",
    "\n",
    "    return 0.5*(sign1*logdet1 - sign2*logdet2 - dim + np.trace(prec_est@np.linalg.inv(prec_true)))\n",
    "\n",
    "\n",
    "# compute KLD for all methods\n",
    "kld_our = compute_KLD(B_true, B_est)\n",
    "kld_ling = compute_KLD(B_true, ling_B_est)\n",
    "kld_dglearn = compute_KLD(B_true, dglearn_B_est)\n",
    "kld_frp = compute_KLD(B_true, frp_B_est)\n",
    "kld_nt = compute_KLD(B_true, nt_B_est)\n",
    "kld_gol = compute_KLD(B_true, gol_B_est)\n",
    "kld_nodags = compute_KLD(B_true, nodags_B_est)\n",
    "\n",
    "# print the KLD scores\n",
    "print(f\"KLD using our method: {kld_our}\")\n",
    "print(f\"KLD using LinG method: {kld_ling}\")\n",
    "print(f\"KLD using dglearn method: {kld_dglearn}\")\n",
    "print(f\"KLD using FRP method: {kld_frp}\")\n",
    "print(f\"KLD using Notears method: {kld_nt}\")\n",
    "print(f\"KLD using GOLEM method: {kld_gol}\")\n",
    "print(f\"KLD using NODAGS method: {kld_nodags}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from castle.common import GraphDAG\n",
    "from castle.metrics import MetricsDAG\n",
    "from castle.datasets import DAG, IIDSimulation\n",
    "from castle.algorithms import GOLEM\n",
    "\n",
    "# GOLEM learn\n",
    "#gol = GOLEM(num_iter=2e4, lambda_2=1e4/X.shape[1], lambda_1=1e-2)\n",
    "#gol = GOLEM(num_iter=2e4, lambda_2=0.0)  setting h_reg to 0 still allows model to be fit (no error thrown)\n",
    "gol = GOLEM(num_iter=2e4, non_equal_variances=True, lambda_1=1e-2) # F1 of 60%\n",
    "#g = GOLEM(num_iter=2e4, non_equal_variances=False) # F1 of 68%, default non_equal_variances=True\n",
    "gol.learn(X)\n",
    "\n",
    "# plot est_dag and true_dag\n",
    "GraphDAG(gol.causal_matrix, B_true)\n",
    "\n",
    "# calculate accuracy\n",
    "gol_met = MetricsDAG(gol.causal_matrix, B_true)\n",
    "print(gol_met.metrics)\n",
    "print(\"The number of edges in golem graph: \", np.sum(gol.causal_matrix))\n",
    "print(\"The number of edges in true graph: \", np.sum(B_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from castle.algorithms import Notears\n",
    "\n",
    "# notears learn\n",
    "nt = Notears(lambda1=1e-3, h_tol=1e-5, max_iter=100) # default loss_type is 'l2', F1 of 27%\n",
    "#nt = Notears(lambda1=1e-2, h_tol=1e-5, max_iter=10000, loss_type='logistic')\n",
    "#nt = Notears(lambda1=1e-2, h_tol=1e-5, max_iter=10000, loss_type='poisson')\n",
    "nt.learn(X)\n",
    "\n",
    "# plot est_dag and true_dag\n",
    "GraphDAG(nt.causal_matrix, B_true)\n",
    "\n",
    "# calculate accuracy\n",
    "nt_met = MetricsDAG(nt.causal_matrix, B_true)\n",
    "print(nt_met.metrics)\n",
    "print(\"The number of edges in notears graph: \", np.sum(nt.causal_matrix))\n",
    "print(\"The number of edges in true graph: \", np.sum(B_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from castle.algorithms import PC\n",
    "\n",
    "# A variant of PC-algorithm, one of [`original`, `stable`, `parallel`]\n",
    "pc = PC(variant='parallel', alpha=0.05, ci_test='fisherz') # F1 of 15%\n",
    "#pc = PC(variant='stable', alpha=0.05) # F1 of 17%\n",
    "#pc = PC(variant='original', alpha=0.05) # F1 of 19%\n",
    "pc.learn(X)\n",
    "\n",
    "# plot est_dag and true_dag\n",
    "GraphDAG(pc.causal_matrix, B_true)\n",
    "\n",
    "# calculate accuracy\n",
    "pc_met = MetricsDAG(pc.causal_matrix, B_true)\n",
    "print(pc_met.metrics)\n",
    "print(\"The number of edges in pc graph: \", np.sum(pc.causal_matrix))\n",
    "print(\"The number of edges in true graph: \", np.sum(B_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from castle.algorithms.ges.ges import GES\n",
    "\n",
    "# GES learn\n",
    "# method is one of ['r2', 'scatter']\n",
    "ges = GES() # F1 of 42%\n",
    "#ges = GES(criterion='bic', method='scatter') # F1 of 47%\n",
    "#ges = GES(criterion='bic', method='r2') # F1 of 4%\n",
    "\n",
    "# learn the graph structure\n",
    "# create X_ges that adds noise to X because GES complains about singular matrix\n",
    "X_ges = X + np.random.normal(0, 1, X.shape)**4\n",
    "ges.learn(X)\n",
    "\n",
    "# plot est_dag and true_dag\n",
    "GraphDAG(ges.causal_matrix, B_true)\n",
    "\n",
    "# calculate accuracy\n",
    "ges_met = MetricsDAG(ges.causal_matrix, B_true)\n",
    "print(ges_met.metrics)\n",
    "print(\"The number of edges in ges graph: \", np.sum(ges.causal_matrix))\n",
    "print(\"The number of edges in true graph: \", np.sum(B_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from castle.algorithms import DirectLiNGAM\n",
    "\n",
    "# measure : {'pwling', 'kernel'}, default='pwling'\n",
    "g_ling = DirectLiNGAM(measure='pwling') # F1 of 74%\n",
    "#g = DirectLiNGAM(measure='kernel') # F1 of nan - takes too long to run (even after 15 minutes not done)\n",
    "g_ling.learn(X)\n",
    "\n",
    "# plot est_dag and true_dag\n",
    "GraphDAG(g_ling.causal_matrix, B_true)\n",
    "\n",
    "# calculate accuracy\n",
    "g_ling_met = MetricsDAG(g_ling.causal_matrix, B_true)\n",
    "print(g_ling_met.metrics)\n",
    "print(\"The number of edges in directlingam graph: \", np.sum(g_ling.causal_matrix))\n",
    "print(\"The number of edges in true graph: \", np.sum(B_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from castle.algorithms import ICALiNGAM\n",
    "\n",
    "# ICALiNGAM learn\n",
    "# max_iter : int, optional (default=1000)\n",
    "#g = ICALiNGAM(max_iter=1000) # F1 of 48%\n",
    "g_ica = ICALiNGAM(max_iter=1000) # F1 of 51%\n",
    "g_ica.learn(X)\n",
    "\n",
    "# plot est_dag and true_dag\n",
    "ica_W_est = g_ica.weight_causal_matrix\n",
    "ica_B_est = g_ica.causal_matrix\n",
    "GraphDAG(ica_B_est, B_true)\n",
    "\n",
    "# calculate accuracy\n",
    "met_ica = MetricsDAG(ica_B_est, B_true)\n",
    "print(met_ica.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shd_dg = min_colperm_shd(equiv_class, learned_support) # dglearn\n",
    "#print(\"SHD to nearest member of equivalence class using dglearn: %d\" % shd_dg)\n",
    "\n",
    "# print compute_cycle_SHD and compute_cycle_F1 for all methods, usage: compute_cycle_F1(B_true, B_est), directly print both with method name in single line\n",
    "shd_our = compute_cycle_SHD(B_true_EC, B_est) # our method\n",
    "shd_gol = compute_cycle_SHD(B_true_EC, gol.causal_matrix) # golem\n",
    "shd_nt = compute_cycle_SHD(B_true_EC, nt.causal_matrix) # notears\n",
    "shd_icag = compute_cycle_SHD(B_true_EC, g_ica.causal_matrix) # icalingam\n",
    "shd_ling = compute_cycle_SHD(B_true_EC, g_ling.causal_matrix) # lingam\n",
    "\n",
    "cycle_f1_our = compute_cycle_F1(B_true, B_est, verbose=True)\n",
    "cycle_f1_gol = compute_cycle_F1(B_true, gol.causal_matrix, verbose=True)\n",
    "cycle_f1_nt = compute_cycle_F1(B_true, nt.causal_matrix, verbose=True)\n",
    "cycle_f1_icag = compute_cycle_F1(B_true, g_ica.causal_matrix, verbose=True)\n",
    "cycle_f1_ling = compute_cycle_F1(B_true, g_ling.causal_matrix, verbose=True)\n",
    "\n",
    "print(f\"SHD cyclic and cycle F1 using our method: {shd_our}, {cycle_f1_our}\")\n",
    "print(f\"SHD cyclic and cycle F1 using golem: {shd_gol}, {cycle_f1_gol}\")\n",
    "print(f\"SHD cyclic and cycle F1 using notears: {shd_nt}, {cycle_f1_nt}\")\n",
    "print(f\"SHD cyclic and cycle F1 using icalingam: {shd_icag}, {cycle_f1_icag}\")\n",
    "print(f\"SHD cyclic and cycle F1 using lingam: {shd_ling}, {cycle_f1_ling}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dglearn import *\n",
    "\n",
    "# learn structure using tabu search, plot learned structure\n",
    "tabu_length = X.shape[1] # number of variables\n",
    "patience = 5\n",
    "\n",
    "manager = CyclicManager(X, bic_coef=0.5)\n",
    "learned_support, best_score, log = tabu_search(manager, tabu_length, patience, first_ascent=False, verbose=1) # returns a binary matrix as learned support\n",
    "# the above does not finish running (within 2 hours) for a graph with 15 nodes and 4000 samples # need to run overnight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform virtual edge correction\n",
    "print(\"virtual edge correction...\")\n",
    "learned_support = virtual_refine(manager, learned_support, patience=0, max_path_len=6, verbose=1)\n",
    "\n",
    "# remove any reducible edges\n",
    "learned_support = reduce_support(learned_support, fill_diagonal=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot est_dag and true_dag\n",
    "GraphDAG(learned_support, B_true)\n",
    "\n",
    "# calculate accuracy\n",
    "dg_met = MetricsDAG(learned_support, B_true)\n",
    "print(dg_met.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get edge list from B_true using networkx\n",
    "G_true = nx.DiGraph(B_true)\n",
    "edges_true = list(G_true.edges())\n",
    "# compute performance metric: SHD\n",
    "true_graph = AdjacencyStucture(X.shape[1], edges_true)\n",
    "search = GraphEquivalenceSearch(true_graph)\n",
    "search.search_dfs()\n",
    "equiv_class = [binary2array(bstr) for bstr in search.visited_graphs]\n",
    "#shd_dg = min_colperm_shd(equiv_class, learned_support) # dglearn\n",
    "shd_our = min_colperm_shd(equiv_class, B_est) # our method\n",
    "shd_gol = min_colperm_shd(equiv_class, gol.causal_matrix) # golem\n",
    "shd_nt = min_colperm_shd(equiv_class, nt.causal_matrix) # notears\n",
    "shd_icag = min_colperm_shd(equiv_class, g_ica.causal_matrix) # icalingam\n",
    "shd_ling = min_colperm_shd(equiv_class, g_ling.causal_matrix) # lingam\n",
    "#print(\"SHD to nearest member of equivalence class using dglearn: %d\" % shd_dg)\n",
    "print(\"SHD to nearest member of equivalence class using our method: %d\" % shd_our)\n",
    "print(\"SHD to nearest member of equivalence class using golem: %d\" % shd_gol)\n",
    "print(\"SHD to nearest member of equivalence class using notears: %d\" % shd_nt)\n",
    "print(\"SHD to nearest member of equivalence class using icalingam: %d\" % shd_icag)\n",
    "print(\"SHD to nearest member of equivalence class using lingam: %d\" % shd_ling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of graphs in the equivalence class: %d\" % len(B_true_EC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pcax24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
