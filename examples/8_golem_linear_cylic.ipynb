{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting CUDA device(s) : [0]\n",
      "2025-02-13 15:32:37,386 - /share/amine.mcharrak/miniconda3/envs/pcax24/lib/python3.10/site-packages/castle/backend/__init__.py[line:36] - INFO: You can use `os.environ['CASTLE_BACKEND'] = backend` to set the backend(`pytorch` or `mindspore`).\n",
      "2025-02-13 15:32:37,395 - /share/amine.mcharrak/miniconda3/envs/pcax24/lib/python3.10/site-packages/castle/algorithms/__init__.py[line:36] - INFO: You are using ``pytorch`` as the backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CyclicManager is imported from: /home/amine.mcharrak/frp/frp_dglearn/learning/cyclic_manager.py\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "mp.set_start_method(\"fork\", force=True)\n",
    "#mp.set_start_method(\"spawn\", force=True)\n",
    "#mp.set_start_method(\"forkserver\", force=True)\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# choose the GPU to use\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "# disable preallocation of memory\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\n",
    "os.environ[\"JAX_PLATFORM_NAME\"] = \"cuda\"  # Force JAX to use CUDA\n",
    "\n",
    "# pcx\n",
    "import pcx as px\n",
    "import pcx.predictive_coding as pxc\n",
    "import pcx.nn as pxnn\n",
    "import pcx.functional as pxf\n",
    "import pcx.utils as pxu\n",
    "\n",
    "# 3rd party\n",
    "import jax\n",
    "from jax import jit\n",
    "import jax.numpy as jnp\n",
    "import jax.numpy.linalg as jax_numpy_linalg # for expm()\n",
    "import jax.scipy.linalg as jax_scipy_linalg # for slogdet()\n",
    "import jax.random as random\n",
    "import optax\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.api.types import is_float_dtype\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import timeit\n",
    "\n",
    "# own\n",
    "import causal_helpers\n",
    "from causal_model import Complete_Graph\n",
    "from causal_helpers import is_dag_nx, MAE, compute_binary_adjacency, compute_h_reg, notears_dag_constraint, dagma_dag_constraint\n",
    "from causal_helpers import simulate_dag, simulate_parameter, simulate_linear_sem, simulate_linear_sem_cyclic\n",
    "from causal_helpers import load_adjacency_matrix, set_random_seed, plot_adjacency_matrices\n",
    "from causal_helpers import load_graph, load_adjacency_matrix\n",
    "from causal_helpers import plot_weights_distribution, analyze_weight_thresholds\n",
    "from causal_metrics import compute_F1_directed, compute_F1_skeleton, compute_AUPRC, compute_AUROC, compute_model_fit\n",
    "from causal_metrics import compute_cycle_F1, compute_cycle_SHD, compute_cycle_KLD, compute_CSS, compute_min_KLD  # CSS: Cyclic Structure Score\n",
    "from connectome_cyclic_data_generator import sample_cyclic_data\n",
    "\n",
    "#################### NODAGS #########################\n",
    "\n",
    "from nodags_flows.models.resblock_trainer import resflow_train_test_wrapper\n",
    "\n",
    "##################### DGLearn ############################\n",
    "\n",
    "from dglearn.dglearn.dg.adjacency_structure import AdjacencyStucture\n",
    "from dglearn.dglearn.dg.graph_equivalence_search import GraphEquivalenceSearch\n",
    "from dglearn.dglearn.dg.converter import binary2array\n",
    "from dglearn.dglearn.learning.cyclic_manager import CyclicManager\n",
    "from dglearn.dglearn.dg.reduction import reduce_support\n",
    "from dglearn.dglearn.learning.search.virtual import virtual_refine\n",
    "from dglearn.dglearn.learning.search.tabu import tabu_search\n",
    "\n",
    "###################### LiNGD #########################\n",
    "\n",
    "from lingd import LiNGD\n",
    "\n",
    "##################### FRP ###########################\n",
    "\n",
    "from frp.run_causal_discovery import  run_filter_rank_prune, run_dglearn\n",
    "\n",
    "#####################################################\n",
    "\n",
    "# Set random seed\n",
    "#seed = 44 # main seed for reproducibility\n",
    "seed = 55\n",
    "set_random_seed(seed)\n",
    "\n",
    "# causal libraries\n",
    "import cdt, castle\n",
    "from castle.algorithms import GOLEM, Notears\n",
    "\n",
    "# causal metrics\n",
    "from cdt.metrics import precision_recall, SHD, SID\n",
    "from castle.metrics import MetricsDAG\n",
    "from castle.common import GraphDAG\n",
    "from causallearn.graph.SHD import SHD as SHD_causallearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Data loaded successfully!\n",
      "\n",
      "Size of equivalence class of true graph for /share/amine.mcharrak/cyclic_data_final/10NWS34_linear_cyclic_GAUSS-EV_seed_1: 21\n",
      "\n",
      "Number of cycles in G_true for /share/amine.mcharrak/cyclic_data_final/10NWS34_linear_cyclic_GAUSS-EV_seed_1: 1\n",
      "\n",
      "Cycles in G_true:  [[8, 7, 9]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "#   # fetch dataset \n",
    "# abalone = fetch_ucirepo(id=1) \n",
    "  \n",
    "# # data (as pandas dataframes) \n",
    "# X = abalone.data.features\n",
    "# y = abalone.data.targets\n",
    "  \n",
    "# # metadata \n",
    "# #print(abalone.metadata) \n",
    "\n",
    "# # now merge X and y to create a single dataframe and give the columns the correct names using abalone.variables.name.values\n",
    "# df = pd.concat([X, y], axis=1)\n",
    "# df.columns = abalone.variables.name.values.tolist()\n",
    "# print(df.head())\n",
    "# print()\n",
    "# # show # of unique values in each column\n",
    "# print(df.nunique())\n",
    "# # finally convert the Rings variable to a binary variable by setting the threshold to mean(rings)\n",
    "# df['Rings'] = df['Rings'] > df['Rings'].mean()\n",
    "# # then convert to integer\n",
    "# df['Rings'] = df['Rings'].astype(int)\n",
    "# # also replace the values in Sex with integers\n",
    "# df['Sex'] = df['Sex'].map({'M': 0, 'F': 1, 'I': 2})\n",
    "\n",
    "# # now show the first 5 rows of the dataframe\n",
    "# print(df.head())\n",
    "\n",
    "# # Create a boolean list for continuous variables (any float dtype)\n",
    "# is_cont_node = df.dtypes.map(is_float_dtype).tolist()\n",
    "\n",
    "# # Print the result\n",
    "# print(is_cont_node)\n",
    "\n",
    "# # plot the distribution of all variables in the dataframe\n",
    "# df.hist(figsize=(15, 10))\n",
    "# plt.show()\n",
    "\n",
    "# Base directory for the dataset\n",
    "#base_path = '/share/amine.mcharrak/cyclic_data_final/10ER41_linear_cyclic_GAUSS-EV_seed_1'\n",
    "#base_path = '/share/amine.mcharrak/cyclic_data_final/20ER59_linear_cyclic_GAUSS-EV_seed_1'\n",
    "base_path = '/share/amine.mcharrak/cyclic_data_final/10NWS34_linear_cyclic_GAUSS-EV_seed_1'\n",
    "\n",
    "# Load adjacency matrices and precision matrix from the base folder\n",
    "adj_matrix = pd.read_csv(os.path.join(base_path, 'adj_matrix.csv'), header=None)\n",
    "weighted_adj_matrix = pd.read_csv(os.path.join(base_path, 'W_adj_matrix.csv'), header=None)\n",
    "prec_matrix = pd.read_csv(os.path.join(base_path, 'prec_matrix.csv'), header=None)\n",
    "\n",
    "# Specify the sample size you want to load (adjust as needed)\n",
    "num_samples = 5000  # Change to 500, 1000, etc., as needed\n",
    "\n",
    "# Load the data from the corresponding sample size folder\n",
    "data_path = os.path.join(base_path, f\"n_samples_{num_samples}\", \"train.csv\")\n",
    "data = pd.read_csv(data_path, header=None)\n",
    "\n",
    "print(\"âœ… Data loaded successfully!\\n\")\n",
    "\n",
    "n_vars = data.shape[1]\n",
    "\n",
    "B_true = adj_matrix.values\n",
    "X = data.values\n",
    "W_true = weighted_adj_matrix.values\n",
    "\n",
    "# Get edge list from B_true using networkx\n",
    "G_true = nx.DiGraph(B_true)\n",
    "edges_list = list(G_true.edges())\n",
    "\n",
    "# Compute performance metric: SHD\n",
    "true_graph = AdjacencyStucture(n_vars=n_vars, edge_list=edges_list)\n",
    "search = GraphEquivalenceSearch(true_graph)\n",
    "search.search_dfs()\n",
    "\n",
    "B_true_EC = [binary2array(bstr) for bstr in search.visited_graphs]\n",
    "print(f\"Size of equivalence class of true graph for {base_path}: {len(B_true_EC)}\\n\")\n",
    "\n",
    "# Check if G_true is cyclic by listing all cycles\n",
    "num_cycles = len(list(nx.simple_cycles(G_true)))\n",
    "cycles = list(nx.simple_cycles(G_true))\n",
    "print(f\"Number of cycles in G_true for {base_path}: {num_cycles}\\n\")\n",
    "print(\"Cycles in G_true: \", cycles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()\n",
    "# show unique values in each column\n",
    "print(data.nunique())\n",
    "# Determine if each variable is continuous or discrete based on the number of unique values\n",
    "is_cont_node = np.array([True if data[col].nunique() > 2 else False for col in data.columns])\n",
    "is_cont_node = is_cont_node.tolist()\n",
    "print(is_cont_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ## Load the actual connectome data\n",
    "\n",
    "# # %%\n",
    "# # load the weighted adjacency matrices for ER and connectome\n",
    "\n",
    "# # Specify the folder where the adjacency matrices were saved\n",
    "# folder = '../data/'\n",
    "\n",
    "# # Specify the folder where the acyclic positive integer weighted connectome data was saved\n",
    "# folder_cyclic = '/home/amine.mcharrak/connectome/data/'\n",
    "# # Specify the folder where the acyclic positive integer weighted connectome data was saved\n",
    "# folder_acyclic = '/home/amine.mcharrak/connectome/data/'\n",
    "\n",
    "# # Example usage to load the saved adjacency matrices\n",
    "# # G_A_init_t_ordered_adj_matrix = load_adjacency_matrix(os.path.join(folder, 'G_A_init_t_ordered_adj_matrix.npy'))\n",
    "# # G_A_init_t_ordered_dag_adj_matrix = load_adjacency_matrix(os.path.join(folder, 'G_A_init_t_ordered_dag_adj_matrix.npy'))\n",
    "# # ER = load_adjacency_matrix(os.path.join(folder, 'ER_adj_matrix.npy'))\n",
    "# # ER_dag = load_adjacency_matrix(os.path.join(folder, 'ER_dag_adj_matrix.npy'))\n",
    "\n",
    "# # Change name of the connectome adjacency matrix to C and C_dag\n",
    "# # C = G_A_init_t_ordered_adj_matrix\n",
    "# # C_dag = G_A_init_t_ordered_dag_adj_matrix\n",
    "\n",
    "# # Now ensure that both DAG adjacency matrices are binary, if they aren't already\n",
    "# # ER_dag_bin = (ER_dag != 0).astype(int)\n",
    "# # C_dag_bin = (C_dag != 0).astype(int)\n",
    "\n",
    "# # ER_true = ER_dag_bin\n",
    "# # C_true = C_dag_bin\n",
    "\n",
    "# # %% [markdown]\n",
    "# # ## Create data to debug and implement the pcax version of NOTEARS\n",
    "\n",
    "# # %%\n",
    "# #Â actual data\n",
    "# B_true = simulate_dag(d=10, s0=20, graph_type='ER') # ER2\n",
    "# # B_true = simulate_dag(d=100, s0=400, graph_type='ER') # ER4\n",
    "# # debugging data\n",
    "# # B_true = simulate_dag(d=10, s0=20, graph_type='ER') # ER2\n",
    "\n",
    "\n",
    "# # B_true = C_dag_bin # if you want to use the connectome-based DAG # best\n",
    "# #B_true = ER_dag_bin # if you want to use the ER-based DAG\n",
    "\n",
    "# #B_true = simulate_dag(d=5, s0=10, graph_type='ER') # ER2\n",
    "# #B_true = simulate_dag(d=10, s0=20, graph_type='ER') # ER2\n",
    "# #B_true = simulate_dag(d=50, s0=100, graph_type='ER') # ER2\n",
    "# #B_true = simulate_dag(d=100, s0=200, graph_type='ER') # ER2\n",
    "# #B_true = simulate_dag(d=279, s0=558, graph_type='ER') # ER2\n",
    "\n",
    "# # create SF2 graph and SF4 graph with d=10 nodes\n",
    "# #B_true = simulate_dag(d=10, s0=20, graph_type='SF') # SF2\n",
    "# #B_true = simulate_dag(d=10, s0=40, graph_type='SF') # SF4\n",
    "# #B_true = simulate_dag(d=100, s0=400, graph_type='SF') # SF4\n",
    "\n",
    "# # create ER2 and ER4 graphs with d=100 nodes\n",
    "# #B_true = simulate_dag(d=100, s0=200, graph_type='ER') # ER2\n",
    "# #B_true = simulate_dag(d=100, s0=400, graph_type='ER') # ER4\n",
    "\n",
    "# # create equivalent ER4 and ER6 graphs\n",
    "# #B_true = simulate_dag(d=279, s0=1116, graph_type='ER') # ER4\n",
    "# #B_true = simulate_dag(d=279, s0=1674, graph_type='ER') # ER6\n",
    "\n",
    "# # create equivalent SF4 and SF6 graphs\n",
    "# #B_true = simulate_dag(d=100, s0=600, graph_type='SF') # SF6\n",
    "# #B_true = simulate_dag(d=279, s0=1116, graph_type='SF') # SF4\n",
    "# #B_true = simulate_dag(d=279, s0=1674, graph_type='SF') # SF6\n",
    "\n",
    "# # create simple data using simulate_dag method from causal_helpers with expected number of edges (s0) and number of nodes (d)\n",
    "# #B_true = simulate_dag(d=100, s0=199, graph_type='ER') # we use pâ‰ˆ0.040226 for the connectome-based ER_dag graph. This means that the expected number of edges is 0.040226 * d * (d-1) / 2\n",
    "# # examples: d=50 -> s0=49 (works), d=100 -> s0=199, d=200 -> s0=800\n",
    "\n",
    "# # create the weighted adjacency matrix based on the binary adjacency matrix\n",
    "# #W_true = simulate_parameter(B_true, connectome=True)\n",
    "# #W_true = simulate_parameter(B_true)\n",
    "\n",
    "# # sample data from the linear SEM\n",
    "# #Â actual data\n",
    "# #X = simulate_linear_sem(W_true, n=10000, sem_type='gauss')\n",
    "# #X = simulate_linear_sem(W_true, n=10000, sem_type='uniform')\n",
    "# # for debugging\n",
    "# #X = simulate_linear_sem(W_true, n=1000, sem_type='gauss')\n",
    "# #X = simulate_linear_sem(W_true, n=2500, sem_type='gauss')\n",
    "# #X = simulate_linear_sem(W_true, n=6250, sem_type='gauss')\n",
    "# #X = simulate_linear_sem(W_true, n=50000, sem_type='gauss')\n",
    "# #X = simulate_linear_sem(W_true, n=100000, sem_type='gauss') # 1000*(279**2)/(20**2) = 194602\n",
    "\n",
    "# # load the cyclic integer weighted connectome data adjacency matrix\n",
    "# #B_true_weighted = load_adjacency_matrix(os.path.join(folder_cyclic, 'A_init_t_ordered_adj_matrix_with_cycles.npy'))\n",
    "# #X, W_true = sample_cyclic_data(B_true_weighted, n_samples=10000, noise_type='non-gaussian')\n",
    "# #B_true = (W_true != 0).astype(int)\n",
    "\n",
    "# # load the acyclic integer weighted connectome data adjacency matrix\n",
    "# # B_true_weighted = load_adjacency_matrix(os.path.join(folder_acyclic, 'A_init_t_ordered_adj_matrix_no_cycles.npy'))\n",
    "# # print(\"B_true_weighted:\\n\", np.array_str(B_true_weighted, precision=4, suppress_small=True))\n",
    "\n",
    "# # B: use this for regular DAGs\n",
    "# W_true = simulate_parameter(B_true)\n",
    "\n",
    "# # B: use this for connectome-based DAGs\n",
    "# #W_true = simulate_parameter(B_true_weighted, connectome=True)\n",
    "# #B_true = (W_true != 0).astype(int)\n",
    "\n",
    "# # some print statements to check the values of W_true\n",
    "# print(\"W_true:\\n\", np.array_str(W_true, precision=4, suppress_small=True))\n",
    "# print(\"Mean of W_true:\", np.mean(W_true))\n",
    "# print(\"Variance of W_true:\", np.var(W_true))\n",
    "# print(\"Max value in W_true:\", np.max(W_true))\n",
    "# print(\"Min value in W_true:\", np.min(W_true))\n",
    "\n",
    "# # sample data from the linear SEM\n",
    "# X = simulate_linear_sem(W_true, n=1000, sem_type='gauss')\n",
    "\n",
    "# # now standardized data, where each variable is normalized to unit variance\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# scaler = StandardScaler()\n",
    "# X_std = scaler.fit_transform(X)\n",
    "\n",
    "# # NOTE: you may not write positional arguments after keyword arguments. \n",
    "# # That is, the values that you are passing positionally have to come first!\n",
    "\n",
    "# # create a dataset using the simulated data\n",
    "# # NOTE: NOTEARS paper uses n=1000 for graph with d=20.\n",
    "# # NOTE: d... number of nodes, p=d^2... number of parameters, n... number of samples. Then: comparing p1=d1^2 vs p2=d2^2 we have that: n1/p1 must be equal to n2/p2\n",
    "# # Thus we have n2 = n1 * p2 / p1. For the case of d2=100 we have that n2 = (n1*p2)/p1 = 1000*(100^2)/(20^2) = 25000 \n",
    "# # we should expect to use that many samples actually to be able to learn the graph in a comparable way.\n",
    "# #dataset = IIDSimulation(W=W_true, n=25000, method='linear', sem_type='gauss')\n",
    "# #true_dag, X = dataset.B, dataset.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print how many non-zero entries are in the true DAG\n",
    "print(f\"Number of non-zero entries in the true DAG: {np.count_nonzero(B_true)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage\n",
    "input_dim = 1\n",
    "n_samples = X.shape[0]\n",
    "n_nodes = X.shape[1]\n",
    "\n",
    "#model = Complete_Graph(input_dim, n_nodes, has_bias=False, is_cont_node=is_cont_node, seed=seed)\n",
    "model = Complete_Graph(input_dim, n_nodes, has_bias=True, is_cont_node=is_cont_node, seed=seed)\n",
    "# Get weighted adjacency matrix\n",
    "W = model.get_W()\n",
    "print(\"This is the weighted adjacency matrix:\\n\", W)\n",
    "print()\n",
    "print(\"The shape of the weighted adjacency matrix is: \", W.shape)\n",
    "print()\n",
    "#print(model)\n",
    "print()\n",
    "\n",
    "# Check if all nodes are frozen initially\n",
    "print(\"Initially, are all nodes frozen?:\", model.are_vodes_frozen())\n",
    "print()\n",
    "\n",
    "# Freezing all nodes\n",
    "print(\"Freezing all nodes...\")\n",
    "model.freeze_nodes(freeze=True)\n",
    "print()\n",
    "\n",
    "# Check if all nodes are frozen after freezing\n",
    "print(\"After freezing, are all nodes frozen?:\", model.are_vodes_frozen())\n",
    "print()\n",
    "\n",
    "# Unfreezing all nodes\n",
    "print(\"Unfreezing all nodes...\")\n",
    "model.freeze_nodes(freeze=False)\n",
    "print()\n",
    "\n",
    "# Check if all nodes are frozen after unfreezing\n",
    "print(\"After unfreezing, are all nodes frozen?:\", model.are_vodes_frozen())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.is_cont_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create plot of lam_l1 = np.log(n_samples) / (2*n_samples) for n_samples {100, 500, 1000, 5000}\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define n_samples values\n",
    "n_values = np.array([100, 500, 1000, 5000])\n",
    "\n",
    "# Compute lambda_l1 values\n",
    "lam_l1_values = np.log(n_values) / (2 * n_values)\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(3, 2))\n",
    "plt.plot(n_values, lam_l1_values, marker='o', linestyle='-', label=r'$\\lambda_{L1} = \\frac{\\log(n)}{2n}$')\n",
    "\n",
    "# Labels and title\n",
    "plt.xscale('log')  # Log scale for better visualization\n",
    "plt.xlabel('Number of Samples (n_samples)')\n",
    "plt.ylabel(r'$\\lambda_{L1}$')\n",
    "plt.title(r'$\\lambda_{L1} = \\frac{\\log(n)}{2n}$ for Different Sample Sizes')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# now plot lam_h = 5/sqrt(d) for d {10, 15, 20}\n",
    "\n",
    "# Define values of d\n",
    "d_values = np.array([10, 15, 20])\n",
    "\n",
    "# Compute lambda_h\n",
    "lam_h_values = 5 / np.sqrt(d_values)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(3, 2))\n",
    "plt.plot(d_values, lam_h_values, marker='o', linestyle='-', label=r'$\\lambda_h = \\frac{5}{\\sqrt{d}}$')\n",
    "plt.xlabel(\"d (number of variables)\")\n",
    "plt.ylabel(r\"$\\lambda_h = \\frac{5}{\\sqrt{d}}$\")\n",
    "plt.title(r\"Plot of $\\lambda_h$ for different $d$\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# now plot lam_l1 = 1/sqrt(d_values)\n",
    "\n",
    "lam_l1_values = 1 / np.sqrt(d_values**2)\n",
    "\n",
    "plt.figure(figsize=(3, 2))\n",
    "plt.plot(d_values, lam_l1_values, marker='o', linestyle='-', label=r'$\\lambda_{L1} = \\frac{1}{\\sqrt{d^2}}$')\n",
    "plt.xlabel(\"d (number of variables)\")\n",
    "plt.ylabel(r\"$\\lambda_{L1} = \\frac{1}{\\sqrt{d^2}}$\")\n",
    "plt.title(r\"Plot of $\\lambda_{L1}$ for different $d$\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: make the below params global or input to the functions in which it is used.\n",
    "w_learning_rate = 1e-3\n",
    "h_learning_rate = 1e-4\n",
    "T = 1\n",
    "\n",
    "nm_epochs = 5000 # not much happens after 2000 epochs\n",
    "every_n_epochs = nm_epochs // 20 # Print every 5% of the epochs\n",
    "batch_size = 256\n",
    "\n",
    "# NOTE: small lam_l1 (1e-5) and larger lam_h (1e1) seem to work well\n",
    "# NOTE: for any graph type, the best way to select lam_h is such that one increases it until the graph becomes acyclic, a value right before that is the best value\n",
    "# NOTE: larger lam_h (1e1) gives better cyclic structure score (CSS) but worse fit metric scores (log-likelihood, KL divergence, etc.)\n",
    "# NOTE: smaller lam_h (5e-4) gives better fit metric scores (log-likelihood, KL divergence, etc.) but worse cyclic structure score (CSS)\n",
    "\n",
    "lam_h = 5e0 # effective value depends on d, does not change during training\n",
    "lam_l1 = 1e0 # effective value depends on Frob Norm of W at any time, thus changes during training\n",
    "\n",
    "# SHD cyclic and cycle F1 using our method: 45, 0.4838709677419355\n",
    "#lam_h = 2e0\n",
    "#lam_l1 = 4e-2\n",
    "\n",
    "# SHD cyclic and cycle F1 using our method: 44, 0.4838709677419355\n",
    "#lam_h = 1.7e0\n",
    "#lam_l1 = 4e-2\n",
    "\n",
    "# SHD cyclic and cycle F1 using our method: 54, 0.5538461538461538\n",
    "#lam_h = 0.8e0\n",
    "#lam_l1 = 4e-2\n",
    "\n",
    "# SHD cyclic and cycle F1 using our method: 45, 0.507936507936508\n",
    "#lam_h = 1.5e0\n",
    "#lam_l1 = 4e-2\n",
    "\n",
    "# Create a file name string for the hyperparameters\n",
    "exp_name = f\"bs_{batch_size}_lrw_{w_learning_rate}_lrh_{h_learning_rate}_lamh_{lam_h}_laml1_{lam_l1}_epochs_{nm_epochs}\"\n",
    "print(\"Name of the experiment: \", exp_name)\n",
    "\n",
    "# Training an1d evaluation functions\n",
    "@pxf.vmap(pxu.M(pxc.VodeParam | pxc.VodeParam.Cache).to((None, 0)), in_axes=(0,), out_axes=0)\n",
    "def forward(x, *, model: Complete_Graph):\n",
    "    return model(x)\n",
    "\n",
    "# @pxf.vmap(pxu.M(pxc.VodeParam | pxc.VodeParam.Cache).to((None, 0)), out_axes=(None, 0), axis_name=\"batch\") # if only one output\n",
    "@pxf.vmap(pxu.M(pxc.VodeParam | pxc.VodeParam.Cache).to((None, 0)), out_axes=(None, None, None, None, 0), axis_name=\"batch\") # if multiple outputs\n",
    "def energy(*, model: Complete_Graph):\n",
    "\n",
    "    print(\"Energy: Starting computation\")\n",
    "    x_ = model(None)\n",
    "    print(\"Energy: Got model output\")\n",
    "    \n",
    "    W = model.get_W()\n",
    "    # Dimensions of W\n",
    "    d = W.shape[0]\n",
    "    print(f\"Energy: Got W (shape: {W.shape}) and d: {d}\")\n",
    "\n",
    "    # PC energy term\n",
    "    pc_energy = jax.lax.pmean(model.energy(), axis_name=\"batch\")\n",
    "    print(f\"Energy: PC energy term: {pc_energy}\")\n",
    "\n",
    "    # L1 regularization using adjacency matrix (scaled by Frobenius norm)\n",
    "    l1_reg = jnp.sum(jnp.abs(W)) / (jnp.linalg.norm(W, ord='fro') + 1e-8)\n",
    "    #l1_reg = jnp.sum(jnp.abs(W)) / d\n",
    "    print(f\"Energy: L1 reg term: {l1_reg}\")\n",
    "\n",
    "    # DAG constraint (stable logarithmic form)\n",
    "    #h_reg = notears_dag_constraint(W)\n",
    "    h_reg = notears_dag_constraint(W)/ (jnp.sqrt(d) + 1e-8)\n",
    "    #h_reg = notears_dag_constraint(W) / d  # with normalization\n",
    "\n",
    "    #h_reg = dagma_dag_constraint(W)\n",
    "    h_reg = dagma_dag_constraint(W) / (jnp.sqrt(d) + 1e-8)\n",
    "    #h_reg = dagma_dag_constraint(W) / d  # with normalization\n",
    "    print(f\"Energy: DAG constraint term: {h_reg}\")\n",
    "        \n",
    "    # Combined loss\n",
    "    obj = pc_energy + lam_h * h_reg + lam_l1 * l1_reg\n",
    "    print(f\"Energy: Final objective: {obj}\")\n",
    "\n",
    "    # Ensure obj is a scalar, not a (1,) array because JAX's grad and value_and_grad functions are designed\n",
    "    # to compute gradients of *scalar-output functions*\n",
    "    obj = obj.squeeze()  # explicitly converte the (1,) array (obj) to a scalar of shape ()  \n",
    "    \n",
    "    return obj, pc_energy, h_reg, l1_reg, x_\n",
    "\n",
    "@pxf.jit(static_argnums=0)\n",
    "def train_on_batch(T: int, x: jax.Array, *, model: Complete_Graph, optim_w: pxu.Optim, optim_h: pxu.Optim):\n",
    "    print(\"1. Starting train_on_batch\")  \n",
    "\n",
    "    model.train()\n",
    "    print(\"2. Model set to train mode\")\n",
    "\n",
    "    model.freeze_nodes(freeze=True)\n",
    "    print(\"3. Nodes frozen\")\n",
    "\n",
    "    # init step\n",
    "    with pxu.step(model, pxc.STATUS.INIT, clear_params=pxc.VodeParam.Cache):\n",
    "        print(\"4. Doing forward for initialization\")\n",
    "        forward(x, model=model)\n",
    "        print(\"5. After forward for initialization\")\n",
    "\n",
    "    \"\"\"\n",
    "    # The following code might not be needed as we are keeping the vodes frozen at all times\n",
    "    # Reinitialize the optimizer state between different batches\n",
    "    optim_h.init(pxu.M(pxc.VodeParam)(model))\n",
    "\n",
    "    for _ in range(T):\n",
    "        with pxu.step(model, clear_params=pxc.VodeParam.Cache):\n",
    "            (e, x_), g = pxf.value_and_grad(\n",
    "                pxu.M(pxu.m(pxc.VodeParam).has_not(frozen=True), [False, True]),\n",
    "                has_aux=True\n",
    "            )(energy)(model=model)\n",
    "        optim_h.step(model, g[\"model\"], True)\n",
    "    \"\"\"\n",
    "\n",
    "    with pxu.step(model, clear_params=pxc.VodeParam.Cache):\n",
    "        print(\"6. Before computing gradients\")\n",
    "        (obj, (pc_energy, h_reg, l1_reg, x_)), g = pxf.value_and_grad(\n",
    "            pxu.M(pxnn.LayerParam).to([False, True]), \n",
    "            has_aux=True\n",
    "        )(energy)(model=model) # pxf.value_and_grad returns a tuple structured as ((value, aux), grad), not as six separate outputs.\n",
    "        \n",
    "        print(\"7. After computing gradients\")\n",
    "        #print(\"Gradient structure:\", g)\n",
    "\n",
    "        print(\"8. Before zeroing out the diagonal gradients\")\n",
    "        # Zero out the diagonal gradients using jax.numpy.fill_diagonal\n",
    "        weight_grads = g[\"model\"].layers[0].nn.weight.get()\n",
    "        weight_grads = jax.numpy.fill_diagonal(weight_grads, 0.0, inplace=False)\n",
    "        # print the grad values using the syntax jax.debug.print(\"ðŸ¤¯ {x} ðŸ¤¯\", x=x)\n",
    "        #jax.debug.print(\"{weight_grads}\", weight_grads=weight_grads)\n",
    "        g[\"model\"].layers[0].nn.weight.set(weight_grads)\n",
    "        print(\"9. After zeroing out the diagonal gradients\")\n",
    "\n",
    "        \n",
    "    print(\"10. Before optimizer step\")\n",
    "    optim_w.step(model, g[\"model\"])\n",
    "    #optim_w.step(model, g[\"model\"], scale_by=1.0/x.shape[0])\n",
    "    print(\"11. After optimizer step\")\n",
    "\n",
    "    with pxu.step(model, clear_params=pxc.VodeParam.Cache):\n",
    "        print(\"12. Before final forward\")\n",
    "        forward(None, model=model)\n",
    "        e_avg_per_sample = model.energy()\n",
    "        print(\"13. After final forward\")\n",
    "\n",
    "    model.freeze_nodes(freeze=False)\n",
    "    print(\"14. Nodes unfrozen\")\n",
    "\n",
    "    return pc_energy, l1_reg, h_reg, obj\n",
    "\n",
    "def train(dl, T, *, model: Complete_Graph, optim_w: pxu.Optim, optim_h: pxu.Optim):\n",
    "    batch_pc_energies = []\n",
    "    batch_l1_regs = []\n",
    "    batch_h_regs = []\n",
    "    batch_objs = []\n",
    "    \n",
    "    for batch in dl:\n",
    "        pc_energy, l1_reg, h_reg, obj = train_on_batch(\n",
    "            T, batch, model=model, optim_w=optim_w, optim_h=optim_h\n",
    "        )\n",
    "        batch_pc_energies.append(pc_energy)\n",
    "        batch_l1_regs.append(l1_reg)\n",
    "        batch_h_regs.append(h_reg)\n",
    "        batch_objs.append(obj)\n",
    "\n",
    "    W = model.get_W()\n",
    "\n",
    "    # Compute epoch averages\n",
    "    epoch_pc_energy = jnp.mean(jnp.array(batch_pc_energies))\n",
    "    epoch_l1_reg = jnp.mean(jnp.array(batch_l1_regs))\n",
    "    epoch_h_reg = jnp.mean(jnp.array(batch_h_regs))\n",
    "    epoch_obj = jnp.mean(jnp.array(batch_objs))\n",
    "    \n",
    "    return W, epoch_pc_energy, epoch_l1_reg, epoch_h_reg, epoch_obj\n",
    "\n",
    "\n",
    "# %%\n",
    "# for reference compute the MAE, SID, and SHD between the true adjacency matrix and an all-zero matrix and then print it\n",
    "# this acts as a baseline for the MAE, SID, and SHD similar to how 1/K accuracy acts as a baseline for classification tasks where K is the number of classes\n",
    "\n",
    "W_zero = np.zeros_like(W_true)\n",
    "print(\"MAE between the true adjacency matrix and an all-zero matrix: \", MAE(W_true, W_zero))\n",
    "print(\"SHD between the true adjacency matrix and an all-zero matrix: \", SHD(B_true, compute_binary_adjacency(W_zero)))\n",
    "#print(\"SID between the true adjacency matrix and an all-zero matrix: \", SID(W_true, W_zero))\n",
    "\n",
    "# %%\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X):\n",
    "        self.X = X\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx]\n",
    "\n",
    "# This is a simple collate function that stacks numpy arrays used to interface\n",
    "# the PyTorch dataloader with JAX. In the future we hope to provide custom dataloaders\n",
    "# that are independent of PyTorch.\n",
    "def numpy_collate(batch):\n",
    "    if isinstance(batch[0], np.ndarray):\n",
    "        return np.stack(batch)\n",
    "    elif isinstance(batch[0], (tuple, list)):\n",
    "        transposed = zip(*batch)\n",
    "        return [numpy_collate(samples) for samples in transposed]\n",
    "    else:\n",
    "        return np.array(batch)\n",
    "\n",
    "# The dataloader assumes cuda is being used, as such it sets 'pin_memory = True' and\n",
    "# 'prefetch_factor = 2'. Note that the batch size should be constant during training, so\n",
    "# we set 'drop_last = True' to avoid having to deal with variable batch sizes.\n",
    "class TorchDataloader(torch.utils.data.DataLoader):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset,\n",
    "        batch_size=1,\n",
    "        shuffle=None,\n",
    "        sampler=None,\n",
    "        batch_sampler=None,\n",
    "        num_workers=1,\n",
    "        pin_memory=True,\n",
    "        timeout=0,\n",
    "        worker_init_fn=None,\n",
    "        persistent_workers=True,\n",
    "        prefetch_factor=2,\n",
    "    ):\n",
    "        super(self.__class__, self).__init__(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            sampler=sampler,\n",
    "            batch_sampler=batch_sampler,\n",
    "            num_workers=num_workers,\n",
    "            collate_fn=numpy_collate,\n",
    "            pin_memory=pin_memory,\n",
    "            drop_last=True if batch_sampler is None else None,\n",
    "            timeout=timeout,\n",
    "            worker_init_fn=worker_init_fn,\n",
    "            persistent_workers=persistent_workers,\n",
    "            prefetch_factor=prefetch_factor,\n",
    "        )\n",
    "\n",
    "\n",
    "# Create the custom dataset\n",
    "dataset = CustomDataset(X)\n",
    "\n",
    "# Create the custom dataset with standardized data\n",
    "#dataset_std = CustomDataset(X_std)\n",
    "\n",
    "# Create the dataloader\n",
    "dl = TorchDataloader(dataset, batch_size=batch_size, shuffle=True)\n",
    "######## OR ########\n",
    "#dl = TorchDataloader(dataset_std, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# %%\n",
    "# Initialize the model and optimizers\n",
    "with pxu.step(model, pxc.STATUS.INIT, clear_params=pxc.VodeParam.Cache):\n",
    "    forward(jnp.zeros((batch_size, model.n_nodes.get())), model=model)\n",
    "    optim_h = pxu.Optim(lambda: optax.sgd(h_learning_rate))\n",
    "\n",
    "    \"\"\"\n",
    "    optim_w = pxu.Optim(\n",
    "    optax.chain(\n",
    "        optax.clip_by_global_norm(clip_value),  # Clip gradients by global norm\n",
    "        optax.sgd(w_learning_rate)  # Apply SGD optimizer\n",
    "    ),\n",
    "    pxu.M(pxnn.LayerParam)(model)  # Masking the parameters of the model\n",
    ")\n",
    "    \"\"\"\n",
    "    #optim_w = pxu.Optim(lambda: optax.adam(w_learning_rate), pxu.M(pxnn.LayerParam)(model)) # no big difference to adamw observed\n",
    "    optim_w = pxu.Optim(lambda: optax.adamw(w_learning_rate, nesterov=True), pxu.M(pxnn.LayerParam)(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store differences and energies\n",
    "MAEs = []\n",
    "SHDs = []\n",
    "SHDs_cyclic = []\n",
    "F1s_cycles = []\n",
    "F1s = []\n",
    "pc_energies = []\n",
    "l1_regs = []\n",
    "h_regs = []\n",
    "objs = []\n",
    "\n",
    "# Calculate the initial MAE, SID, and SHD\n",
    "\n",
    "W_init = model.get_W()\n",
    "B_init = compute_binary_adjacency(W_init)\n",
    "\n",
    "MAE_init = MAE(W_true, W_init)\n",
    "print(f\"Start difference (cont.) between W_true and W_init: {MAE_init:.4f}\")\n",
    "\n",
    "SHD_init = SHD(B_true, B_init, double_for_anticausal=False)\n",
    "print(f\"Start SHD between B_true and B_init: {SHD_init:.4f}\")\n",
    "\n",
    "SHD_cyclic_init = compute_cycle_SHD(B_true_EC, B_init)\n",
    "print(f\"Start SHD (cyclic) between B_true and B_init: {SHD_cyclic_init:.4f}\")\n",
    "\n",
    "F1_init = compute_F1_directed(B_true, B_init)\n",
    "print(f\"Start F1 between B_true and B_init: {F1_init:.4f}\")\n",
    "\n",
    "cycle_f1_init = compute_cycle_F1(B_true, B_init)\n",
    "print(f\"Start cycle accuracy between B_true and B_init: {cycle_f1_init:.4f}\")\n",
    "\n",
    "# print the values of the diagonal of the initial W\n",
    "print(\"The diagonal of the initial W: \", jnp.diag(W_init))\n",
    "\n",
    "# Start timing\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "# Training loop\n",
    "with tqdm(range(nm_epochs), position=0, leave=True) as pbar:\n",
    "    for epoch in pbar:\n",
    "        # Train for one epoch using the dataloader\n",
    "        W, epoch_pc_energy, epoch_l1_reg, epoch_h_reg, epoch_obj = train(dl, T=T, model=model, optim_w=optim_w, optim_h=optim_h)\n",
    "        \n",
    "        # Extract the weighted adjacency matrix W and compute the binary adjacency matrix B\n",
    "        W = np.array(W)\n",
    "        B = compute_binary_adjacency(W)\n",
    "\n",
    "        # Compute metrics every 100 epochs\n",
    "        if (epoch + 1) % every_n_epochs == 0 or epoch == 0:\n",
    "            MAEs.append(float(MAE(W_true, W)))\n",
    "            SHDs.append(float(SHD(B_true, compute_binary_adjacency(W), double_for_anticausal=False)))\n",
    "            SHDs_cyclic.append(float(compute_cycle_SHD(B_true_EC, compute_binary_adjacency(W))))\n",
    "            F1s.append(float(compute_F1_directed(B_true, B)))\n",
    "            F1s_cycles.append(float(compute_cycle_F1(B_true, B)))\n",
    "            pc_energies.append(float(epoch_pc_energy))\n",
    "            l1_regs.append(float(epoch_l1_reg))\n",
    "            epoch_h_reg_raw = compute_h_reg(W)\n",
    "            h_regs.append(float(epoch_h_reg_raw))\n",
    "            objs.append(float(epoch_obj))\n",
    "\n",
    "            # Update progress bar with the current status\n",
    "            pbar.set_description(f\"MAE: {MAEs[-1]:.4f}, Cycle F1: {F1s_cycles[-1]:.4f}, F1: {F1s[-1]:.4f}, SHD: {SHDs[-1]:.4f}, Cycle SHD: {SHDs_cyclic[-1]:.4f} || PC Energy: {pc_energies[-1]:.4f}, L1 Reg: {l1_regs[-1]:.4f}, H Reg: {h_regs[-1]:.4f}, Obj: {objs[-1]:.4f}\")\n",
    "\n",
    "# End timing\n",
    "end_time = timeit.default_timer()\n",
    "\n",
    "# Print the average time per epoch\n",
    "average_time_per_epoch = (end_time - start_time) / nm_epochs\n",
    "print(f\"An epoch (with compiling and testing) took on average: {average_time_per_epoch:.4f} seconds\")\n",
    "# print the values of the diagonal of the final W\n",
    "print(\"The diagonal of the final W: \", jnp.diag(model.get_W()))\n",
    "\n",
    "\n",
    "# print in big that training is done\n",
    "print(\"\\n\\n ###########################  Training is done  ########################### \\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define experiment name\n",
    "exp_name = f\"bs_{batch_size}_lrw_{w_learning_rate}_lrh_{h_learning_rate}_lamh_{lam_h}_laml1_{lam_l1}_epochs_{nm_epochs}\"\n",
    "\n",
    "# Create subdirectory in linear folder with the name stored in exp_name\n",
    "save_path = os.path.join('plots/linear_cyclic', exp_name)\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "# Reset to default style and set seaborn style\n",
    "plt.style.use('default')\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Update matplotlib parameters\n",
    "plt.rcParams.update({\n",
    "    'figure.dpi': 300,\n",
    "    'savefig.dpi': 300,\n",
    "    'font.size': 12,\n",
    "    'axes.titlesize': 14,\n",
    "    'axes.labelsize': 12,\n",
    "    'xtick.labelsize': 10,\n",
    "    'ytick.labelsize': 10,\n",
    "    'legend.fontsize': 10,\n",
    "    'figure.titlesize': 16,\n",
    "    'axes.grid': True,\n",
    "    'grid.alpha': 0.4,\n",
    "    'axes.spines.top': False,\n",
    "    'axes.spines.right': False\n",
    "})\n",
    "\n",
    "# Create a figure and subplots using GridSpec with 3x3 layout\n",
    "fig = plt.figure(figsize=(22, 12))\n",
    "gs = fig.add_gridspec(3, 3, height_ratios=[1, 1, 1], width_ratios=[1, 1, 1])\n",
    "\n",
    "# Adjust layout to make room for titles and subtitles\n",
    "plt.subplots_adjust(top=0.92, hspace=0.4, wspace=0.3)\n",
    "\n",
    "# Create axes\n",
    "axes = [fig.add_subplot(gs[i, j]) for i in range(3) for j in range(3)]\n",
    "\n",
    "# Updated plot configurations\n",
    "plot_configs = [\n",
    "    {'metric': MAEs, 'title': 'MAE', 'ylabel': 'MAE', 'color': '#2ecc71', 'ax': axes[0]},\n",
    "    {'metric': SHDs, 'title': 'SHD', 'ylabel': 'SHD', 'color': '#e74c3c', 'ax': axes[1]},\n",
    "    {'metric': SHDs_cyclic, 'title': 'SHD Cyclic', 'ylabel': 'SHD Cyclic', 'color': '#8e44ad', 'ax': axes[2]},\n",
    "    {'metric': F1s, 'title': 'F1 Score', 'ylabel': 'F1 Structure', 'color': '#3498db', 'ax': axes[3]},\n",
    "    {'metric': l1_regs, 'title': 'L1 Regularization', 'ylabel': 'L1', 'color': '#f1c40f', 'ax': axes[4]},\n",
    "    {'metric': h_regs, 'title': 'DAG Constraint', 'ylabel': 'h(W)', 'color': '#e67e22', 'ax': axes[5]},\n",
    "    {'metric': pc_energies, 'title': 'PC Energy', 'ylabel': 'PC Energy', 'color': '#9b59b6', 'ax': axes[6]},\n",
    "    {'metric': objs, 'title': 'Total Objective', 'ylabel': 'Total Loss', 'color': '#1abc9c', 'ax': axes[7]},\n",
    "    {'metric': F1s_cycles, 'title': 'F1 Cyclic', 'ylabel': 'F1 Cyclic', 'color': '#16a085', 'ax': axes[8]}\n",
    "]\n",
    "\n",
    "# Create all subplots\n",
    "for config in plot_configs:\n",
    "    ax = config['ax']\n",
    "\n",
    "    epochs = range(0, len(config['metric']) * every_n_epochs, every_n_epochs)\n",
    "    \n",
    "    # Determine if we should use log scale and/or scaling factor\n",
    "    use_log_scale = config['title'] in ['PC Energy', 'Total Objective']\n",
    "    #scale_factor = 1e4 if config['title'] == 'DAG Constraint' else 1\n",
    "    scale_factor = 1 if config['title'] == 'DAG Constraint' else 1\n",
    "    \n",
    "    # Apply scaling and/or log transform to the metric\n",
    "    metric_values = np.array(config['metric'])\n",
    "    if use_log_scale:\n",
    "        # Add small constant to avoid log(0)\n",
    "        metric_values = np.log10(np.abs(metric_values) + 1e-10)\n",
    "    metric_values = metric_values * scale_factor\n",
    "\n",
    "    # Plot raw data\n",
    "    ax.plot(epochs, metric_values, alpha=0.3, color=config['color'], label='Raw')\n",
    "\n",
    "    # Calculate rolling average\n",
    "    window_size = max(1, len(metric_values) // 50)  # Dynamic window size based on number of epochs\n",
    "    if len(metric_values) > window_size:\n",
    "        rolling_mean = np.convolve(metric_values, np.ones(window_size) / window_size, mode='valid')\n",
    "        ax.plot(epochs[window_size - 1:], rolling_mean, color=config['color'], linewidth=2, label='Rolling Avg')\n",
    "\n",
    "    # Customize subplot\n",
    "    ax.set_xlabel('Epoch', labelpad=10)\n",
    "    # Adjust ylabel based on transformations\n",
    "    ylabel = config['ylabel']\n",
    "    if use_log_scale:\n",
    "        ylabel = f'log10({ylabel})'\n",
    "    if scale_factor != 1:\n",
    "        ylabel = f'{ylabel} (Ã—{int(scale_factor)})'\n",
    "    ax.set_ylabel(ylabel, labelpad=10)\n",
    "    \n",
    "    ax.grid(True, linestyle='--', alpha=0.7)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "\n",
    "    # Add legend for Total Objective plot\n",
    "    if config['title'] == 'Total Objective':\n",
    "        ax.legend(loc='upper right')\n",
    "\n",
    "    # Add note about scaling if applicable\n",
    "    if use_log_scale or scale_factor != 1:\n",
    "        transform_text = []\n",
    "        if use_log_scale:\n",
    "            transform_text.append('log scale')\n",
    "        if scale_factor != 1:\n",
    "            transform_text.append(f'Ã—{int(scale_factor)}')\n",
    "        ax.text(0.02, 0.98, f\"({', '.join(transform_text)})\", \n",
    "                transform=ax.transAxes, \n",
    "                fontsize=8, \n",
    "                verticalalignment='top',\n",
    "                bbox=dict(facecolor='white', alpha=0.8, edgecolor='none'))        \n",
    "\n",
    "# Add overall title and subtitle with adjusted positions\n",
    "fig.suptitle('Training Metrics Over Time', \n",
    "             fontsize=16, \n",
    "             weight='bold', \n",
    "             y=0.95)  # Lower the main title slightly\n",
    "\n",
    "subtitle = f'Î»_h = {lam_h}, Î»_l1 = {lam_l1}, Weights Learning Rate = {w_learning_rate}'\n",
    "fig.text(0.5, 0.90,  # Adjusted y position for the subtitle\n",
    "         subtitle, \n",
    "         horizontalalignment='center',\n",
    "         fontsize=12,\n",
    "         style='italic')\n",
    "\n",
    "# Adjust layout to make more room for title and subtitle\n",
    "plt.subplots_adjust(top=0.88, hspace=0.4, wspace=0.3)\n",
    "\n",
    "\n",
    "# Save and show the figure as a .pdf file\n",
    "plt.savefig(os.path.join(save_path, 'training_metrics.pdf'), bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "########################## Plotting the DAG comparison  ##########################\n",
    "\n",
    "# Create a separate figure for the adjacency matrices comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Use a better colormap - options:\n",
    "# 'YlOrBr' - Yellow-Orange-Brown (good for sparse matrices)\n",
    "# 'viridis' - Perceptually uniform, colorblind-friendly\n",
    "# 'Greys' - Black and white, professional\n",
    "# 'YlGnBu' - Yellow-Green-Blue, professional\n",
    "cmap = 'YlOrBr'  # Choose one of the above\n",
    "\n",
    "# Plot estimated adjacency matrix (now on the left)\n",
    "im1 = ax1.imshow(compute_binary_adjacency(W), cmap=cmap, interpolation='nearest')\n",
    "ax1.set_title('Estimated DAG', pad=10)\n",
    "plt.colorbar(im1, ax=ax1, fraction=0.046, pad=0.04)\n",
    "ax1.set_xlabel('Node', labelpad=10)\n",
    "ax1.set_ylabel('Node', labelpad=10)\n",
    "\n",
    "# Plot true adjacency matrix (now on the right)\n",
    "im2 = ax2.imshow(B_true, cmap=cmap, interpolation='nearest')\n",
    "ax2.set_title('True DAG', pad=10)\n",
    "plt.colorbar(im2, ax=ax2, fraction=0.046, pad=0.04)\n",
    "ax2.set_xlabel('Node', labelpad=10)\n",
    "ax2.set_ylabel('Node', labelpad=10)\n",
    "\n",
    "# Add overall title\n",
    "fig.suptitle('Estimated vs True DAG Structure', \n",
    "             fontsize=16, \n",
    "             weight='bold', \n",
    "             y=1.05)\n",
    "\n",
    "# Add grid lines to better separate the nodes\n",
    "for ax in [ax1, ax2]:\n",
    "    ax.set_xticks(np.arange(-.5, B_true.shape[0], 1), minor=True)\n",
    "    ax.set_yticks(np.arange(-.5, B_true.shape[0], 1), minor=True)\n",
    "    ax.grid(which=\"minor\", color=\"w\", linestyle='-', linewidth=0.3)\n",
    "    ax.tick_params(which=\"minor\", bottom=False, left=False)\n",
    "\n",
    "# Improve layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the comparison plot as a .pdf file at the specified location\n",
    "plt.savefig(os.path.join(save_path, 'dag_comparison.pdf'), \n",
    "            bbox_inches='tight', \n",
    "            dpi=300,\n",
    "            facecolor='white',\n",
    "            edgecolor='none')\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# Now use a threshold of 0.3 to binarize the weighted adjacency matrix W\n",
    "W_est = np.array(model.get_W())\n",
    "B_est = compute_binary_adjacency(W_est, threshold=0.3)\n",
    "\n",
    "# Check if the estimated binary adjacency matrix B_est is a DAG\n",
    "is_dag_B_est = is_dag_nx(B_est)\n",
    "print(f\"Is the estimated binary adjacency matrix a DAG? {is_dag_B_est}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Compute the h_reg term for the true weighted adjacency matrix W_true\n",
    "h_reg_true = compute_h_reg(W_true)\n",
    "print(f\"The h_reg term for the TRUE weighted adjacency matrix W_true is: {h_reg_true:.4f}\")\n",
    "# Compute the h_reg term for the estimated weighted adjacency matrix W_est\n",
    "h_reg_est = compute_h_reg(W_est)\n",
    "print(f\"The h_reg term for the EST weighted adjacency matrix W_est is: {h_reg_est:.4f}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# print the number of edges in the true graph and the estimated graph\n",
    "print(f\"The number of edges in the TRUE graph: {np.sum(B_true)}\")\n",
    "print(f\"The number of edges in the EST graph: {np.sum(B_est)}\")\n",
    "\n",
    "# print first 5 rows and columsn of W_est and round values to 4 decimal places and show as non-scientific notation\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"The first 5 rows and columns of the estimated weighted adjacency matrix W_est\\n{}\".format(W_est[:5, :5]))\n",
    "\n",
    "# now show the adjacency matrix of the true graph and the estimated graph side by side\n",
    "plot_adjacency_matrices(true_matrix=B_true, est_matrix=B_est, save_path=os.path.join(save_path, 'adjacency_matrices.png'))\n",
    "\n",
    "\n",
    "# %%\n",
    "# plot est_dag and true_dag\n",
    "GraphDAG(B_est, B_true, save_name=os.path.join(save_path, 'est_dag_true_dag.png'))\n",
    "# calculate accuracy\n",
    "met_pcx = MetricsDAG(B_est, B_true) # expects first arg to be the predicted labels and the second arg to be the true labels\n",
    "print(met_pcx.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the style and color palette\n",
    "sns.set(style=\"whitegrid\")\n",
    "palette = sns.color_palette(\"tab10\")\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(18, 5))  # Adjusting layout to 1 row and 3 columns\n",
    "fig.suptitle('Performance Metrics Over Epochs', fontsize=16, weight='bold')\n",
    "\n",
    "# Plot the Cyclic F1 score\n",
    "sns.lineplot(x=range(len(F1s_cycles)), y=F1s_cycles, ax=axs[0], color=palette[0])\n",
    "axs[0].set_xlabel(\"Epoch\", fontsize=12)\n",
    "axs[0].set_ylabel(\"F1 Cyclic\", fontsize=12)\n",
    "axs[0].grid(True)\n",
    "\n",
    "# Plot the Cyclic SHD\n",
    "sns.lineplot(x=range(len(SHDs)), y=SHDs_cyclic, ax=axs[1], color=palette[2])\n",
    "axs[1].set_xlabel(\"Epoch\", fontsize=12)\n",
    "axs[1].set_ylabel(\"SHD Cyclic\", fontsize=12)\n",
    "axs[1].grid(True)\n",
    "\n",
    "# Plot the vanilla SHD\n",
    "sns.lineplot(x=range(len(SHDs)), y=SHDs, ax=axs[2], color=palette[1])\n",
    "axs[2].set_xlabel(\"Epoch\", fontsize=12)\n",
    "axs[2].set_ylabel(\"SHD Structure\", fontsize=12)\n",
    "axs[2].grid(True)\n",
    "\n",
    "# Improve layout and show the plot\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjust layout to fit the suptitle\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print first 5 rows and columsn of W_est and round values to 4 decimal places and show as non-scientific notation\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "print(\"The first 4 rows and columns of the estimated weighted adjacency matrix W_est\\n{}\".format(W_est[:11, :11]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now show the adjacency matrix of the true graph and the estimated graph side by side\n",
    "plot_adjacency_matrices(B_true, B_est)\n",
    "\n",
    "# print the number of edges in the true graph and the estimated graph\n",
    "print(f\"The number of edges in the estimated graph: {np.sum(B_est)}\")\n",
    "print(f\"The number of edges in the true graph: {np.sum(B_true)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot est_dag and true_dag\n",
    "GraphDAG(B_est, B_true)\n",
    "# calculate accuracy\n",
    "met_pcax = MetricsDAG(B_est, B_true)\n",
    "print(met_pcax.metrics)\n",
    "\n",
    "# print experiment name\n",
    "print(exp_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nodags\n",
    "\n",
    "# the below setup is taken from the nodags-flows/validation /run_synthetic_benchmark.py file which runs on linear data\n",
    "\n",
    "# Initialize the NODAGS model with linear functions\n",
    "resblock = resflow_train_test_wrapper(\n",
    "    n_nodes=X.shape[1],\n",
    "    batch_size=128,\n",
    "    l1_reg=True, # default is False\n",
    "    lambda_c=5e-2, # default is 1e-2\n",
    "    fun_type='lin-mlp',  # Linear version\n",
    "    act_fun='none', # No activation function\n",
    "    lr=1e-3, # default learning rate is 1e-3\n",
    "    lip_const=0.99,\n",
    "    epochs=50, # default number of epochs is 10\n",
    "    optim='adam', # default optimizer is 'sgd'\n",
    "    n_hidden=0, # default number of hidden layers is 1\n",
    "    thresh_val=0.3, # default threshold value is 0.01\n",
    "    centered=False, # default is True\n",
    "    v=True, # verbose\n",
    "    inline=True, # more verbose\n",
    "    upd_lip=True, # default is True\n",
    "    full_input=False, # default is False\n",
    ")\n",
    "\n",
    "# Train the model with only observational data\n",
    "resblock.train([X], [[]], batch_size=128)\n",
    "\n",
    "# get the weighted adjacency matrix\n",
    "W_est_nodags = np.array(resblock.get_adjacency())\n",
    "# get the binary adjacency matrix\n",
    "B_est_nodags = (W_est_nodags > 0.3).astype(int)\n",
    "\n",
    "# plot est_dag and true_dag\n",
    "GraphDAG(B_est_nodags, B_true)\n",
    "# calculate accuracy\n",
    "met_nodags = MetricsDAG(B_est_nodags, B_true) # expects first arg to be the predicted labels and the second arg to be the true labels\n",
    "print(met_nodags.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lingd\n",
    "\n",
    "ling = LiNGD(k=5, random_state=seed)\n",
    "ling.fit(X)\n",
    "W_ling = np.array(ling._adjacency_matrices[0].T) # 0 to get first candidate adjacency matrix, Transpose to make each row correspond to parents\n",
    "B_est_ling = compute_binary_adjacency(W_ling)\n",
    "\n",
    "# plot est_dag and true_dag\n",
    "GraphDAG(B_est_ling, B_true)\n",
    "# calculate accuracy\n",
    "met_ling = MetricsDAG(B_est_ling, B_true)\n",
    "print(met_ling.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dglearn\n",
    "\n",
    "#tabu_length = 2\n",
    "tabu_length = 4\n",
    "#tabu_length = 10\n",
    "\n",
    "patience = 4\n",
    "\n",
    "#max_iter = np.inf\n",
    "#max_iter = 20\n",
    "#max_iter = 40\n",
    "max_iter = int(np.sum(B_true))\n",
    "\n",
    "manager = CyclicManager(X, bic_coef=0.5, num_workers = 200)\n",
    "B_est_dglearn, best_score, log = tabu_search(manager, tabu_length, patience, max_iter=max_iter, first_ascent=False, verbose=1) # returns a binary matrix as learned support\n",
    "\n",
    "# perform virtual edge correction\n",
    "print(\"virtual edge correction...\")\n",
    "B_est_dglearn = virtual_refine(manager, B_est_dglearn, patience=0, max_iter=max_iter, max_path_len=6, verbose=1)\n",
    "\n",
    "# remove any reducible edges\n",
    "B_est_dglearn = reduce_support(B_est_dglearn, fill_diagonal=False) # same as done in FRP paper\n",
    "B_est_dglearn = np.array(B_est_dglearn)\n",
    "\n",
    "# plot est_dag and true_dag\n",
    "GraphDAG(B_est_dglearn, B_true)\n",
    "# calculate accuracy\n",
    "met_dglearn = MetricsDAG(B_est_dglearn, B_true)\n",
    "print(met_dglearn.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frp\n",
    "\n",
    "edge_penalty = 0.5 * np.log(X.shape[0]) / X.shape[0] # BIC choice\n",
    "frp_result = run_filter_rank_prune(\n",
    "        X, \n",
    "        loss_type=\"kld\",\n",
    "        reg_type=\"scad\",\n",
    "        reg_params={\"lam\": edge_penalty, \"gamma\": 3.7},\n",
    "        edge_penalty=edge_penalty,\n",
    "        n_inits=5,\n",
    "        n_threads=200,\n",
    "        parcorr_thrs=0.1 * (X.shape[0] / 1000)**(-1/4),\n",
    "        use_loss_cache=True,\n",
    "        seed=seed,\n",
    "        verbose=True,\n",
    ")\n",
    "\n",
    "B_est_frp = np.array(frp_result[\"learned_support\"].astype(int))\n",
    "\n",
    "# plot est_dag and true_dag\n",
    "GraphDAG(B_est_frp, B_true)\n",
    "# calculate accuracy\n",
    "met_frp = MetricsDAG(B_est_frp, B_true)\n",
    "print(met_frp.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notears\n",
    "\n",
    "nt = Notears(lambda1=5e-2, h_tol=1e-5, max_iter=100, no_dag_constraint=True, loss_type='l2', seed=seed) # default loss_type is 'l2'\n",
    "#nt = Notears(lambda1=lambda1=5e-2, h_tol=1e-5, max_iter=100, no_dag_constraint=True, loss_type='laplace', seed=seed)\n",
    "#nt = Notears(lambda1=lambda1=5e-2, h_tol=1e-5, max_iter=100, no_dag_constraint=True, loss_type='logistic', seed=seed)\n",
    "nt.learn(X)\n",
    "\n",
    "# plot est_dag and true_dag\n",
    "W_est_nt = np.array(nt.weight_causal_matrix)\n",
    "B_est_nt = np.array(nt.causal_matrix)\n",
    "GraphDAG(B_est_nt, B_true)\n",
    "\n",
    "# calculate accuracy\n",
    "met_nt = MetricsDAG(B_est_nt, B_true)\n",
    "print(met_nt.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# golem\n",
    "\n",
    "gol = GOLEM(num_iter=50000, lambda_1=5e-2, lambda_2=0.0, equal_variances=True, device_type='gpu', no_dag_constraint=True, seed=seed) # we deactivate DAG constraint by setting lambda_2=0.0 as done in FRP paper\n",
    "#gol = GOLEM(num_iter=50000, lambda_1=5e-2, lambda_2=0.0, device_type='gpu', no_dag_constraint=True, seed=seed) # we deactivate DAG constraint by setting lambda_2=0.0 as done in FRP paper\n",
    "gol.learn(X)\n",
    "\n",
    "# plot est_dag and true_dag\n",
    "W_est_gol = np.array(gol.weight_causal_matrix)\n",
    "B_est_gol = np.array(gol.causal_matrix)\n",
    "GraphDAG(B_est_gol, B_true)\n",
    "\n",
    "# calculate accuracy\n",
    "met_gol = MetricsDAG(B_est_gol, B_true)\n",
    "print(met_gol.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyse our model\n",
    "counts, num_edges = analyze_weight_thresholds(W_est, B_est, threshold_values=[0.1, 0.2, 0.3])\n",
    "plot_weights_distribution(W_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: update metrics for newly added models, then create a utility function to print the metrics of all models\n",
    "\n",
    "# Show metrics of all models (ours, LiNG, dglearn, FRP, Notears, GOLEM, NODAGS)\n",
    "\n",
    "# use the function compute_model_fit to get the BIC and AIC scores for our method\n",
    "# NOTE: in theory compute_model_fit expects W and X as arguments, for the models that do not return W, we can use the B_est and X as arguments\n",
    "bic_our, aic_our = compute_model_fit(W_est, X)\n",
    "bic_ling, aic_ling = compute_model_fit(W_ling, X)\n",
    "bic_dglearn, aic_dglearn = compute_model_fit(B_est_dglearn, X) # does not return W, only learns B\n",
    "bic_frp, aic_frp = compute_model_fit(B_est_frp, X) # does not return W, only learns B\n",
    "bic_nt, aic_nt = compute_model_fit(W_est_nt, X)\n",
    "bic_gol, aic_gol = compute_model_fit(W_est_gol, X)\n",
    "bic_nodags, aic_nodags = compute_model_fit(W_est_nodags, X)\n",
    "\n",
    "print(f\"BIC and AIC scores for our method: {bic_our:.3f}, {aic_our:.3f}\")\n",
    "print(f\"BIC and AIC scores for LinG method: {bic_ling:.3f}, {aic_ling:.3f}\")\n",
    "print(f\"BIC and AIC scores for dglearn method: {bic_dglearn:.3f}, {aic_dglearn:.3f}\")\n",
    "print(f\"BIC and AIC scores for FRP method: {bic_frp:.3f}, {aic_frp:.3f}\")\n",
    "print(f\"BIC and AIC scores for Notears method: {bic_nt:.3f}, {aic_nt:.3f}\")\n",
    "print(f\"BIC and AIC scores for GOLEM method: {bic_gol:.3f}, {aic_gol:.3f}\")\n",
    "print(f\"BIC and AIC scores for NODAGS method: {bic_nodags:.3f}, {aic_nodags:.3f}\")\n",
    "\n",
    "# Compute SHD and cycle F1 for all methods\n",
    "cycle_shd_our = compute_cycle_SHD(B_true_EC, B_est) # Our method\n",
    "cycle_shd_ling = compute_cycle_SHD(B_true_EC, B_est_ling)  # LinG method\n",
    "cycle_shd_dglearn = compute_cycle_SHD(B_true_EC, B_est_dglearn)  # dglearn method\n",
    "cycle_shd_frp = compute_cycle_SHD(B_true_EC, B_est_frp)  # FRP method\n",
    "cycle_shd_nt = compute_cycle_SHD(B_true_EC, B_est_nt)  # Notears method\n",
    "cycle_shd_gol = compute_cycle_SHD(B_true_EC, B_est_gol)  # GOLEM method\n",
    "cycle_shd_nodags = compute_cycle_SHD(B_true_EC, B_est_nodags)  # NODAGS method\n",
    "\n",
    "# B or B.T can be used for cycle F1, it doesn't matter\n",
    "cycle_f1_our = compute_cycle_F1(B_true, B_est)\n",
    "cycle_f1_ling = compute_cycle_F1(B_true, B_est_ling)\n",
    "cycle_f1_dglearn = compute_cycle_F1(B_true, B_est_dglearn)\n",
    "cycle_f1_frp = compute_cycle_F1(B_true, B_est_frp)\n",
    "cycle_f1_nt = compute_cycle_F1(B_true, B_est_nt)\n",
    "cycle_f1_gol = compute_cycle_F1(B_true, B_est_gol)\n",
    "cycle_f1_nodags = compute_cycle_F1(B_true, B_est_nodags)\n",
    "\n",
    "# Print SHD and cycle F1 scores\n",
    "print(f\"SHD cyclic and cycle F1 using our method: {cycle_shd_our}, {cycle_f1_our}\")\n",
    "print(f\"SHD cyclic and cycle F1 using LinG method: {cycle_shd_ling}, {cycle_f1_ling}\")\n",
    "print(f\"SHD cyclic and cycle F1 using dglearn method: {cycle_shd_dglearn}, {cycle_f1_dglearn}\")\n",
    "print(f\"SHD cyclic and cycle F1 using FRP method: {cycle_shd_frp}, {cycle_f1_frp}\")\n",
    "print(f\"SHD cyclic and cycle F1 using Notears method: {cycle_shd_nt}, {cycle_f1_nt}\")\n",
    "print(f\"SHD cyclic and cycle F1 using GOLEM method: {cycle_shd_gol}, {cycle_f1_gol}\")\n",
    "print(f\"SHD cyclic and cycle F1 using NODAGS method: {cycle_shd_nodags}, {cycle_f1_nodags}\")\n",
    "\n",
    "# compute_CSS using compute_CSS(shd_cyclic, cycle_f1, epsilon=1e-8)\n",
    "css_our = compute_CSS(cycle_shd_our, cycle_f1_our, epsilon=1e-8)\n",
    "css_ling = compute_CSS(cycle_shd_ling, cycle_f1_ling, epsilon=1e-8)\n",
    "css_dglearn = compute_CSS(cycle_shd_dglearn, cycle_f1_dglearn, epsilon=1e-8)\n",
    "css_frp = compute_CSS(cycle_shd_frp, cycle_f1_frp, epsilon=1e-8)\n",
    "css_nt = compute_CSS(cycle_shd_nt, cycle_f1_nt, epsilon=1e-8)\n",
    "css_gol = compute_CSS(cycle_shd_gol, cycle_f1_gol, epsilon=1e-8)\n",
    "css_nodags = compute_CSS(cycle_shd_nodags, cycle_f1_nodags, epsilon=1e-8)\n",
    "\n",
    "# compute cycle KLD using compute_cycle_KLD\n",
    "cycle_kld_our = compute_cycle_KLD(prec_matrix, B_est)\n",
    "cycle_kld_ling = compute_cycle_KLD(prec_matrix, B_est_ling)\n",
    "cycle_kld_dglearn = compute_cycle_KLD(prec_matrix, B_est_dglearn)\n",
    "cycle_kld_frp = compute_cycle_KLD(prec_matrix, B_est_frp)\n",
    "cycle_kld_nt = compute_cycle_KLD(prec_matrix, B_est_nt)\n",
    "cycle_kld_gol = compute_cycle_KLD(prec_matrix, B_est_gol)\n",
    "cycle_kld_nodags = compute_cycle_KLD(prec_matrix, B_est_nodags)\n",
    "\n",
    "# print the cycle KLD scores and CSS scores\n",
    "print(f\"Cycle KLD and CSS using our method: {cycle_kld_our}, {css_our}\")\n",
    "print(f\"Cycle KLD and CSS using LinG method: {cycle_kld_ling}, {css_ling}\")\n",
    "print(f\"Cycle KLD and CSS using dglearn method: {cycle_kld_dglearn}, {css_dglearn}\")\n",
    "print(f\"Cycle KLD and CSS using FRP method: {cycle_kld_frp}, {css_frp}\")\n",
    "print(f\"Cycle KLD and CSS using Notears method: {cycle_kld_nt}, {css_nt}\")\n",
    "print(f\"Cycle KLD and CSS using GOLEM method: {cycle_kld_gol}, {css_gol}\")\n",
    "print(f\"Cycle KLD and CSS using NODAGS method: {cycle_kld_nodags}, {css_nodags}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def estimate_weighted_adjacency(X, B_learned):\n",
    "    \"\"\"Estimate W while enforcing the structure from B_learned.\"\"\"\n",
    "    d = X.shape[1]\n",
    "    W_est = np.zeros((d, d))\n",
    "\n",
    "    for j in range(d):\n",
    "        parents = np.where(B_learned[:, j] == 1)[0]\n",
    "        if len(parents) > 0:\n",
    "            W_est[parents, j], _, _, _ = np.linalg.lstsq(X[:, parents], X[:, j], rcond=None)\n",
    "\n",
    "    np.fill_diagonal(W_est, 0)  # Remove self-loops\n",
    "    W_est /= np.linalg.norm(W_est, axis=0, ord=2, keepdims=True) + 1e-6  # Normalize columns\n",
    "    return W_est\n",
    "\n",
    "\n",
    "def kl_divergence(X, predicted_X):\n",
    "    \"\"\"Compute KL divergence between X and predicted_X.\"\"\"\n",
    "    d = X.shape[1]\n",
    "    \n",
    "    cov_X = np.cov(X, rowvar=False) + np.eye(d) * 1e-3\n",
    "    cov_pred = np.cov(predicted_X, rowvar=False) + np.eye(d) * 1e-3\n",
    "\n",
    "    eigvals_X, eigvecs_X = np.linalg.eigh(cov_X)\n",
    "    eigvals_pred, eigvecs_pred = np.linalg.eigh(cov_pred)\n",
    "\n",
    "    eigvals_X = np.clip(eigvals_X, 1e-6, None)\n",
    "    eigvals_pred = np.clip(eigvals_pred, 1e-6, None)\n",
    "\n",
    "    sqrt_cov_X = eigvecs_X @ np.diag(np.sqrt(eigvals_X)) @ eigvecs_X.T\n",
    "    sqrt_cov_pred = eigvecs_pred @ np.diag(np.sqrt(eigvals_pred)) @ eigvecs_pred.T\n",
    "\n",
    "    predicted_X = predicted_X @ np.linalg.inv(sqrt_cov_pred) @ sqrt_cov_X\n",
    "\n",
    "    try:\n",
    "        inv_cov_pred = np.linalg.inv(cov_pred)\n",
    "    except np.linalg.LinAlgError:\n",
    "        return np.inf\n",
    "\n",
    "    trace_term = np.trace(inv_cov_pred @ cov_X)\n",
    "    mean_diff = np.mean(predicted_X, axis=0) - np.mean(X, axis=0)\n",
    "    quadratic_term = mean_diff.T @ inv_cov_pred @ mean_diff\n",
    "\n",
    "    log_det_term = np.sum(np.log(np.maximum(eigvals_pred, 1e-12))) - np.sum(np.log(np.maximum(eigvals_X, 1e-12)))\n",
    "\n",
    "    kl = 0.5 * (trace_term + quadratic_term - d + log_det_term)\n",
    "    return np.maximum(kl, 1e-12)\n",
    "\n",
    "\n",
    "def js_divergence(X, predicted_X):\n",
    "    \"\"\"Compute the JS divergence between X and predicted_X.\"\"\"\n",
    "    M = 0.5 * (X + predicted_X)\n",
    "    return 0.5 * (kl_divergence(X, M) + kl_divergence(predicted_X, M))\n",
    "\n",
    "\n",
    "def compute_cyclic_fit_metrics(X, B_learned, noise_std=1.0):\n",
    "    \"\"\"Compute Log-Likelihood and KL Divergence\"\"\"\n",
    "    d = X.shape[1]\n",
    "    I = np.eye(d)\n",
    "\n",
    "    try:\n",
    "        W_est = estimate_weighted_adjacency(X, B_learned)\n",
    "\n",
    "        noise_est = ((I - W_est) @ X.T).T  # Solve for noise Îµ\n",
    "        log_p = -0.5 * np.sum((noise_est / noise_std) ** 2) - (X.shape[0] * d / 2) * np.log(2 * np.pi * noise_std ** 2)\n",
    "        log_p = np.maximum(log_p, -1e12)\n",
    "\n",
    "        kl_div = kl_divergence(X, (W_est @ X.T).T)\n",
    "\n",
    "        return log_p.item(), kl_div\n",
    "\n",
    "    except np.linalg.LinAlgError:\n",
    "        return -1e12, np.inf        \n",
    "\n",
    "# apply the above function compute_cyclic_fit_metrics to all methods' B_est and print the log-likelihood and KL divergence\n",
    "log_likelihood_our, kl_div_our = compute_cyclic_fit_metrics(X, B_est)\n",
    "log_likelihood_ling, kl_div_ling = compute_cyclic_fit_metrics(X, B_est_ling)\n",
    "log_likelihood_dglearn, kl_div_dglearn = compute_cyclic_fit_metrics(X, B_est_dglearn)\n",
    "log_likelihood_frp, kl_div_frp = compute_cyclic_fit_metrics(X, B_est_frp)\n",
    "log_likelihood_nt, kl_div_nt = compute_cyclic_fit_metrics(X, B_est_nt)\n",
    "log_likelihood_gol, kl_div_gol = compute_cyclic_fit_metrics(X, B_est_gol)\n",
    "log_likelihood_nodags, kl_div_nodags = compute_cyclic_fit_metrics(X, B_est_nodags)\n",
    "\n",
    "print(f\"Log-likelihood, KL divergence using our method: {log_likelihood_our:.3f}, {kl_div_our:.3f}\")\n",
    "print(f\"Log-likelihood, KL divergence using LinG method: {log_likelihood_ling:.3f}, {kl_div_ling:.3f}\")\n",
    "print(f\"Log-likelihood, KL divergence using dglearn method: {log_likelihood_dglearn:.3f}, {kl_div_dglearn:.3f}\")\n",
    "print(f\"Log-likelihood, KL divergence using FRP method: {log_likelihood_frp:.3f}, {kl_div_frp:.3f}\")\n",
    "print(f\"Log-likelihood, KL divergence using Notears method: {log_likelihood_nt:.3f}, {kl_div_nt:.3f}\")\n",
    "print(f\"Log-likelihood, KL divergence using GOLEM method: {log_likelihood_gol:.3f}, {kl_div_gol:.3f}\")\n",
    "print(f\"Log-likelihood, KL divergence using NODAGS method: {log_likelihood_nodags:.3f}, {kl_div_nodags:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute compute_min_KLD() for all methods and print the results\n",
    "minimize_kld_our = compute_min_KLD(prec_matrix, B_est)\n",
    "minimize_kld_ling = compute_min_KLD(prec_matrix, B_est_ling)\n",
    "minimize_kld_dglearn = compute_min_KLD(prec_matrix, B_est_dglearn)\n",
    "minimize_kld_frp = compute_min_KLD(prec_matrix, B_est_frp)\n",
    "minimize_kld_nt = compute_min_KLD(prec_matrix, B_est_nt)\n",
    "minimize_kld_gol = compute_min_KLD(prec_matrix, B_est_gol)\n",
    "minimize_kld_nodags = compute_min_KLD(prec_matrix, B_est_nodags)\n",
    "\n",
    "print(f\"Min KLD using our method: {minimize_kld_our}\")\n",
    "print(f\"Min KLD using LinG method: {minimize_kld_ling}\")\n",
    "print(f\"Min KLD using dglearn method: {minimize_kld_dglearn}\")\n",
    "print(f\"Min KLD using FRP method: {minimize_kld_frp}\")\n",
    "print(f\"Min KLD using Notears method: {minimize_kld_nt}\")\n",
    "print(f\"Min KLD using GOLEM method: {minimize_kld_gol}\")\n",
    "print(f\"Min KLD using NODAGS method: {minimize_kld_nodags}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################Â MISC NON CYCLICÂ MODELS ################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import semopy\n",
    "is_ordinal = np.array([0 if len(np.unique(X[:, col])) > 2 else 1 for col in range(X.shape[1])]).tolist() # 0 for continuous, 1 for ordinal\n",
    "\n",
    "# build desc\n",
    "desc = \"\"\n",
    "eta_names = []\n",
    "\n",
    "for i, row in enumerate(B_est_frp):\n",
    "    # exogenous\n",
    "    if np.sum(np.isnan(row)) == 0 and np.sum(np.isclose(row, 0)) == row.shape[0]:\n",
    "        continue\n",
    "\n",
    "    desc += f\"x{i:d} ~ \"\n",
    "\n",
    "    for j, elem in enumerate(row):\n",
    "        if np.isnan(elem):\n",
    "            eta_name = f\"eta_{i}_{j}\" if i < j else f\"eta_{j}_{i}\"\n",
    "            desc += f\"{eta_name} + \"\n",
    "            if eta_name not in eta_names:\n",
    "                eta_names.append(eta_name)\n",
    "        elif not np.isclose(elem, 0):\n",
    "            desc += f\"{elem:f} * x{j:d} + \"\n",
    "    desc = desc[: -len(\" * \")] + \"\\n\"\n",
    "\n",
    "if len(eta_names) > 0:\n",
    "    desc += \"DEFINE(latent) \" + \" \".join(eta_names) + \"\\n\"\n",
    "\n",
    "if sum(is_ordinal) > 0:\n",
    "    indices = np.argwhere(is_ordinal).flatten()\n",
    "\n",
    "    desc += \"DEFINE(ordinal)\"\n",
    "    for i in indices:\n",
    "        desc += f\" x{i}\"\n",
    "    desc += \"\\n\"\n",
    "\n",
    "columns = [f\"x{i:d}\" for i in range(X.shape[1])]\n",
    "_X = pd.DataFrame(X, columns=columns)\n",
    "\n",
    "m = semopy.Model(desc)\n",
    "m.fit(_X)\n",
    "stats = semopy.calc_stats(m)\n",
    "display(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_temp = B_est # rank 9 and zero determinant\n",
    "# B_temp = B_est_ling # full rank and non-zero determinant\n",
    "# B_temp = B_est_dglearn # rank 8 and zero determinant\n",
    "# B_temp = B_est_frp # full rank and non-zero determinant\n",
    "# B_temp = B_est_nt # rank 9 and non-zero determinant\n",
    "# B_temp = B_est_gol # rank 7 and zero determinant\n",
    "# B_temp = B_est_nodags # full rank and non-zero determinant\n",
    "\n",
    "# Assuming B_temp is already defined\n",
    "print(\"Shape of B_temp:\", B_temp.shape)\n",
    "print(\"Data type of B_temp:\", B_temp.dtype)\n",
    "\n",
    "# Convert B_temp to a numerical matrix\n",
    "B_temp_numeric = B_temp.astype(float)\n",
    "\n",
    "det = np.linalg.det(B_temp_numeric)\n",
    "print(\"Determinant of B_temp:\", det)\n",
    "\n",
    "rank = np.linalg.matrix_rank(B_temp_numeric)\n",
    "print(\"Rank of B_temp:\", rank)\n",
    "\n",
    "cond_num = np.linalg.cond(B_temp_numeric)\n",
    "print(\"Condition number of B_temp:\", cond_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pcax24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
