{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-27 00:06:17.121400: W external/xla/xla/service/gpu/nvptx_compiler.cc:760] The NVIDIA driver's CUDA version is 12.2 which is older than the ptxas CUDA version (12.5.40). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n"
     ]
    }
   ],
   "source": [
    "from typing import Callable\n",
    "import random\n",
    "from collections import namedtuple\n",
    "\n",
    "import jax\n",
    "import numpy as np\n",
    "import optax\n",
    "\n",
    "import pcax as px\n",
    "import pcax.nn as pxnn\n",
    "import pcax.functional as pxf\n",
    "import pcax.utils as pxu\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset = namedtuple(\"Dataset\", [\"train_loader\", \"val_loader\", \"test_loader\"])\n",
    "\n",
    "# This is a simple collate function that stacks numpy arrays used to interface\n",
    "# the PyTorch dataloader with JAX. In the future we hope to provide custom dataloaders\n",
    "# that are independent of PyTorch.\n",
    "def numpy_collate(batch):\n",
    "    if isinstance(batch[0], np.ndarray):\n",
    "        return np.stack(batch)\n",
    "    elif isinstance(batch[0], (tuple, list)):\n",
    "        transposed = zip(*batch)\n",
    "        return [numpy_collate(samples) for samples in transposed]\n",
    "    else:\n",
    "        return np.array(batch)\n",
    "\n",
    "# The dataloader assumes cuda is being used, as such it sets 'pin_memory = True' and\n",
    "# 'prefetch_factor = 2'. Note that the batch size should be constant during training, so\n",
    "# we set 'drop_last = True' to avoid having to deal with variable batch sizes.\n",
    "class TorchDataloader(torch.utils.data.DataLoader):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset,\n",
    "        batch_size=1,\n",
    "        shuffle=None,\n",
    "        sampler=None,\n",
    "        batch_sampler=None,\n",
    "        num_workers=16,\n",
    "        pin_memory=True,\n",
    "        timeout=0,\n",
    "        worker_init_fn=None,\n",
    "        persistent_workers=True,\n",
    "        prefetch_factor=2,\n",
    "    ):\n",
    "        super(self.__class__, self).__init__(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            sampler=sampler,\n",
    "            batch_sampler=batch_sampler,\n",
    "            num_workers=num_workers,\n",
    "            collate_fn=numpy_collate,\n",
    "            pin_memory=pin_memory,\n",
    "            drop_last=True if batch_sampler is None else None,\n",
    "            timeout=timeout,\n",
    "            worker_init_fn=worker_init_fn,\n",
    "            persistent_workers=persistent_workers,\n",
    "            prefetch_factor=prefetch_factor,\n",
    "        )\n",
    "\n",
    "# Function to add noise to the labels in the dataset\n",
    "def add_label_noise(dataset, noise_level=0.2):\n",
    "    targets = np.array(dataset.targets)\n",
    "    num_classes = len(np.unique(targets))\n",
    "    num_noisy = int(noise_level * len(targets))\n",
    "    noisy_indices = np.random.choice(len(targets), num_noisy, replace=False)\n",
    "\n",
    "    for idx in noisy_indices:\n",
    "        original_label = targets[idx]\n",
    "        new_label = original_label\n",
    "        while new_label == original_label:\n",
    "            new_label = np.random.randint(0, num_classes)\n",
    "        targets[idx] = new_label\n",
    "\n",
    "    dataset.targets = torch.tensor(targets)\n",
    "    return dataset\n",
    "\n",
    "# Function to get the dataloaders\n",
    "def get_dataloaders(dataset_name, train_subset_size, batch_size, noise_level=0.2):\n",
    "    if dataset_name.lower() == \"mnist\":\n",
    "        ds = datasets.MNIST\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Dataset {dataset_name} isn't available\")\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,)),\n",
    "        transforms.Lambda(lambda x: x.view(-1).numpy())  # Flatten the image to a vector\n",
    "    ])\n",
    "\n",
    "    train_set = ds(root='./data', download=True, train=True, transform=transform)\n",
    "    train_set = add_label_noise(train_set, noise_level=noise_level)\n",
    "\n",
    "    val_subset_size = int(0.2 * train_subset_size)\n",
    "    random_train_indices = np.random.choice(len(train_set), size=train_subset_size, replace=False)\n",
    "    remaining_indices = np.setdiff1d(np.arange(len(train_set)), random_train_indices)\n",
    "    random_val_indices = np.random.choice(remaining_indices, size=val_subset_size, replace=False)\n",
    "\n",
    "    train_loader = TorchDataloader(\n",
    "        train_set, batch_size=batch_size, num_workers=16,\n",
    "        sampler=torch.utils.data.sampler.SubsetRandomSampler(random_train_indices))\n",
    "    val_loader = TorchDataloader(\n",
    "        train_set, batch_size=batch_size, num_workers=16,\n",
    "        sampler=torch.utils.data.sampler.SubsetRandomSampler(random_val_indices))\n",
    "\n",
    "    test_set = ds(root='./data', download=True, train=False, transform=transform)\n",
    "    test_loader = TorchDataloader(\n",
    "        test_set, batch_size=batch_size, shuffle=False, num_workers=16)\n",
    "\n",
    "    return Dataset(train_loader=train_loader, val_loader=val_loader, test_loader=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(px.Module):\n",
    "    def __init__(\n",
    "        self, input_dim: int, hidden_dim: int, output_dim: int, act_fn: Callable[[jax.Array], jax.Array]\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.act_fn = px.static(act_fn)\n",
    "\n",
    "        self.layers = [\n",
    "            pxnn.Linear(input_dim, hidden_dim),\n",
    "            pxnn.Linear(hidden_dim, output_dim)\n",
    "        ]\n",
    "\n",
    "        # create a glorot uniform initializer:\n",
    "        # see: https://pytorch.org/docs/2.0/nn.init.html?highlight=xavier#torch.nn.init.xavier_uniform_\n",
    "        # see: https://jax.readthedocs.io/en/latest/_autosummary/jax.nn.initializers.variance_scaling.html#jax.nn.initializers.variance_scaling\n",
    "        #initializer = jax.nn.initializers.glorot_uniform() # this is wrong\n",
    "        # relu adjust JAX scale value\n",
    "        scale_ = 6.0\n",
    "        initializer_ = jax.nn.initializers.variance_scaling(scale=scale_, mode='fan_avg', distribution='uniform')\n",
    "        # more here: https://jax.readthedocs.io/en/latest/_autosummary/jax.nn.initializers.variance_scaling.html#jax.nn.initializers.variance_scaling\n",
    "        # now apply glorot uniform initialization to the weights only\n",
    "        for l in self.layers:\n",
    "            l.nn.weight.set(initializer_(px.RKG(), l.nn.weight.shape))\n",
    "\n",
    "    @staticmethod\n",
    "    def name():\n",
    "        return \"two_layer_nn\"\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = self.act_fn(layer(x))\n",
    "\n",
    "        x = self.layers[-1](x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ce_loss(output, one_hot_label):\n",
    "    return -one_hot_label * jax.nn.log_softmax(output)\n",
    "\n",
    "\n",
    "@pxf.vmap({\"model\": None}, in_axes=0, out_axes=0)\n",
    "def forward(x, *, model: Model):\n",
    "    return model(x)\n",
    "\n",
    "\n",
    "@pxf.vmap({\"model\": None}, in_axes=(0, 0), out_axes=(None, 0), axis_name=\"batch\")\n",
    "def loss(x, y, *, model: Model):\n",
    "    y_ = model(x)\n",
    "    return jax.lax.pmean(ce_loss(y_, y).sum(), \"batch\"), y_\n",
    "\n",
    "\n",
    "@pxf.jit()\n",
    "def train_on_batch(x: jax.Array, y: jax.Array, *, model: Model, optim_w: pxu.Optim):\n",
    "    model.train()\n",
    "\n",
    "    with pxu.step(model):\n",
    "        (e, y_), g = pxf.value_and_grad(pxu.Mask(pxnn.LayerParam, [False, True]), has_aux=True)(loss)(x, y, model=model)\n",
    "    optim_w.step(model, g[\"model\"])\n",
    "\n",
    "\n",
    "def train(dl, *, model: Model, optim_w: pxu.Optim):\n",
    "    for x, y in dl:\n",
    "        train_on_batch(x, jax.nn.one_hot(y, 10), model=model, optim_w=optim_w)\n",
    "\n",
    "\n",
    "@pxf.jit()\n",
    "def eval_on_batch(x: jax.Array, y: jax.Array, *, model: Model):\n",
    "    model.eval()\n",
    "\n",
    "    with pxu.step(model):\n",
    "        e, y_ = loss(x, jax.nn.one_hot(y, 10), model=model)\n",
    "        y_ = y_.argmax(axis=-1)\n",
    "\n",
    "    return (y_ == y).mean(), y_, e\n",
    "\n",
    "\n",
    "def eval(dl, *, model: Model):\n",
    "    acc = []\n",
    "    es = []\n",
    "    ys_ = []\n",
    "    for x, y in dl:\n",
    "        a, y_, e = eval_on_batch(x, y, model=model)\n",
    "        acc.append(a)\n",
    "        es.append(e)\n",
    "        ys_.append(y_)\n",
    "\n",
    "    return np.mean(acc), np.concatenate(ys_), np.mean(es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 4000 samples\n",
      "Validation set: 800 samples\n",
      "Test set: 10000 samples\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "nm_epochs = 4000\n",
    "model = Model(input_dim=784, hidden_dim=128, output_dim=10, act_fn=jax.nn.relu)\n",
    "\n",
    "# Assuming dataset is the namedtuple with the dataloaders\n",
    "dataset = get_dataloaders(\"mnist\", train_subset_size=4000, batch_size=batch_size, noise_level=0.0)\n",
    "# Check the sizes of the datasets\n",
    "print(f\"Training set: {len(dataset.train_loader.sampler)} samples\")\n",
    "print(f\"Validation set: {len(dataset.val_loader.sampler)} samples\")\n",
    "print(f\"Test set: {len(dataset.test_loader.dataset)} samples\")\n",
    "\n",
    "with pxu.step(model):\n",
    "    optim_w = pxu.Optim(optax.sgd(1e-2, momentum=0.95), pxu.Mask(pxnn.LayerParam)(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "784\n",
      "MNIST\n",
      "two_layer_nn\n",
      "(Model):\n",
      "  .layers[0].nn.weight: LayerParam([128,784], float32)\n",
      "  .layers[0].nn.bias: LayerParam([128], float32)\n",
      "  .layers[1].nn.weight: LayerParam([10,128], float32)\n",
      "  .layers[1].nn.bias: LayerParam([10], float32)\n"
     ]
    }
   ],
   "source": [
    "# show number of classes in the dataset by accessing countingn the unique labels\n",
    "print(len(np.unique(dataset.train_loader.dataset.targets)))\n",
    "# compute input dimension of the model\n",
    "input_dim = dataset.train_loader.dataset[0][0].shape[0]\n",
    "print(input_dim)\n",
    "# show the name of the dataset\n",
    "print(type(dataset.train_loader.dataset).__name__)\n",
    "# print name of the model\n",
    "print(model.name())\n",
    "# now compute the total number of parameters in the model\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].nn.bias.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4142135623730951"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "nn.init.calculate_gain(\"relu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to keep track of losses and accuracies\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "test_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "# keep track of the train and test losses and accuracies for plotting purposes\n",
    "for e in range(nm_epochs):\n",
    "    # train the model\n",
    "    train(dataset.train_loader, model=model, optim_w=optim_w)\n",
    "    \n",
    "    # evaluate the model and get accuracies and losses\n",
    "    a_train, ys_train, e_train = eval(dataset.train_loader, model=model)\n",
    "    a_val, ys_val, e_val = eval(dataset.val_loader, model=model)\n",
    "    a_test, ys_test, e_test = eval(dataset.test_loader, model=model)\n",
    "\n",
    "    # append losses and accuracies to lists\n",
    "    train_losses.append(e_train)\n",
    "    val_losses.append(e_val)\n",
    "    test_losses.append(e_test)\n",
    "    train_accuracies.append(a_train)\n",
    "    val_accuracies.append(a_val)\n",
    "    test_accuracies.append(a_test)\n",
    "\n",
    "    # print accuracies\n",
    "    print(f\"Epoch {e + 1}/{nm_epochs} - Training Accuracy: {a_train * 100:.2f}% - Validation Accuracy: {a_val * 100:.2f}% - Test Accuracy: {a_test * 100:.2f}%\")\n",
    "    # print losses\n",
    "    print(f\"Epoch {e + 1}/{nm_epochs} - Training Loss: {e_train:.4f} - Validation Loss: {e_val:.4f} - Test Loss: {e_test:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store/save the lists to a file for later use\n",
    "np.savez(\"mnist_bp_results.npz\", train_losses=train_losses, val_losses=val_losses, test_losses=test_losses, train_accuracies=train_accuracies, val_accuracies=val_accuracies, test_accuracies=test_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now reload the file and plot the results\n",
    "results = np.load(\"mnist_bp_results.npz\")\n",
    "train_losses = results[\"train_losses\"]\n",
    "val_losses = results[\"val_losses\"]\n",
    "test_losses = results[\"test_losses\"]\n",
    "train_accuracies = results[\"train_accuracies\"]\n",
    "val_accuracies = results[\"val_accuracies\"]\n",
    "test_accuracies = results[\"test_accuracies\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the losses\n",
    "nm_epochs = len(train_losses)\n",
    "plt.figure(figsize=(24, 12))\n",
    "plt.plot(range(1, nm_epochs + 1), train_losses, label='Training Loss')\n",
    "#plt.plot(range(1, nm_epochs + 1), val_losses, label='Validation Loss')\n",
    "plt.plot(range(1, nm_epochs + 1), test_losses, label='Test Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('BP: Loss vs. Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plotting the accuracies\n",
    "plt.figure(figsize=(24, 12))\n",
    "plt.plot(range(1, nm_epochs + 1), train_accuracies, label='Training Accuracy')\n",
    "#plt.plot(range(1, nm_epochs + 1), val_accuracies, label='Validation Accuracy')\n",
    "plt.plot(range(1, nm_epochs + 1), test_accuracies, label='Test Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('BP: Accuracy vs. Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a random matplotlib figure of a sine wave\n",
    "x = np.linspace(0, 2 * np.pi, 100)\n",
    "y = np.sin(x)\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pcax24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
