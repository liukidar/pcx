{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-26 02:10:18.778193: W external/xla/xla/service/gpu/nvptx_compiler.cc:760] The NVIDIA driver's CUDA version is 12.2 which is older than the ptxas CUDA version (12.5.40). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n"
     ]
    }
   ],
   "source": [
    "from typing import Callable\n",
    "import random\n",
    "from collections import namedtuple\n",
    "\n",
    "import jax\n",
    "import numpy as np\n",
    "import optax\n",
    "\n",
    "import pcax as px\n",
    "import pcax.nn as pxnn\n",
    "import pcax.functional as pxf\n",
    "import pcax.utils as pxu\n",
    "\n",
    "import torchvision\n",
    "#import torchvision.transforms as transforms\n",
    "from torchvision import datasets, transforms\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset = namedtuple(\"Dataset\", [\"train_loader\", \"val_loader\", \"test_loader\"])\n",
    "\n",
    "# This is a simple collate function that stacks numpy arrays used to interface\n",
    "# the PyTorch dataloader with JAX. In the future we hope to provide custom dataloaders\n",
    "# that are independent of PyTorch.\n",
    "def numpy_collate(batch):\n",
    "    if isinstance(batch[0], np.ndarray):\n",
    "        return np.stack(batch)\n",
    "    elif isinstance(batch[0], (tuple, list)):\n",
    "        transposed = zip(*batch)\n",
    "        return [numpy_collate(samples) for samples in transposed]\n",
    "    else:\n",
    "        return np.array(batch)\n",
    "\n",
    "# The dataloader assumes cuda is being used, as such it sets 'pin_memory = True' and\n",
    "# 'prefetch_factor = 2'. Note that the batch size should be constant during training, so\n",
    "# we set 'drop_last = True' to avoid having to deal with variable batch sizes.\n",
    "class TorchDataloader(torch.utils.data.DataLoader):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset,\n",
    "        batch_size=1,\n",
    "        shuffle=None,\n",
    "        sampler=None,\n",
    "        batch_sampler=None,\n",
    "        num_workers=16,\n",
    "        pin_memory=True,\n",
    "        timeout=0,\n",
    "        worker_init_fn=None,\n",
    "        persistent_workers=True,\n",
    "        prefetch_factor=2,\n",
    "    ):\n",
    "        super(self.__class__, self).__init__(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            sampler=sampler,\n",
    "            batch_sampler=batch_sampler,\n",
    "            num_workers=num_workers,\n",
    "            collate_fn=numpy_collate,\n",
    "            pin_memory=pin_memory,\n",
    "            drop_last=True if batch_sampler is None else None,\n",
    "            timeout=timeout,\n",
    "            worker_init_fn=worker_init_fn,\n",
    "            persistent_workers=persistent_workers,\n",
    "            prefetch_factor=prefetch_factor,\n",
    "        )\n",
    "\n",
    "# Function to add noise to the labels in the dataset\n",
    "def add_label_noise(dataset, noise_level=0.2):\n",
    "    targets = np.array(dataset.targets)\n",
    "    num_classes = len(np.unique(targets))\n",
    "    num_noisy = int(noise_level * len(targets))\n",
    "    noisy_indices = np.random.choice(len(targets), num_noisy, replace=False)\n",
    "\n",
    "    for idx in noisy_indices:\n",
    "        original_label = targets[idx]\n",
    "        new_label = original_label\n",
    "        while new_label == original_label:\n",
    "            new_label = np.random.randint(0, num_classes)\n",
    "        targets[idx] = new_label\n",
    "\n",
    "    dataset.targets = torch.tensor(targets)\n",
    "    return dataset\n",
    "\n",
    "# Function to get the dataloaders\n",
    "def get_dataloaders(dataset_name, train_subset_size, batch_size, noise_level=0.2):\n",
    "    if dataset_name.lower() == \"mnist\":\n",
    "        ds = datasets.MNIST\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Dataset {dataset_name} isn't available\")\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,)),\n",
    "        transforms.Lambda(lambda x: x.view(-1).numpy())  # Flatten the image to a vector\n",
    "    ])\n",
    "\n",
    "    train_set = ds(root='./data', download=True, train=True, transform=transform)\n",
    "    train_set = add_label_noise(train_set, noise_level=noise_level)\n",
    "\n",
    "    val_subset_size = int(0.2 * train_subset_size)\n",
    "    random_train_indices = np.random.choice(len(train_set), size=train_subset_size, replace=False)\n",
    "    remaining_indices = np.setdiff1d(np.arange(len(train_set)), random_train_indices)\n",
    "    random_val_indices = np.random.choice(remaining_indices, size=val_subset_size, replace=False)\n",
    "\n",
    "    train_loader = TorchDataloader(\n",
    "        train_set, batch_size=batch_size, num_workers=16,\n",
    "        sampler=torch.utils.data.sampler.SubsetRandomSampler(random_train_indices))\n",
    "    val_loader = TorchDataloader(\n",
    "        train_set, batch_size=batch_size, num_workers=16,\n",
    "        sampler=torch.utils.data.sampler.SubsetRandomSampler(random_val_indices))\n",
    "\n",
    "    test_set = ds(root='./data', download=True, train=False, transform=transform)\n",
    "    test_loader = TorchDataloader(\n",
    "        test_set, batch_size=batch_size, shuffle=False, num_workers=16)\n",
    "\n",
    "    return Dataset(train_loader=train_loader, val_loader=val_loader, test_loader=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(px.Module):\n",
    "    def __init__(\n",
    "        self, input_dim: int, hidden_dim: int, output_dim: int, act_fn: Callable[[jax.Array], jax.Array]\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.act_fn = px.static(act_fn)\n",
    "\n",
    "        self.layers = [\n",
    "            pxnn.Linear(input_dim, hidden_dim),\n",
    "            pxnn.Linear(hidden_dim, output_dim)\n",
    "        ]\n",
    "\n",
    "        # create a glorot uniform initializer\n",
    "        initializer = jax.nn.initializers.glorot_uniform()\n",
    "        # now apply glorot uniform initialization to the weights only\n",
    "        # the basic syntax is: model.layers[i].nn.weight.set(initializer(key, model.layers[i].nn.weight.shape))\n",
    "        for l in self.layers:\n",
    "            l.nn.weight.set(initializer(px.RKG(), l.nn.weight.shape))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = self.act_fn(layer(x))\n",
    "\n",
    "        x = self.layers[-1](x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ce_loss(output, one_hot_label):\n",
    "    return -one_hot_label * jax.nn.log_softmax(output)\n",
    "\n",
    "\n",
    "@pxf.vmap({\"model\": None}, in_axes=0, out_axes=0)\n",
    "def forward(x, *, model: Model):\n",
    "    return model(x)\n",
    "\n",
    "\n",
    "@pxf.vmap({\"model\": None}, in_axes=(0, 0), out_axes=(None, 0), axis_name=\"batch\")\n",
    "def loss(x, y, *, model: Model):\n",
    "    y_ = model(x)\n",
    "    return jax.lax.pmean(ce_loss(y_, y).sum(), \"batch\"), y_\n",
    "\n",
    "\n",
    "@pxf.jit()\n",
    "def train_on_batch(x: jax.Array, y: jax.Array, *, model: Model, optim_w: pxu.Optim):\n",
    "    model.train()\n",
    "\n",
    "    with pxu.step(model):\n",
    "        (e, y_), g = pxf.value_and_grad(pxu.Mask(pxnn.LayerParam, [False, True]), has_aux=True)(loss)(x, y, model=model)\n",
    "    optim_w.step(model, g[\"model\"])\n",
    "\n",
    "\n",
    "def train(dl, *, model: Model, optim_w: pxu.Optim):\n",
    "    for x, y in dl:\n",
    "        train_on_batch(x, jax.nn.one_hot(y, 10), model=model, optim_w=optim_w)\n",
    "\n",
    "\n",
    "@pxf.jit()\n",
    "def eval_on_batch(x: jax.Array, y: jax.Array, *, model: Model):\n",
    "    model.eval()\n",
    "\n",
    "    with pxu.step(model):\n",
    "        #y_ = forward(x, model=model).argmax(axis=-1)\n",
    "        # apply the loss function to get the loss and the predicted labels\n",
    "        e, y_ = loss(x, y, model=model)\n",
    "        y_ = y_.argmax(axis=-1)\n",
    "\n",
    "    return (y_ == y).mean(), y_, e\n",
    "\n",
    "def eval(dl, *, model: Model):\n",
    "    acc = []\n",
    "    ys_ = []\n",
    "    es = []\n",
    "    for x, y in dl:\n",
    "        a, y_, e = eval_on_batch(x, y, model=model)\n",
    "        acc.append(a)\n",
    "        ys_.append(y_)\n",
    "        es.append(e)\n",
    "\n",
    "    return np.mean(acc), np.concatenate(ys_), np.mean(es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 4000 samples\n",
      "Validation set: 800 samples\n",
      "Test set: 10000 samples\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "nm_epochs = 10\n",
    "model = Model(input_dim=784, hidden_dim=128, output_dim=10, act_fn=jax.nn.relu)\n",
    "\n",
    "# Assuming dataset is the namedtuple with the dataloaders\n",
    "dataset = get_dataloaders(\"mnist\", train_subset_size=4000, batch_size=batch_size, noise_level=0.0)\n",
    "# Check the sizes of the datasets\n",
    "print(f\"Training set: {len(dataset.train_loader.sampler)} samples\")\n",
    "print(f\"Validation set: {len(dataset.val_loader.sampler)} samples\")\n",
    "print(f\"Test set: {len(dataset.test_loader.dataset)} samples\")\n",
    "\n",
    "with pxu.step(model):\n",
    "    optim_w = pxu.Optim(optax.sgd(1e-3, momentum=0.95), pxu.Mask(pxnn.LayerParam)(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users-2/amine/miniconda3/envs/pcax24/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/users-2/amine/miniconda3/envs/pcax24/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Training Accuracy: 65.60% - Validation Accuracy: 61.07% - Test Accuracy: 65.84%\n",
      "Epoch 1/10 - Training Loss: 136.4472 - Validation Loss: 137.0139 - Test Loss: 137.3691\n",
      "Epoch 2/10 - Training Accuracy: 81.33% - Validation Accuracy: 76.56% - Test Accuracy: 80.58%\n",
      "Epoch 2/10 - Training Loss: 192.9845 - Validation Loss: 195.2245 - Test Loss: 195.8788\n",
      "Epoch 3/10 - Training Accuracy: 84.95% - Validation Accuracy: 81.12% - Test Accuracy: 84.85%\n",
      "Epoch 3/10 - Training Loss: 225.9127 - Validation Loss: 229.6466 - Test Loss: 229.9116\n",
      "Epoch 4/10 - Training Accuracy: 87.85% - Validation Accuracy: 83.85% - Test Accuracy: 86.73%\n",
      "Epoch 4/10 - Training Loss: 245.0537 - Validation Loss: 247.9847 - Test Loss: 249.4379\n",
      "Epoch 5/10 - Training Accuracy: 88.99% - Validation Accuracy: 84.64% - Test Accuracy: 88.02%\n",
      "Epoch 5/10 - Training Loss: 257.1745 - Validation Loss: 258.1328 - Test Loss: 261.7552\n",
      "Epoch 6/10 - Training Accuracy: 89.99% - Validation Accuracy: 85.16% - Test Accuracy: 88.60%\n",
      "Epoch 6/10 - Training Loss: 266.0207 - Validation Loss: 269.9104 - Test Loss: 270.6356\n",
      "Epoch 7/10 - Training Accuracy: 90.78% - Validation Accuracy: 86.59% - Test Accuracy: 89.25%\n",
      "Epoch 7/10 - Training Loss: 273.6975 - Validation Loss: 277.7762 - Test Loss: 278.4084\n",
      "Epoch 8/10 - Training Accuracy: 91.53% - Validation Accuracy: 86.07% - Test Accuracy: 89.54%\n",
      "Epoch 8/10 - Training Loss: 279.5911 - Validation Loss: 280.8452 - Test Loss: 284.2423\n",
      "Epoch 9/10 - Training Accuracy: 92.14% - Validation Accuracy: 87.24% - Test Accuracy: 89.89%\n",
      "Epoch 9/10 - Training Loss: 286.0422 - Validation Loss: 290.0071 - Test Loss: 291.1201\n",
      "Epoch 10/10 - Training Accuracy: 92.49% - Validation Accuracy: 86.85% - Test Accuracy: 90.31%\n",
      "Epoch 10/10 - Training Loss: 291.5862 - Validation Loss: 293.1792 - Test Loss: 296.8571\n"
     ]
    }
   ],
   "source": [
    "for e in range(nm_epochs):\n",
    "    # train the model and get the training loss\n",
    "    e_train = train(dataset.train_loader, model=model, optim_w=optim_w)\n",
    "    \n",
    "    # show the training accuracy, validation accuracy, and test accuracy all in one line together with the epoch number\n",
    "    a_train, _, e_train = eval(dataset.train_loader, model=model)\n",
    "    a_test, _, e_test = eval(dataset.test_loader, model=model)\n",
    "    a_val, _, e_val = eval(dataset.val_loader, model=model)\n",
    "\n",
    "    # print accuracies\n",
    "    print(f\"Epoch {e + 1}/{nm_epochs} - Training Accuracy: {a_train * 100:.2f}% - Validation Accuracy: {a_val * 100:.2f}% - Test Accuracy: {a_test * 100:.2f}%\")\n",
    "    # print losses\n",
    "    print(f\"Epoch {e + 1}/{nm_epochs} - Training Loss: {e_train:.4f} - Validation Loss: {e_val:.4f} - Test Loss: {e_test:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Model):\n",
      "  .layers[0].nn.weight: LayerParam([128,784], float32)\n",
      "  .layers[0].nn.bias: LayerParam([128], float32)\n",
      "  .layers[1].nn.weight: LayerParam([10,128], float32)\n",
      "  .layers[1].nn.bias: LayerParam([10], float32)\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pcax24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
