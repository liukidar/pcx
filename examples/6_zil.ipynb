{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial #7: Zer-divergence Inference Learning (ZIL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "import jax\n",
    "import optax\n",
    "import numpy as np\n",
    "\n",
    "import pcx as px\n",
    "import pcx.predictive_coding as pxc\n",
    "import pcx.nn as pxnn\n",
    "import pcx.functional as pxf\n",
    "import pcx.utils as pxu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(pxc.EnergyModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        hidden_dim: int,\n",
    "        output_dim: int,\n",
    "        nm_layers: int,\n",
    "        act_fn: Callable[[jax.Array], jax.Array],\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.act_fn = px.static(act_fn)\n",
    "\n",
    "        self.layers = (\n",
    "            [pxnn.Linear(input_dim, hidden_dim)]\n",
    "            + [pxnn.Linear(hidden_dim, hidden_dim) for _ in range(nm_layers - 2)]\n",
    "            + [pxnn.Linear(hidden_dim, output_dim)]\n",
    "        )\n",
    "\n",
    "        self.vodes = [pxc.Vode() for _ in range(nm_layers - 1)] + [pxc.Vode(energy_fn=pxc.ce_energy)]\n",
    "        self.vodes[-1].h.frozen = True\n",
    "\n",
    "    def __call__(self, x, y):\n",
    "        for v, l in zip(self.vodes[:-1], self.layers[:-1]):\n",
    "            x = v(self.act_fn(l(x)))\n",
    "\n",
    "        x = self.vodes[-1](self.layers[-1](x))\n",
    "\n",
    "        if y is not None:\n",
    "            self.vodes[-1].set(\"h\", y)\n",
    "\n",
    "        return self.vodes[-1].get(\"u\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pxf.vmap(\n",
    "    pxu.M(pxc.VodeParam | pxc.VodeParam.Cache).to((None, 0)), in_axes=(0, 0), out_axes=0\n",
    ")\n",
    "def forward(x, y, *, model: Model):\n",
    "    return model(x, y)\n",
    "\n",
    "\n",
    "@pxf.vmap(\n",
    "    pxu.M(pxc.VodeParam | pxc.VodeParam.Cache).to((None, 0)),\n",
    "    in_axes=(0,),\n",
    "    out_axes=(None, 0),\n",
    "    axis_name=\"batch\",\n",
    ")\n",
    "def energy(x, *, model: Model):\n",
    "    y_ = model(x, None)\n",
    "    return jax.lax.psum(model.energy(), \"batch\"), y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pxf.jit()\n",
    "def zil_train_on_batch(\n",
    "    x: jax.Array,\n",
    "    y: jax.Array,\n",
    "    *,\n",
    "    model: Model,\n",
    "    optim_w: pxu.Optim,\n",
    "    optim_h: pxu.Optim,\n",
    "):\n",
    "    model.train()\n",
    "\n",
    "    # Init step\n",
    "    with pxu.step(model, pxc.STATUS.INIT, clear_params=pxc.VodeParam.Cache):\n",
    "        forward(x, y, model=model)\n",
    "    optim_h.init(pxu.M_hasnot(pxc.VodeParam, frozen=True)(model))\n",
    "\n",
    "    # Inference steps\n",
    "    L = len(model.vodes)\n",
    "    for t in range(L):\n",
    "        with pxu.step(model, clear_params=pxc.VodeParam.Cache):\n",
    "            # We create an 'empty' mask, with all the parameters set to False.\n",
    "            mask = pxu.M(None).to([False, False])(model, is_pytree=True)\n",
    "\n",
    "            # Mask in vode: in order for ZIL to work, it is necessary to update\n",
    "            # only the vode of the layer that is being updated.\n",
    "            mask.vodes[L - t - 2].h = True\n",
    "            \n",
    "            # Mask in layer: we update the weights only of layer l, such that l = L - t - 1,\n",
    "            # as defined by the ZIL algorithm.\n",
    "            # Here, we could also manually set all the corresponding layer parameters of the\n",
    "            # mask to True, but we use the `pxu.M` utility to do it for us and replace\n",
    "            # the whole subtree of the model with the correctly masked one.\n",
    "            mask.layers[L - t - 1] = pxu.M(pxnn.LayerParam).to([False, True])(\n",
    "                model.layers[L - t - 1], is_pytree=True\n",
    "            )\n",
    "\n",
    "            (e, y_), g = pxf.value_and_grad({\"model\": mask}, has_aux=True)(energy)(\n",
    "                x, model=model\n",
    "            )\n",
    "\n",
    "        optim_h.step(model, g[\"model\"])\n",
    "        optim_w.step(model, g[\"model\"], scale_by=1.0 / x.shape[0])\n",
    "\n",
    "    optim_h.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_on_batch(x: jax.Array, y: jax.Array, *, model: Model):\n",
    "    model.eval()\n",
    "\n",
    "    with pxu.step(model, pxc.STATUS.INIT, clear_params=pxc.VodeParam.Cache):\n",
    "        y_ = forward(x, None, model=model).argmax(axis=-1)\n",
    "\n",
    "    return (y_ == y).mean(), y_\n",
    "\n",
    "\n",
    "# Standard training loop\n",
    "def zil_train(dl, *, model: Model, optim_w: pxu.Optim, optim_h: pxu.Optim):\n",
    "    for x, y in dl:\n",
    "        zil_train_on_batch(\n",
    "            x, jax.nn.one_hot(y, 2), model=model, optim_w=optim_w, optim_h=optim_h\n",
    "        )\n",
    "\n",
    "\n",
    "# Standard evaluation loop\n",
    "def eval(dl, *, model: Model):\n",
    "    acc = []\n",
    "    ys_ = []\n",
    "\n",
    "    for x, y in dl:\n",
    "        a, y_ = eval_on_batch(x, y, model=model)\n",
    "        acc.append(a)\n",
    "        ys_.append(y_)\n",
    "\n",
    "    return np.mean(acc), np.concatenate(ys_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "\n",
    "# this is unrelated to pcax: we generate and display the training set.\n",
    "nm_elements = 1024\n",
    "batch_size = nm_elements // 4\n",
    "nm_epochs = 8\n",
    "X, y = make_moons(\n",
    "    n_samples=batch_size * (nm_elements // batch_size), noise=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we split the dataset in training batches and do the same for the generated test set.\n",
    "train_dl = list(zip(X.reshape(-1, batch_size, 2), y.reshape(-1, batch_size)))\n",
    "\n",
    "X_test, y_test = make_moons(\n",
    "    n_samples=batch_size * (nm_elements // batch_size), noise=0.2, random_state=0\n",
    ")\n",
    "test_dl = tuple(zip(X_test.reshape(-1, batch_size, 2), y_test.reshape(-1, batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_seeds = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.RKG.seed(0)\n",
    "\n",
    "\n",
    "def zil_test():\n",
    "    max_acc = 0\n",
    "    max_acc_at_half_epochs = 0\n",
    "    model = Model(\n",
    "        input_dim=2, hidden_dim=32, output_dim=2, nm_layers=3, act_fn=jax.nn.leaky_relu\n",
    "    )\n",
    "\n",
    "\n",
    "    # 'pxu.Optim' accepts a optax optimizer and the target parameters in input. pxu.Mask\n",
    "    # can be used to partition between target parameters and not: when no 'map_to' is\n",
    "    # provided, such as here, it acts as 'eqx.partition', using pxc.VodeParam as filter.\n",
    "    optim_w = pxu.OptimTree(\n",
    "        lambda: optax.sgd(0.2, momentum=0.9, nesterov=True),\n",
    "        lambda x: isinstance(x, pxnn.Layer),\n",
    "        pxu.M(pxnn.LayerParam)(model),\n",
    "    )\n",
    "\n",
    "    # We only create the state optimizer `optim_h` without initialising it, since its state\n",
    "    # is batch-dependent and we want to re-initialise it for each new batch.\n",
    "    optim_h = pxu.OptimTree(\n",
    "        lambda: optax.sgd(1.0), lambda x: isinstance(x, pxc.Vode)\n",
    "    )\n",
    "\n",
    "\n",
    "    for e in range(nm_epochs):\n",
    "        zil_train(train_dl, model=model, optim_w=optim_w, optim_h=optim_h)\n",
    "        a, y = eval(test_dl, model=model)\n",
    "        \n",
    "        if a > max_acc:\n",
    "            max_acc = a\n",
    "            \n",
    "        if e == nm_epochs // 2:\n",
    "            max_acc_at_half_epochs = a\n",
    "\n",
    "    return max_acc, max_acc_at_half_epochs\n",
    "\n",
    "\n",
    "zil_accs = []\n",
    "for _ in range(n_seeds):\n",
    "    zil_accs.append(zil_test())\n",
    "\n",
    "avg_a = np.mean(zil_accs, axis=0)\n",
    "std_a = np.std(zil_accs, axis=0)\n",
    "print(f\"ZIL: {avg_a[0]:.2%} ± {std_a[0]:.2%}\")\n",
    "print(f\"ZIL[{nm_epochs // 2} epochs]: {avg_a[1]:.2%} ± {std_a[1]:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid training\n",
    "Here we combine ZIL with PC by first performing a weight update for each weight when $t=l$ and then, after $T$ steps performing a complete weight update, as we would do in PC.\n",
    "\n",
    "Note that ZIL and PC require significantly different learning rates, so that aspect could be still refined by creating a custom optimiser. However, as it is, hybrid training significantly outperform ZIL (and thus BP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pxf.jit(static_argnums=0)\n",
    "def hybrid_train_on_batch(\n",
    "    T: int,\n",
    "    x: jax.Array,\n",
    "    y: jax.Array,\n",
    "    *,\n",
    "    model: Model,\n",
    "    optim_w: pxu.Optim,\n",
    "    optim_h: pxu.Optim,\n",
    "):\n",
    "    model.train()\n",
    "\n",
    "    # Init step\n",
    "    with pxu.step(model, pxc.STATUS.INIT, clear_params=pxc.VodeParam.Cache):\n",
    "        forward(x, y, model=model)\n",
    "    optim_h.init(pxu.M_hasnot(pxc.VodeParam, frozen=True)(model))\n",
    "\n",
    "    # Inference steps\n",
    "    L = len(model.vodes)\n",
    "    for t in range(T):\n",
    "        with pxu.step(model, clear_params=pxc.VodeParam.Cache):\n",
    "            mask = pxu.M_hasnot(pxc.VodeParam, frozen=True).to([False, True])(\n",
    "                model, is_pytree=True\n",
    "            )\n",
    "\n",
    "            # Mask out vodes for better efficiency.\n",
    "            for j in range(0, L - t - 2):\n",
    "                mask.vodes[j].h = False\n",
    "            \n",
    "            # Mask in layer: we update the weights only of layer l, such that l = L - t - 1,\n",
    "            # as defined by the ZIL algorithm. If t > L - 1, we do not update any layer as\n",
    "            # we have already done the \"backpropagation\" pass, and we simply keep updating\n",
    "            # the state to then perform a \"predictive coding\" updated.\n",
    "            if L - t - 1 >= 0:\n",
    "                mask.layers[L - t - 1] = pxu.M(pxnn.LayerParam).to([False, True])(\n",
    "                    model.layers[L - t - 1], is_pytree=True\n",
    "                )\n",
    "\n",
    "            (e, y_), g = pxf.value_and_grad({\"model\": mask}, has_aux=True)(energy)(\n",
    "                x, model=model\n",
    "            )\n",
    "\n",
    "        optim_h.step(model, g[\"model\"])\n",
    "        if L - t - 1 >= 0:\n",
    "            optim_w.step(model, g[\"model\"], scale_by=1.0 / x.shape[0])\n",
    "\n",
    "    optim_h.clear()\n",
    "\n",
    "    # Weight update step\n",
    "    with pxu.step(model, clear_params=pxc.VodeParam.Cache):\n",
    "        (e, y_), g = pxf.value_and_grad(\n",
    "            pxu.M(pxnn.LayerParam).to([False, True]), has_aux=True\n",
    "        )(energy)(x, model=model)\n",
    "\n",
    "    optim_w.step(model, g[\"model\"], scale_by=1.0 / x.shape[0])\n",
    "\n",
    "\n",
    "# Standard training loop\n",
    "def hybrid_train(dl, T, *, model: Model, optim_w: pxu.Optim, optim_h: pxu.Optim):\n",
    "    for x, y in dl:\n",
    "        hybrid_train_on_batch(\n",
    "            T, x, jax.nn.one_hot(y, 2), model=model, optim_w=optim_w, optim_h=optim_h\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.RKG.seed(0)\n",
    "\n",
    "def zil_test(T):\n",
    "    max_acc = 0\n",
    "    max_acc_at_half_epochs = 0\n",
    "    model = Model(\n",
    "        input_dim=2, hidden_dim=32, output_dim=2, nm_layers=3, act_fn=jax.nn.leaky_relu\n",
    "    )\n",
    "\n",
    "    optim_w = pxu.OptimTree(\n",
    "        lambda: optax.sgd(0.2, momentum=0.9, nesterov=True),\n",
    "        lambda x: isinstance(x, pxnn.Layer),\n",
    "        pxu.M(pxnn.LayerParam)(model),\n",
    "    )\n",
    "\n",
    "    optim_h = pxu.OptimTree(\n",
    "        lambda: optax.sgd(\n",
    "            optax.linear_schedule(1.0, 0.1, 1)\n",
    "        ), lambda x: isinstance(x, pxc.Vode)\n",
    "    )\n",
    "\n",
    "    for e in range(nm_epochs):\n",
    "        hybrid_train(train_dl, T, model=model, optim_w=optim_w, optim_h=optim_h)\n",
    "        a, y = eval(test_dl, model=model)\n",
    "        \n",
    "        if a > max_acc:\n",
    "            max_acc = a\n",
    "            \n",
    "        if e == nm_epochs // 2:\n",
    "            max_acc_at_half_epochs = a\n",
    "\n",
    "    return max_acc, max_acc_at_half_epochs\n",
    "\n",
    "\n",
    "T = 8\n",
    "zil_accs = []\n",
    "for _ in range(n_seeds):\n",
    "    zil_accs.append(zil_test(T))\n",
    "\n",
    "avg_a = np.mean(zil_accs, axis=0)\n",
    "std_a = np.std(zil_accs, axis=0)\n",
    "print(f\"HIL: {avg_a[0]:.2%} ± {std_a[0]:.2%}\")\n",
    "print(f\"HIL[{nm_epochs // 2} epochs]: {avg_a[1]:.2%} ± {std_a[1]:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
