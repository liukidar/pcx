{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# choose the GPU to use\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "# disable preallocation of memory\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\n",
    "\n",
    "# pcx\n",
    "import pcx as px\n",
    "import pcx.predictive_coding as pxc\n",
    "import pcx.nn as pxnn\n",
    "import pcx.functional as pxf\n",
    "import pcx.utils as pxu\n",
    "\n",
    "# 3rd party\n",
    "import jax\n",
    "from jax import jit\n",
    "import jax.numpy as jnp\n",
    "import jax.random as random\n",
    "import optax\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.api.types import is_float_dtype\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import timeit\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# own\n",
    "import causal_helpers\n",
    "from causal_helpers import simulate_dag, simulate_parameter, simulate_linear_sem, simulate_linear_sem_cyclic\n",
    "from causal_helpers import load_adjacency_matrix, set_random_seed, plot_adjacency_matrices\n",
    "from causal_helpers import load_graph, load_adjacency_matrix\n",
    "from causal_metrics import compute_F1_directed, compute_F1_skeleton, compute_AUPRC, compute_AUROC, compute_cycle_F1\n",
    "from connectome_cyclic_data_generator import sample_cyclic_data\n",
    "\n",
    "# Set random seed\n",
    "seed = 42 # main seed for reproducibility\n",
    "set_random_seed(seed)\n",
    "\n",
    "# causal libraries\n",
    "import cdt, castle\n",
    "\n",
    "# causal metrics\n",
    "from cdt.metrics import precision_recall, SHD, SID\n",
    "from castle.metrics import MetricsDAG\n",
    "from castle.common import GraphDAG\n",
    "from causallearn.graph.SHD import SHD as SHD_causallearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   # fetch dataset \n",
    "# abalone = fetch_ucirepo(id=1) \n",
    "  \n",
    "# # data (as pandas dataframes) \n",
    "# X = abalone.data.features\n",
    "# y = abalone.data.targets\n",
    "  \n",
    "# # metadata \n",
    "# #print(abalone.metadata) \n",
    "\n",
    "# # now merge X and y to create a single dataframe and give the columns the correct names using abalone.variables.name.values\n",
    "# df = pd.concat([X, y], axis=1)\n",
    "# df.columns = abalone.variables.name.values.tolist()\n",
    "# print(df.head())\n",
    "# print()\n",
    "# # show # of unique values in each column\n",
    "# print(df.nunique())\n",
    "# # finally convert the Rings variable to a binary variable by setting the threshold to mean(rings)\n",
    "# df['Rings'] = df['Rings'] > df['Rings'].mean()\n",
    "# # then convert to integer\n",
    "# df['Rings'] = df['Rings'].astype(int)\n",
    "# # also replace the values in Sex with integers\n",
    "# df['Sex'] = df['Sex'].map({'M': 0, 'F': 1, 'I': 2})\n",
    "\n",
    "# # now show the first 5 rows of the dataframe\n",
    "# print(df.head())\n",
    "\n",
    "# # Create a boolean list for continuous variables (any float dtype)\n",
    "# is_cont_node = df.dtypes.map(is_float_dtype).tolist()\n",
    "\n",
    "# # Print the result\n",
    "# print(is_cont_node)\n",
    "\n",
    "# # plot the distribution of all variables in the dataframe\n",
    "# df.hist(figsize=(15, 10))\n",
    "# plt.show()\n",
    "\n",
    "# path to mixed_confounding data\n",
    "#path = '../data/custom_mixed_confounding/'\n",
    "path = '../data/custom_mixed_confounding_softplus/'\n",
    "\n",
    "# file name adjacency matrix in csv format\n",
    "mixed_confounding_adjacency_matrix = 'adj_matrix.csv'\n",
    "# file name observational data in csv format\n",
    "mixed_confounding_obs_data = 'train.csv'\n",
    "\n",
    "# load adjacency matrix and data as pandas dataframe, both files have no header\n",
    "adj_matrix = pd.read_csv(path + mixed_confounding_adjacency_matrix, header=None)\n",
    "data = pd.read_csv(path + mixed_confounding_obs_data, header=None)\n",
    "weighted_adj_matrix = pd.read_csv(path + 'W_adj_matrix.csv', header=None)\n",
    "\n",
    "B_true = adj_matrix.values\n",
    "X = data.values\n",
    "W_true = weighted_adj_matrix.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()\n",
    "# show unique values in each column\n",
    "print(data.nunique())\n",
    "# Determine if each variable is continuous or discrete based on the number of unique values\n",
    "is_cont_node = np.array([True if data[col].nunique() > 2 else False for col in data.columns])\n",
    "is_cont_node = is_cont_node.tolist()\n",
    "print(is_cont_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ## Load the actual connectome data\n",
    "\n",
    "# # %%\n",
    "# # load the weighted adjacency matrices for ER and connectome\n",
    "\n",
    "# # Specify the folder where the adjacency matrices were saved\n",
    "# folder = '../data/'\n",
    "\n",
    "# # Specify the folder where the acyclic positive integer weighted connectome data was saved\n",
    "# folder_cyclic = '/home/amine.mcharrak/connectome/data/'\n",
    "# # Specify the folder where the acyclic positive integer weighted connectome data was saved\n",
    "# folder_acyclic = '/home/amine.mcharrak/connectome/data/'\n",
    "\n",
    "# # Example usage to load the saved adjacency matrices\n",
    "# # G_A_init_t_ordered_adj_matrix = load_adjacency_matrix(os.path.join(folder, 'G_A_init_t_ordered_adj_matrix.npy'))\n",
    "# # G_A_init_t_ordered_dag_adj_matrix = load_adjacency_matrix(os.path.join(folder, 'G_A_init_t_ordered_dag_adj_matrix.npy'))\n",
    "# # ER = load_adjacency_matrix(os.path.join(folder, 'ER_adj_matrix.npy'))\n",
    "# # ER_dag = load_adjacency_matrix(os.path.join(folder, 'ER_dag_adj_matrix.npy'))\n",
    "\n",
    "# # Change name of the connectome adjacency matrix to C and C_dag\n",
    "# # C = G_A_init_t_ordered_adj_matrix\n",
    "# # C_dag = G_A_init_t_ordered_dag_adj_matrix\n",
    "\n",
    "# # Now ensure that both DAG adjacency matrices are binary, if they aren't already\n",
    "# # ER_dag_bin = (ER_dag != 0).astype(int)\n",
    "# # C_dag_bin = (C_dag != 0).astype(int)\n",
    "\n",
    "# # ER_true = ER_dag_bin\n",
    "# # C_true = C_dag_bin\n",
    "\n",
    "# # %% [markdown]\n",
    "# # ## Create data to debug and implement the pcax version of NOTEARS\n",
    "\n",
    "# # %%\n",
    "# # actual data\n",
    "# B_true = simulate_dag(d=10, s0=20, graph_type='ER') # ER2\n",
    "# # B_true = simulate_dag(d=100, s0=400, graph_type='ER') # ER4\n",
    "# # debugging data\n",
    "# # B_true = simulate_dag(d=10, s0=20, graph_type='ER') # ER2\n",
    "\n",
    "\n",
    "# # B_true = C_dag_bin # if you want to use the connectome-based DAG # best\n",
    "# #B_true = ER_dag_bin # if you want to use the ER-based DAG\n",
    "\n",
    "# #B_true = simulate_dag(d=5, s0=10, graph_type='ER') # ER2\n",
    "# #B_true = simulate_dag(d=10, s0=20, graph_type='ER') # ER2\n",
    "# #B_true = simulate_dag(d=50, s0=100, graph_type='ER') # ER2\n",
    "# #B_true = simulate_dag(d=100, s0=200, graph_type='ER') # ER2\n",
    "# #B_true = simulate_dag(d=279, s0=558, graph_type='ER') # ER2\n",
    "\n",
    "# # create SF2 graph and SF4 graph with d=10 nodes\n",
    "# #B_true = simulate_dag(d=10, s0=20, graph_type='SF') # SF2\n",
    "# #B_true = simulate_dag(d=10, s0=40, graph_type='SF') # SF4\n",
    "# #B_true = simulate_dag(d=100, s0=400, graph_type='SF') # SF4\n",
    "\n",
    "# # create ER2 and ER4 graphs with d=100 nodes\n",
    "# #B_true = simulate_dag(d=100, s0=200, graph_type='ER') # ER2\n",
    "# #B_true = simulate_dag(d=100, s0=400, graph_type='ER') # ER4\n",
    "\n",
    "# # create equivalent ER4 and ER6 graphs\n",
    "# #B_true = simulate_dag(d=279, s0=1116, graph_type='ER') # ER4\n",
    "# #B_true = simulate_dag(d=279, s0=1674, graph_type='ER') # ER6\n",
    "\n",
    "# # create equivalent SF4 and SF6 graphs\n",
    "# #B_true = simulate_dag(d=100, s0=600, graph_type='SF') # SF6\n",
    "# #B_true = simulate_dag(d=279, s0=1116, graph_type='SF') # SF4\n",
    "# #B_true = simulate_dag(d=279, s0=1674, graph_type='SF') # SF6\n",
    "\n",
    "# # create simple data using simulate_dag method from causal_helpers with expected number of edges (s0) and number of nodes (d)\n",
    "# #B_true = simulate_dag(d=100, s0=199, graph_type='ER') # we use p≈0.040226 for the connectome-based ER_dag graph. This means that the expected number of edges is 0.040226 * d * (d-1) / 2\n",
    "# # examples: d=50 -> s0=49 (works), d=100 -> s0=199, d=200 -> s0=800\n",
    "\n",
    "# # create the weighted adjacency matrix based on the binary adjacency matrix\n",
    "# #W_true = simulate_parameter(B_true, connectome=True)\n",
    "# #W_true = simulate_parameter(B_true)\n",
    "\n",
    "# # sample data from the linear SEM\n",
    "# # actual data\n",
    "# #X = simulate_linear_sem(W_true, n=10000, sem_type='gauss')\n",
    "# #X = simulate_linear_sem(W_true, n=10000, sem_type='uniform')\n",
    "# # for debugging\n",
    "# #X = simulate_linear_sem(W_true, n=1000, sem_type='gauss')\n",
    "# #X = simulate_linear_sem(W_true, n=2500, sem_type='gauss')\n",
    "# #X = simulate_linear_sem(W_true, n=6250, sem_type='gauss')\n",
    "# #X = simulate_linear_sem(W_true, n=50000, sem_type='gauss')\n",
    "# #X = simulate_linear_sem(W_true, n=100000, sem_type='gauss') # 1000*(279**2)/(20**2) = 194602\n",
    "\n",
    "# # load the cyclic integer weighted connectome data adjacency matrix\n",
    "# #B_true_weighted = load_adjacency_matrix(os.path.join(folder_cyclic, 'A_init_t_ordered_adj_matrix_with_cycles.npy'))\n",
    "# #X, W_true = sample_cyclic_data(B_true_weighted, n_samples=10000, noise_type='non-gaussian')\n",
    "# #B_true = (W_true != 0).astype(int)\n",
    "\n",
    "# # load the acyclic integer weighted connectome data adjacency matrix\n",
    "# # B_true_weighted = load_adjacency_matrix(os.path.join(folder_acyclic, 'A_init_t_ordered_adj_matrix_no_cycles.npy'))\n",
    "# # print(\"B_true_weighted:\\n\", np.array_str(B_true_weighted, precision=4, suppress_small=True))\n",
    "\n",
    "# # A: use this for regular DAGs\n",
    "# W_true = simulate_parameter(B_true)\n",
    "\n",
    "# # B: use this for connectome-based DAGs\n",
    "# #W_true = simulate_parameter(B_true_weighted, connectome=True)\n",
    "# #B_true = (W_true != 0).astype(int)\n",
    "\n",
    "# # some print statements to check the values of W_true\n",
    "# print(\"W_true:\\n\", np.array_str(W_true, precision=4, suppress_small=True))\n",
    "# print(\"Mean of W_true:\", np.mean(W_true))\n",
    "# print(\"Variance of W_true:\", np.var(W_true))\n",
    "# print(\"Max value in W_true:\", np.max(W_true))\n",
    "# print(\"Min value in W_true:\", np.min(W_true))\n",
    "\n",
    "# # sample data from the linear SEM\n",
    "# X = simulate_linear_sem(W_true, n=1000, sem_type='gauss')\n",
    "\n",
    "# # now standardized data, where each variable is normalized to unit variance\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# scaler = StandardScaler()\n",
    "# X_std = scaler.fit_transform(X)\n",
    "\n",
    "# # NOTE: you may not write positional arguments after keyword arguments. \n",
    "# # That is, the values that you are passing positionally have to come first!\n",
    "\n",
    "# # create a dataset using the simulated data\n",
    "# # NOTE: NOTEARS paper uses n=1000 for graph with d=20.\n",
    "# # NOTE: d... number of nodes, p=d^2... number of parameters, n... number of samples. Then: comparing p1=d1^2 vs p2=d2^2 we have that: n1/p1 must be equal to n2/p2\n",
    "# # Thus we have n2 = n1 * p2 / p1. For the case of d2=100 we have that n2 = (n1*p2)/p1 = 1000*(100^2)/(20^2) = 25000 \n",
    "# # we should expect to use that many samples actually to be able to learn the graph in a comparable way.\n",
    "# #dataset = IIDSimulation(W=W_true, n=25000, method='linear', sem_type='gauss')\n",
    "# #true_dag, X = dataset.B, dataset.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print how many non-zero entries are in the true DAG\n",
    "print(f\"Number of non-zero entries in the true DAG: {np.count_nonzero(B_true)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility and evaluation functions\n",
    "@jit\n",
    "def MAE(W_true, W):\n",
    "    \"\"\"This function returns the Mean Absolute Error for the difference between the true weighted adjacency matrix W_true and th estimated one, W.\"\"\"\n",
    "    MAE_ = jnp.mean(jnp.abs(W - W_true))\n",
    "    return MAE_\n",
    "\n",
    "# Define fucntion to compute h_reg based W with h_reg = jnp.trace(jax.scipy.linalg.expm(W * W)) - d, here * denotes the hadamard product\n",
    "def compute_h_reg(W):\n",
    "    \"\"\"This function computes the h_reg term based on the matrix W.\"\"\"\n",
    "    h_reg = jnp.trace(jax.scipy.linalg.expm(W * W)) - W.shape[0]\n",
    "    return h_reg\n",
    "\n",
    "def compute_binary_adjacency(W, threshold=0.3, return_boolean=False):\n",
    "    \"\"\"\n",
    "    Compute the binary adjacency matrix by thresholding the input matrix.\n",
    "\n",
    "    Args:\n",
    "    - W (array-like): The weighted adjacency matrix (can be a JAX array or a NumPy array).\n",
    "    - threshold (float): The threshold value to determine the binary matrix. Default is 0.3.\n",
    "    - return_boolean (bool): If True, return the binary adjacency matrix as a Boolean array.\n",
    "                             Otherwise, return as an integer array (0/1). Default is False.\n",
    "\n",
    "    Returns:\n",
    "    - B_est (np.ndarray): The binary adjacency matrix where each element is 1 if the corresponding \n",
    "                          element in W is greater than the threshold, otherwise 0 (or Boolean True/False).\n",
    "    \"\"\"\n",
    "    # Convert JAX array to NumPy array if necessary\n",
    "    if isinstance(W, jnp.ndarray):\n",
    "        W = np.array(W)\n",
    "\n",
    "    # Compute the binary adjacency matrix\n",
    "    if return_boolean:\n",
    "        B_est = np.abs(W) > threshold\n",
    "    else:\n",
    "        B_est = np.where(np.abs(W) > threshold, 1, 0)\n",
    "\n",
    "    return B_est\n",
    "\n",
    "\n",
    "def ensure_DAG(W):\n",
    "    \"\"\"\n",
    "    Ensure that the weighted adjacency matrix corresponds to a DAG.\n",
    "\n",
    "    Inputs:\n",
    "        W: numpy.ndarray - a weighted adjacency matrix representing a directed graph\n",
    "\n",
    "    Outputs:\n",
    "        W: numpy.ndarray - a weighted adjacency matrix without cycles (DAG)\n",
    "    \"\"\"\n",
    "    # Convert the adjacency matrix to a directed graph\n",
    "    g = nx.DiGraph(W)\n",
    "\n",
    "    # Make a copy of the graph to modify\n",
    "    gg = g.copy()\n",
    "\n",
    "    # Remove cycles by removing edges\n",
    "    while not nx.is_directed_acyclic_graph(gg):\n",
    "        h = gg.copy()\n",
    "\n",
    "        # Remove all the sources and sinks\n",
    "        while True:\n",
    "            finished = True\n",
    "\n",
    "            for node, in_degree in nx.in_degree_centrality(h).items():\n",
    "                if in_degree == 0:\n",
    "                    h.remove_node(node)\n",
    "                    finished = False\n",
    "\n",
    "            for node, out_degree in nx.out_degree_centrality(h).items():\n",
    "                if out_degree == 0:\n",
    "                    h.remove_node(node)\n",
    "                    finished = False\n",
    "\n",
    "            if finished:\n",
    "                break\n",
    "\n",
    "        # Find a cycle with a random walk starting at a random node\n",
    "        node = list(h.nodes)[0]\n",
    "        cycle = [node]\n",
    "        while True:\n",
    "            edges = list(h.out_edges(node))\n",
    "            _, node = edges[np.random.choice(len(edges))]\n",
    "\n",
    "            if node in cycle:\n",
    "                break\n",
    "\n",
    "            cycle.append(node)\n",
    "\n",
    "        # Extract the cycle path and adjust it to start at the first occurrence of the repeated node\n",
    "        cycle = np.array(cycle)\n",
    "        i = np.argwhere(cycle == node)[0][0]\n",
    "        cycle = cycle[i:]\n",
    "        cycle = cycle.tolist() + [node]\n",
    "\n",
    "        # Find edges in that cycle\n",
    "        edges = list(zip(cycle[:-1], cycle[1:]))\n",
    "\n",
    "        # Randomly pick an edge to remove\n",
    "        edge = edges[np.random.choice(len(edges))]\n",
    "        gg.remove_edge(*edge)\n",
    "\n",
    "    # Convert the modified graph back to a weighted adjacency matrix\n",
    "    W_acyclic = nx.to_numpy_array(gg)\n",
    "\n",
    "    return W_acyclic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Complete_Graph(pxc.EnergyModule):\n",
    "    def __init__(self, input_dim: int, n_nodes: int, has_bias: bool = False, is_cont_node: list = None) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = px.static(input_dim)  # Ensure input_dim is static\n",
    "        self.n_nodes = px.static(n_nodes)  # Keep n_nodes as a static value\n",
    "        self.has_bias = has_bias\n",
    "        self.is_cont_node = is_cont_node\n",
    "\n",
    "        # Initialize a single linear layer for the weights and wrap it in a list\n",
    "        self.layers = [pxnn.Linear(n_nodes, n_nodes, bias=has_bias)] # vanilla initialization is uniform(-stdv, stdv) with stdv = 1/sqrt(n_nodes), here n_nodes = 12, thus stdv = 1/sqrt(12) = 0.2887\n",
    "        \n",
    "        #stddev = jnp.sqrt(0.01) # this equals 0.1 (default would have been 0.2887)\n",
    "        stddev = 1/n_nodes\n",
    "        key = random.PRNGKey(0)\n",
    "        #new_weight_matrix = random.normal(key, shape=(n_nodes, n_nodes)) * stddev # option 1 using normal distribution\n",
    "        new_weight_matrix = random.uniform(key, shape=(n_nodes, n_nodes), minval=-stddev, maxval=stddev) # option 2 using uniform distribution\n",
    "\n",
    "        # Step 3: Replace diagonal elements with 0\n",
    "        for i in range(n_nodes):\n",
    "            new_weight_matrix = new_weight_matrix.at[i, i].set(0.0)\n",
    "\n",
    "        # Step 5: Update the weight matrix\n",
    "        self.layers[0].nn.weight.set(new_weight_matrix)\n",
    "\n",
    "        # Initialize vodes based on is_cont_node\n",
    "        if is_cont_node is None:\n",
    "            is_cont_node = [True] * n_nodes  # Default to all continuous nodes if not provided\n",
    "\n",
    "        self.vodes = []\n",
    "        for is_cont in is_cont_node:\n",
    "            if is_cont:\n",
    "                self.vodes.append(pxc.Vode())\n",
    "            else:\n",
    "                self.vodes.append(pxc.Vode(pxc.bce_energy))\n",
    "\n",
    "    def freeze_nodes(self, freeze=True):\n",
    "        \"\"\"Freeze or unfreeze all vodes in the model.\"\"\"\n",
    "        for vode in self.vodes:\n",
    "            vode.h.frozen = freeze\n",
    "\n",
    "    def are_vodes_frozen(self):\n",
    "        \"\"\"Check if all vodes in the model are frozen.\"\"\"\n",
    "        return all(hasattr(vode.h, 'frozen') and vode.h.frozen for vode in self.vodes)\n",
    "    \n",
    "    def get_W(self):\n",
    "        \"\"\"This function returns the weighted adjacency matrix based on the linear layer in the model.\"\"\"\n",
    "        W = self.layers[0].nn.weight.get()\n",
    "        W_T = W.T\n",
    "        return W_T\n",
    "\n",
    "    def __call__(self, x=None):\n",
    "        n_nodes = self.n_nodes.get()\n",
    "\n",
    "        if x is not None:\n",
    "            # Initialize nodes with given data\n",
    "            reshaped_x = x.reshape(n_nodes, -1)  # Infer input_dim from x\n",
    "            \n",
    "            # Print the shape of reshaped_x[0] when x is not None\n",
    "            print(\"The shape of reshaped_x[0] when x is not None is: \", reshaped_x[0].shape)\n",
    "\n",
    "            for i in range(n_nodes):\n",
    "                self.vodes[i](reshaped_x[i])\n",
    "\n",
    "        else:\n",
    "            # Stack current state of vodes into a matrix of shape (n_nodes, input_dim)\n",
    "            x_ = jnp.vstack([vode.get('h') for vode in self.vodes])\n",
    "\n",
    "            # Print the shape of x_ when x is None\n",
    "            print(\"The shape of x_ when x is None is: \", x_.shape)\n",
    "\n",
    "            # Apply the linear transformation\n",
    "            output = self.layers[0](x_)\n",
    "\n",
    "            # Print the shape of output when x is None\n",
    "            print(\"The shape of output when x is None is: \", output.shape)\n",
    "\n",
    "            # Update the vodes with the output\n",
    "            for i in range(n_nodes):\n",
    "                self.vodes[i](output[i])\n",
    "\n",
    "        # Stack the final state of vodes for output\n",
    "        output = jnp.vstack([vode.get('h') for vode in self.vodes])\n",
    "\n",
    "        # Print the shape of the output\n",
    "        print(\"The shape of the output is: \", output.shape)\n",
    "\n",
    "        return output\n",
    "\n",
    "# Usage\n",
    "input_dim = 1\n",
    "n_nodes = X.shape[1]\n",
    "#model = Complete_Graph(input_dim, n_nodes, has_bias=False)\n",
    "#model = Complete_Graph(input_dim, n_nodes, has_bias=True)\n",
    "model = Complete_Graph(input_dim, n_nodes, has_bias=True, is_cont_node=is_cont_node)\n",
    "# Get weighted adjacency matrix\n",
    "W = model.get_W()\n",
    "print(\"This is the weighted adjacency matrix:\\n\", W)\n",
    "print()\n",
    "print(\"The shape of the weighted adjacency matrix is: \", W.shape)\n",
    "print()\n",
    "#print(model)\n",
    "print()\n",
    "\n",
    "# Check if all nodes are frozen initially\n",
    "print(\"Initially, are all nodes frozen?:\", model.are_vodes_frozen())\n",
    "print()\n",
    "\n",
    "# Freezing all nodes\n",
    "print(\"Freezing all nodes...\")\n",
    "model.freeze_nodes(freeze=True)\n",
    "print()\n",
    "\n",
    "# Check if all nodes are frozen after freezing\n",
    "print(\"After freezing, are all nodes frozen?:\", model.are_vodes_frozen())\n",
    "print()\n",
    "\n",
    "# Unfreezing all nodes\n",
    "print(\"Unfreezing all nodes...\")\n",
    "model.freeze_nodes(freeze=False)\n",
    "print()\n",
    "\n",
    "# Check if all nodes are frozen after unfreezing\n",
    "print(\"After unfreezing, are all nodes frozen?:\", model.are_vodes_frozen())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.is_cont_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: make the below params global or input to the functions in which it is used.\n",
    "w_learning_rate = 1e-3 # Notes: 5e-1 is too high\n",
    "h_learning_rate = 1e-4\n",
    "T = 1\n",
    "\n",
    "nm_epochs = 2000 # not much happens after 2000 epochs\n",
    "every_n_epochs = 1\n",
    "batch_size = 128\n",
    "\n",
    "\n",
    "# {'fdr': 0.0667, 'tpr': 0.9333, 'fpr': 0.0196, 'shd': 1, 'nnz': 15, 'precision': 0.9333, 'recall': 0.9333, 'F1': 0.9333, 'gscore': 0.8667}\n",
    "# bs_128_lrw_0.001_lrh_0.0001_lamh_10000.0_laml1_1.0_epochs_2000 gives F1 score of 0.9333\n",
    "# NOTE THIS USES EPSILON=1e-8 FOR THE NORMALIZATION OF THE REGULARIZATION TERMS\n",
    "# lam_h = 1e4\n",
    "# lam_l1 = 1e0\n",
    "\n",
    "lam_h = 1e4\n",
    "lam_l1 = 1e0\n",
    "\n",
    "# Create a file name string for the hyperparameters\n",
    "exp_name = f\"bs_{batch_size}_lrw_{w_learning_rate}_lrh_{h_learning_rate}_lamh_{lam_h}_laml1_{lam_l1}_epochs_{nm_epochs}\"\n",
    "print(\"Name of the experiment: \", exp_name)\n",
    "\n",
    "# Training an1d evaluation functions\n",
    "@pxf.vmap(pxu.M(pxc.VodeParam | pxc.VodeParam.Cache).to((None, 0)), in_axes=(0,), out_axes=0)\n",
    "def forward(x, *, model: Complete_Graph):\n",
    "    return model(x)\n",
    "\n",
    "# @pxf.vmap(pxu.M(pxc.VodeParam | pxc.VodeParam.Cache).to((None, 0)), out_axes=(None, 0), axis_name=\"batch\") # if only one output\n",
    "@pxf.vmap(pxu.M(pxc.VodeParam | pxc.VodeParam.Cache).to((None, 0)), out_axes=(None, None, None, None, 0), axis_name=\"batch\") # if multiple outputs\n",
    "def energy(*, model: Complete_Graph):\n",
    "\n",
    "    print(\"Energy: Starting computation\")\n",
    "    x_ = model(None)\n",
    "    print(\"Energy: Got model output\")\n",
    "    \n",
    "    W = model.get_W()\n",
    "    d = model.n_nodes.get()\n",
    "    print(f\"Energy: Got W (shape: {W.shape}) and d: {d}\")\n",
    "\n",
    "    # PC energy term\n",
    "    pc_energy = jax.lax.pmean(model.energy(), axis_name=\"batch\")\n",
    "    print(f\"Energy: PC energy term: {pc_energy}\")\n",
    "\n",
    "    # L1 regularization using adjacency matrix (scaled by Frobenius norm)\n",
    "    l1_reg = jnp.sum(jnp.abs(W)) / (jnp.linalg.norm(W, ord='fro') + 1e-8)\n",
    "    #l1_reg = jnp.sum(jnp.abs(W)) / d\n",
    "    print(f\"Energy: L1 reg term: {l1_reg}\")\n",
    "\n",
    "    # DAG constraint (stable logarithmic form)\n",
    "    h_reg = (jnp.trace(jax.scipy.linalg.expm(jnp.multiply(W, W))) - d) / (jnp.sqrt(d) + 1e-8)\n",
    "    #h_reg = (jnp.trace(jax.scipy.linalg.expm(jnp.multiply(W, W))) - d) / d  # with normalization\n",
    "    print(f\"Energy: DAG constraint term: {h_reg}\")\n",
    "        \n",
    "    # Combined loss\n",
    "    obj = pc_energy + lam_h * h_reg + lam_l1 * l1_reg\n",
    "    print(f\"Energy: Final objective: {obj}\")\n",
    "\n",
    "    # Ensure obj is a scalar, not a (1,) array because JAX's grad and value_and_grad functions are designed\n",
    "    # to compute gradients of *scalar-output functions*\n",
    "    obj = obj.squeeze()  # explicitly converte the (1,) array (obj) to a scalar of shape ()  \n",
    "    \n",
    "    return obj, pc_energy, h_reg, l1_reg, x_\n",
    "\n",
    "@pxf.jit(static_argnums=0)\n",
    "def train_on_batch(T: int, x: jax.Array, *, model: Complete_Graph, optim_w: pxu.Optim, optim_h: pxu.Optim):\n",
    "    print(\"1. Starting train_on_batch\")  \n",
    "\n",
    "    model.train()\n",
    "    print(\"2. Model set to train mode\")\n",
    "\n",
    "    model.freeze_nodes(freeze=True)\n",
    "    print(\"3. Nodes frozen\")\n",
    "\n",
    "    # init step\n",
    "    with pxu.step(model, pxc.STATUS.INIT, clear_params=pxc.VodeParam.Cache):\n",
    "        print(\"4. Doing forward for initialization\")\n",
    "        forward(x, model=model)\n",
    "        print(\"5. After forward for initialization\")\n",
    "\n",
    "    \"\"\"\n",
    "    # The following code might not be needed as we are keeping the vodes frozen at all times\n",
    "    # Reinitialize the optimizer state between different batches\n",
    "    optim_h.init(pxu.M(pxc.VodeParam)(model))\n",
    "\n",
    "    for _ in range(T):\n",
    "        with pxu.step(model, clear_params=pxc.VodeParam.Cache):\n",
    "            (e, x_), g = pxf.value_and_grad(\n",
    "                pxu.M(pxu.m(pxc.VodeParam).has_not(frozen=True), [False, True]),\n",
    "                has_aux=True\n",
    "            )(energy)(model=model)\n",
    "        optim_h.step(model, g[\"model\"], True)\n",
    "    \"\"\"\n",
    "\n",
    "    with pxu.step(model, clear_params=pxc.VodeParam.Cache):\n",
    "        print(\"6. Before computing gradients\")\n",
    "        (obj, (pc_energy, h_reg, l1_reg, x_)), g = pxf.value_and_grad(\n",
    "            pxu.M(pxnn.LayerParam).to([False, True]), \n",
    "            has_aux=True\n",
    "        )(energy)(model=model) # pxf.value_and_grad returns a tuple structured as ((value, aux), grad), not as six separate outputs.\n",
    "        \n",
    "        print(\"7. After computing gradients\")\n",
    "        #print(\"Gradient structure:\", g)\n",
    "\n",
    "        print(\"8. Before zeroing out the diagonal gradients\")\n",
    "        # Zero out the diagonal gradients using jax.numpy.fill_diagonal\n",
    "        weight_grads = g[\"model\"].layers[0].nn.weight.get()\n",
    "        weight_grads = jax.numpy.fill_diagonal(weight_grads, 0.0, inplace=False)\n",
    "        # print the grad values using the syntax jax.debug.print(\"🤯 {x} 🤯\", x=x)\n",
    "        #jax.debug.print(\"{weight_grads}\", weight_grads=weight_grads)\n",
    "        g[\"model\"].layers[0].nn.weight.set(weight_grads)\n",
    "        print(\"9. After zeroing out the diagonal gradients\")\n",
    "\n",
    "        \n",
    "    print(\"10. Before optimizer step\")\n",
    "    optim_w.step(model, g[\"model\"])\n",
    "    #optim_w.step(model, g[\"model\"], scale_by=1.0/x.shape[0])\n",
    "    print(\"11. After optimizer step\")\n",
    "\n",
    "    with pxu.step(model, clear_params=pxc.VodeParam.Cache):\n",
    "        print(\"12. Before final forward\")\n",
    "        forward(None, model=model)\n",
    "        e_avg_per_sample = model.energy()\n",
    "        print(\"13. After final forward\")\n",
    "\n",
    "    model.freeze_nodes(freeze=False)\n",
    "    print(\"14. Nodes unfrozen\")\n",
    "\n",
    "    return pc_energy, l1_reg, h_reg, obj\n",
    "\n",
    "def train(dl, T, *, model: Complete_Graph, optim_w: pxu.Optim, optim_h: pxu.Optim):\n",
    "    batch_pc_energies = []\n",
    "    batch_l1_regs = []\n",
    "    batch_h_regs = []\n",
    "    batch_objs = []\n",
    "    \n",
    "    for batch in dl:\n",
    "        pc_energy, l1_reg, h_reg, obj = train_on_batch(\n",
    "            T, batch, model=model, optim_w=optim_w, optim_h=optim_h\n",
    "        )\n",
    "        batch_pc_energies.append(pc_energy)\n",
    "        batch_l1_regs.append(l1_reg)\n",
    "        batch_h_regs.append(h_reg)\n",
    "        batch_objs.append(obj)\n",
    "\n",
    "    W = model.get_W()\n",
    "\n",
    "    # Compute epoch averages\n",
    "    epoch_pc_energy = jnp.mean(jnp.array(batch_pc_energies))\n",
    "    epoch_l1_reg = jnp.mean(jnp.array(batch_l1_regs))\n",
    "    epoch_h_reg = jnp.mean(jnp.array(batch_h_regs))\n",
    "    epoch_obj = jnp.mean(jnp.array(batch_objs))\n",
    "    \n",
    "    return W, epoch_pc_energy, epoch_l1_reg, epoch_h_reg, epoch_obj\n",
    "\n",
    "# %%\n",
    "# for reference compute the MAE, SID, and SHD between the true adjacency matrix and an all-zero matrix and then print it\n",
    "# this acts as a baseline for the MAE, SID, and SHD similar to how 1/K accuracy acts as a baseline for classification tasks where K is the number of classes\n",
    "\n",
    "W_zero = np.zeros_like(W_true)\n",
    "print(\"MAE between the true adjacency matrix and an all-zero matrix: \", MAE(W_true, W_zero))\n",
    "print(\"SHD between the true adjacency matrix and an all-zero matrix: \", SHD(B_true, compute_binary_adjacency(W_zero)))\n",
    "#print(\"SID between the true adjacency matrix and an all-zero matrix: \", SID(W_true, W_zero))\n",
    "\n",
    "# %%\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X):\n",
    "        self.X = X\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx]\n",
    "\n",
    "# This is a simple collate function that stacks numpy arrays used to interface\n",
    "# the PyTorch dataloader with JAX. In the future we hope to provide custom dataloaders\n",
    "# that are independent of PyTorch.\n",
    "def numpy_collate(batch):\n",
    "    if isinstance(batch[0], np.ndarray):\n",
    "        return np.stack(batch)\n",
    "    elif isinstance(batch[0], (tuple, list)):\n",
    "        transposed = zip(*batch)\n",
    "        return [numpy_collate(samples) for samples in transposed]\n",
    "    else:\n",
    "        return np.array(batch)\n",
    "\n",
    "# The dataloader assumes cuda is being used, as such it sets 'pin_memory = True' and\n",
    "# 'prefetch_factor = 2'. Note that the batch size should be constant during training, so\n",
    "# we set 'drop_last = True' to avoid having to deal with variable batch sizes.\n",
    "class TorchDataloader(torch.utils.data.DataLoader):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset,\n",
    "        batch_size=1,\n",
    "        shuffle=None,\n",
    "        sampler=None,\n",
    "        batch_sampler=None,\n",
    "        num_workers=1,\n",
    "        pin_memory=True,\n",
    "        timeout=0,\n",
    "        worker_init_fn=None,\n",
    "        persistent_workers=True,\n",
    "        prefetch_factor=2,\n",
    "    ):\n",
    "        super(self.__class__, self).__init__(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            sampler=sampler,\n",
    "            batch_sampler=batch_sampler,\n",
    "            num_workers=num_workers,\n",
    "            collate_fn=numpy_collate,\n",
    "            pin_memory=pin_memory,\n",
    "            drop_last=True if batch_sampler is None else None,\n",
    "            timeout=timeout,\n",
    "            worker_init_fn=worker_init_fn,\n",
    "            persistent_workers=persistent_workers,\n",
    "            prefetch_factor=prefetch_factor,\n",
    "        )\n",
    "\n",
    "\n",
    "# Create the custom dataset\n",
    "dataset = CustomDataset(X)\n",
    "\n",
    "# Create the custom dataset with standardized data\n",
    "#dataset_std = CustomDataset(X_std)\n",
    "\n",
    "# Create the dataloader\n",
    "dl = TorchDataloader(dataset, batch_size=batch_size, shuffle=True)\n",
    "######## OR ########\n",
    "#dl = TorchDataloader(dataset_std, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# %%\n",
    "# Initialize the model and optimizers\n",
    "with pxu.step(model, pxc.STATUS.INIT, clear_params=pxc.VodeParam.Cache):\n",
    "    forward(jnp.zeros((batch_size, model.n_nodes.get())), model=model)\n",
    "    optim_h = pxu.Optim(lambda: optax.sgd(h_learning_rate))\n",
    "\n",
    "    \"\"\"\n",
    "    optim_w = pxu.Optim(\n",
    "    optax.chain(\n",
    "        optax.clip_by_global_norm(clip_value),  # Clip gradients by global norm\n",
    "        optax.sgd(w_learning_rate)  # Apply SGD optimizer\n",
    "    ),\n",
    "    pxu.M(pxnn.LayerParam)(model)  # Masking the parameters of the model\n",
    ")\n",
    "    \"\"\"\n",
    "    #optim_w = pxu.Optim(lambda: optax.adam(w_learning_rate), pxu.M(pxnn.LayerParam)(model))\n",
    "    optim_w = pxu.Optim(lambda: optax.adamw(w_learning_rate, nesterov=True), pxu.M(pxnn.LayerParam)(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store differences and energies\n",
    "MAEs = []\n",
    "SHDs = []\n",
    "F1s = []\n",
    "pc_energies = []\n",
    "l1_regs = []\n",
    "h_regs = []\n",
    "objs = []\n",
    "\n",
    "# Calculate the initial MAE, SID, and SHD\n",
    "\n",
    "W_init = model.get_W()\n",
    "B_init = compute_binary_adjacency(W_init)\n",
    "\n",
    "MAE_init = MAE(W_true, W_init)\n",
    "print(f\"Start difference (cont.) between W_true and W_init: {MAE_init:.4f}\")\n",
    "\n",
    "SHD_init = SHD(B_true, B_init)\n",
    "print(f\"Start SHD between B_true and B_init: {SHD_init:.4f}\")\n",
    "\n",
    "F1_init = compute_F1_directed(B_true, B_init)\n",
    "print(f\"Start F1 between B_true and B_init: {F1_init:.4f}\")\n",
    "\n",
    "# print the values of the diagonal of the initial W\n",
    "print(\"The diagonal of the initial W: \", jnp.diag(W_init))\n",
    "\n",
    "# Start timing\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "# Training loop\n",
    "with tqdm(range(nm_epochs), position=0, leave=True) as pbar:\n",
    "    for epoch in pbar:\n",
    "        # Train for one epoch using the dataloader\n",
    "        W, epoch_pc_energy, epoch_l1_reg, epoch_h_reg, epoch_obj = train(dl, T=T, model=model, optim_w=optim_w, optim_h=optim_h)\n",
    "        \n",
    "        # Extract the weighted adjacency matrix W and compute the binary adjacency matrix B\n",
    "        W = np.array(W)\n",
    "        B = compute_binary_adjacency(W)\n",
    "\n",
    "        # Compute metrics every n epochs\n",
    "        if (epoch + 1) % every_n_epochs == 0 or epoch == 0:\n",
    "            MAEs.append(float(MAE(W_true, W)))\n",
    "            SHDs.append(float(SHD(B_true, compute_binary_adjacency(W), double_for_anticausal=False)))\n",
    "            F1s.append(float(compute_F1_directed(B_true, B)))\n",
    "            pc_energies.append(float(epoch_pc_energy))\n",
    "            l1_regs.append(float(epoch_l1_reg))\n",
    "            epoch_h_reg_raw = compute_h_reg(W)\n",
    "            h_regs.append(float(epoch_h_reg_raw))\n",
    "            objs.append(float(epoch_obj))\n",
    "\n",
    "            # Update progress bar with the current status\n",
    "            pbar.set_description(f\"MAE: {MAEs[-1]:.4f}, F1: {F1s[-1]:.4f}, SHD: {SHDs[-1]:.4f} || PC Energy: {pc_energies[-1]:.4f}, L1 Reg: {l1_regs[-1]:.4f}, H Reg: {h_regs[-1]:.4f}, Obj: {objs[-1]:.4f}\")\n",
    "\n",
    "\n",
    "# End timing\n",
    "end_time = timeit.default_timer()\n",
    "\n",
    "# Print the average time per epoch\n",
    "average_time_per_epoch = (end_time - start_time) / nm_epochs\n",
    "print(f\"An epoch (with compiling and testing) took on average: {average_time_per_epoch:.4f} seconds\")\n",
    "# print the values of the diagonal of the final W\n",
    "print(\"The diagonal of the final W: \", jnp.diag(model.get_W()))\n",
    "\n",
    "\n",
    "# print in big that training is done\n",
    "print(\"\\n\\n ###########################  Training is done  ########################### \\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define experiment name\n",
    "exp_name = f\"bs_{batch_size}_lrw_{w_learning_rate}_lrh_{h_learning_rate}_lamh_{lam_h}_laml1_{lam_l1}_epochs_{nm_epochs}\"\n",
    "\n",
    "# Create subdirectory in linear folder with the name stored in exp_name\n",
    "save_path = os.path.join('plots/linear_mixed_disc_x0', exp_name)\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "# Reset to default style and set seaborn style\n",
    "plt.style.use('default')\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Update matplotlib parameters\n",
    "plt.rcParams.update({\n",
    "    'figure.dpi': 300,\n",
    "    'savefig.dpi': 300,\n",
    "    'font.size': 12,\n",
    "    'axes.titlesize': 14,\n",
    "    'axes.labelsize': 12,\n",
    "    'xtick.labelsize': 10,\n",
    "    'ytick.labelsize': 10,\n",
    "    'legend.fontsize': 10,\n",
    "    'figure.titlesize': 16,\n",
    "    'axes.grid': True,\n",
    "    'grid.alpha': 0.3,\n",
    "    'axes.spines.top': False,\n",
    "    'axes.spines.right': False\n",
    "})\n",
    "\n",
    "# Create a figure and subplots using GridSpec\n",
    "fig = plt.figure(figsize=(20, 12))\n",
    "gs = fig.add_gridspec(2, 4, height_ratios=[1, 1], width_ratios=[1, 1, 1, 1])\n",
    "\n",
    "# Adjust layout to make more room for title and subtitle\n",
    "plt.subplots_adjust(top=0.85, hspace=0.4, wspace=0.3)\n",
    "\n",
    "# Create axes\n",
    "ax00 = fig.add_subplot(gs[0, 0])\n",
    "ax01 = fig.add_subplot(gs[0, 1])\n",
    "ax02 = fig.add_subplot(gs[0, 2])\n",
    "ax03 = fig.add_subplot(gs[0, 3])\n",
    "ax10 = fig.add_subplot(gs[1, 0])\n",
    "ax11 = fig.add_subplot(gs[1, 1])\n",
    "ax12 = fig.add_subplot(gs[1, 2:])\n",
    "# ax12 spans two columns\n",
    "\n",
    "axes = [ax00, ax01, ax02, ax03, ax10, ax11, ax12]\n",
    "\n",
    "# Plot configurations\n",
    "plot_configs = [\n",
    "    {'metric': MAEs, 'title': 'Mean Absolute Error', 'ylabel': 'MAE', 'color': '#2ecc71', 'ax': ax00},\n",
    "    {'metric': SHDs, 'title': 'Structural Hamming Distance', 'ylabel': 'SHD', 'color': '#e74c3c', 'ax': ax01},\n",
    "    {'metric': F1s, 'title': 'F1 Score', 'ylabel': 'F1', 'color': '#3498db', 'ax': ax02},\n",
    "    {'metric': pc_energies, 'title': 'PC Energy', 'ylabel': 'Energy', 'color': '#9b59b6', 'ax': ax03},\n",
    "    {'metric': l1_regs, 'title': 'L1 Regularization', 'ylabel': 'L1', 'color': '#f1c40f', 'ax': ax10},\n",
    "    {'metric': h_regs, 'title': 'DAG Constraint', 'ylabel': 'h(W)', 'color': '#e67e22', 'ax': ax11},\n",
    "    {'metric': objs, 'title': 'Total Objective', 'ylabel': 'Loss', 'color': '#1abc9c', 'ax': ax12}\n",
    "]\n",
    "\n",
    "# Create all subplots\n",
    "for config in plot_configs:\n",
    "    ax = config['ax']\n",
    "    \n",
    "    # Plot data with rolling average\n",
    "    epochs = range(len(config['metric']))\n",
    "    \n",
    "    # Determine if we should use log scale and/or scaling factor\n",
    "    use_log_scale = config['title'] in ['PC Energy', 'Total Objective']\n",
    "    scale_factor = 1e4 if config['title'] == 'DAG Constraint' else 1\n",
    "    \n",
    "    # Apply scaling and/or log transform to the metric\n",
    "    metric_values = np.array(config['metric'])\n",
    "    if use_log_scale:\n",
    "        # Add small constant to avoid log(0)\n",
    "        metric_values = np.log10(np.abs(metric_values) + 1e-10)\n",
    "    metric_values = metric_values * scale_factor\n",
    "    \n",
    "    # Plot raw data\n",
    "    raw_line = ax.plot(epochs, metric_values, \n",
    "                      alpha=0.3, \n",
    "                      color=config['color'], \n",
    "                      label='Raw')\n",
    "    \n",
    "    # Calculate and plot rolling average\n",
    "    window_size = 50\n",
    "    if len(metric_values) > window_size:\n",
    "        rolling_mean = np.convolve(metric_values, \n",
    "                                 np.ones(window_size)/window_size, \n",
    "                                 mode='valid')\n",
    "        ax.plot(range(window_size-1, len(metric_values)), \n",
    "                rolling_mean, \n",
    "                color=config['color'], \n",
    "                linewidth=2, \n",
    "                label='Moving Average')\n",
    "    \n",
    "    # Customize each subplot\n",
    "    ax.set_title(config['title'], pad=10)\n",
    "    ax.set_xlabel('Epoch', labelpad=10)\n",
    "    \n",
    "    # Adjust ylabel based on transformations\n",
    "    ylabel = config['ylabel']\n",
    "    if use_log_scale:\n",
    "        ylabel = f'log10({ylabel})'\n",
    "    if scale_factor != 1:\n",
    "        ylabel = f'{ylabel} (×{int(scale_factor)})'\n",
    "    ax.set_ylabel(ylabel, labelpad=10)\n",
    "    \n",
    "    ax.grid(True, linestyle='--', alpha=0.7)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    \n",
    "    # Add legend if it's the total objective plot\n",
    "    if config['title'] == 'Total Objective':\n",
    "        ax.legend(loc='upper right')\n",
    "        \n",
    "    # Add note about scaling if applicable\n",
    "    if use_log_scale or scale_factor != 1:\n",
    "        transform_text = []\n",
    "        if use_log_scale:\n",
    "            transform_text.append('log scale')\n",
    "        if scale_factor != 1:\n",
    "            transform_text.append(f'×{int(scale_factor)}')\n",
    "        ax.text(0.02, 0.98, f\"({', '.join(transform_text)})\", \n",
    "                transform=ax.transAxes, \n",
    "                fontsize=8, \n",
    "                verticalalignment='top',\n",
    "                bbox=dict(facecolor='white', alpha=0.8, edgecolor='none'))\n",
    "\n",
    "# Add overall title and subtitle with adjusted positions\n",
    "fig.suptitle('Training Metrics Over Time', \n",
    "            fontsize=16, \n",
    "            weight='bold', \n",
    "            y=0.98)\n",
    "\n",
    "subtitle = f'λ_h = {lam_h}, λ_l1 = {lam_l1}, Weights Learning Rate = {w_learning_rate}'\n",
    "fig.text(0.5, 0.93, \n",
    "         subtitle, \n",
    "         horizontalalignment='center',\n",
    "         fontsize=12,\n",
    "         style='italic')\n",
    "\n",
    "# Save and show the figure as a .pdf file at the specified location\n",
    "plt.savefig(os.path.join(save_path, 'training_metrics.pdf'), \n",
    "            bbox_inches='tight', \n",
    "            dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Create a separate figure for the adjacency matrices comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Use a better colormap - options:\n",
    "# 'YlOrBr' - Yellow-Orange-Brown (good for sparse matrices)\n",
    "# 'viridis' - Perceptually uniform, colorblind-friendly\n",
    "# 'Greys' - Black and white, professional\n",
    "# 'YlGnBu' - Yellow-Green-Blue, professional\n",
    "cmap = 'YlOrBr'  # Choose one of the above\n",
    "\n",
    "# Plot estimated adjacency matrix (now on the left)\n",
    "im1 = ax1.imshow(compute_binary_adjacency(W), cmap=cmap, interpolation='nearest')\n",
    "ax1.set_title('Estimated DAG', pad=10)\n",
    "plt.colorbar(im1, ax=ax1, fraction=0.046, pad=0.04)\n",
    "ax1.set_xlabel('Node', labelpad=10)\n",
    "ax1.set_ylabel('Node', labelpad=10)\n",
    "\n",
    "# Plot true adjacency matrix (now on the right)\n",
    "im2 = ax2.imshow(B_true, cmap=cmap, interpolation='nearest')\n",
    "ax2.set_title('True DAG', pad=10)\n",
    "plt.colorbar(im2, ax=ax2, fraction=0.046, pad=0.04)\n",
    "ax2.set_xlabel('Node', labelpad=10)\n",
    "ax2.set_ylabel('Node', labelpad=10)\n",
    "\n",
    "# Add overall title\n",
    "fig.suptitle('Estimated vs True DAG Structure', \n",
    "             fontsize=16, \n",
    "             weight='bold', \n",
    "             y=1.05)\n",
    "\n",
    "# Add grid lines to better separate the nodes\n",
    "for ax in [ax1, ax2]:\n",
    "    ax.set_xticks(np.arange(-.5, B_true.shape[0], 1), minor=True)\n",
    "    ax.set_yticks(np.arange(-.5, B_true.shape[0], 1), minor=True)\n",
    "    ax.grid(which=\"minor\", color=\"w\", linestyle='-', linewidth=0.3)\n",
    "    ax.tick_params(which=\"minor\", bottom=False, left=False)\n",
    "\n",
    "# Improve layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the comparison plot as a .pdf file at the specified location\n",
    "plt.savefig(os.path.join(save_path, 'dag_comparison.pdf'), \n",
    "            bbox_inches='tight', \n",
    "            dpi=300,\n",
    "            facecolor='white',\n",
    "            edgecolor='none')\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# Now use a threshold of 0.3 to binarize the weighted adjacency matrix W\n",
    "W_est = np.array(model.get_W())\n",
    "B_est = compute_binary_adjacency(W_est, threshold=0.3)\n",
    "\n",
    "# %%\n",
    "# Check if B_est is indeed a DAG\n",
    "def is_dag(adjacency_matrix):\n",
    "    \"\"\"\n",
    "    Check if a given adjacency matrix represents a Directed Acyclic Graph (DAG).\n",
    "    \n",
    "    Parameters:\n",
    "        adjacency_matrix (numpy.ndarray): A square matrix representing the adjacency of a directed graph.\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if the graph is a DAG, False otherwise.\n",
    "    \"\"\"\n",
    "    # Create a directed graph from the adjacency matrix\n",
    "    graph = nx.DiGraph(adjacency_matrix)\n",
    "    \n",
    "    # Check if the graph is a DAG\n",
    "    return nx.is_directed_acyclic_graph(graph)\n",
    "\n",
    "# Check if the estimated binary adjacency matrix B_est is a DAG\n",
    "is_dag_B_est = is_dag(B_est)\n",
    "print(f\"Is the estimated binary adjacency matrix a DAG? {is_dag_B_est}\")\n",
    "\n",
    "\n",
    "# Compute the h_reg term for the true weighted adjacency matrix W_true\n",
    "h_reg_true = compute_h_reg(W_true)\n",
    "print(f\"The h_reg term for the true weighted adjacency matrix W_true is: {h_reg_true:.4f}\")\n",
    "\n",
    "# Compute the h_reg term for the estimated weighted adjacency matrix W_est\n",
    "h_reg_est = compute_h_reg(W_est)\n",
    "print(f\"The h_reg term for the estimated weighted adjacency matrix W_est is: {h_reg_est:.4f}\")\n",
    "\n",
    "# print first 5 rows and columsn of W_est and round values to 4 decimal places and show as non-scientific notation\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "print(\"The first 5 rows and columns of the estimated weighted adjacency matrix W_est\\n{}\".format(W_est[:5, :5]))\n",
    "\n",
    "# now show the adjacency matrix of the true graph and the estimated graph side by side\n",
    "plot_adjacency_matrices(true_matrix=B_true, est_matrix=B_est, save_path=os.path.join(save_path, 'adjacency_matrices.png'))\n",
    "\n",
    "# print the number of edges in the true graph and the estimated graph\n",
    "print(f\"The number of edges in the true graph: {np.sum(B_true)}\")\n",
    "print(f\"The number of edges in the estimated graph: {np.sum(B_est)}\")\n",
    "\n",
    "# %%\n",
    "# plot est_dag and true_dag\n",
    "GraphDAG(B_est, B_true, save_name=os.path.join(save_path, 'est_dag_true_dag.png'))\n",
    "# calculate accuracy\n",
    "met_pcx = MetricsDAG(B_est, B_true)\n",
    "print(met_pcx.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the style and color palette\n",
    "sns.set(style=\"whitegrid\")\n",
    "palette = sns.color_palette(\"tab10\")\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(18, 5))  # Adjusting layout to 1 row and 3 columns\n",
    "fig.suptitle('Performance Metrics Over Epochs', fontsize=16, weight='bold')\n",
    "\n",
    "# Plot the MAE\n",
    "sns.lineplot(x=range(len(MAEs)), y=MAEs, ax=axs[0], color=palette[0])\n",
    "axs[0].set_title(\"Mean Absolute Error (MAE)\", fontsize=14)\n",
    "axs[0].set_xlabel(\"Epoch\", fontsize=12)\n",
    "axs[0].set_ylabel(\"MAE\", fontsize=12)\n",
    "axs[0].grid(True)\n",
    "\n",
    "# Plot the SHD\n",
    "sns.lineplot(x=range(len(SHDs)), y=SHDs, ax=axs[1], color=palette[2])\n",
    "axs[1].set_title(\"Structural Hamming Distance (SHD)\", fontsize=14)\n",
    "axs[1].set_xlabel(\"Epoch\", fontsize=12)\n",
    "axs[1].set_ylabel(\"SHD\", fontsize=12)\n",
    "axs[1].grid(True)\n",
    "\n",
    "# Plot the Energy\n",
    "sns.lineplot(x=range(len(pc_energies)), y=pc_energies, ax=axs[2], color=palette[3])\n",
    "axs[2].set_title(\"Energy\", fontsize=14)\n",
    "axs[2].set_xlabel(\"Epoch\", fontsize=12)\n",
    "axs[2].set_ylabel(\"Energy\", fontsize=12)\n",
    "axs[2].grid(True)\n",
    "\n",
    "# Improve layout and show the plot\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjust layout to fit the suptitle\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print first 5 rows and columsn of W_est and round values to 4 decimal places and show as non-scientific notation\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "print(\"The first 4 rows and columns of the estimated weighted adjacency matrix W_est\\n{}\".format(W_est[:11, :11]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now show the adjacency matrix of the true graph and the estimated graph side by side\n",
    "plot_adjacency_matrices(B_true, B_est)\n",
    "\n",
    "# print the number of edges in the true graph and the estimated graph\n",
    "print(f\"The number of edges in the true graph: {np.sum(B_true)}\")\n",
    "print(f\"The number of edges in the estimated graph: {np.sum(B_est)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot est_dag and true_dag\n",
    "GraphDAG(B_est, B_true)\n",
    "# calculate accuracy\n",
    "met_pcax = MetricsDAG(B_est, B_true)\n",
    "print(met_pcax.metrics)\n",
    "\n",
    "# print experiment name\n",
    "print(exp_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from castle.common import GraphDAG\n",
    "from castle.metrics import MetricsDAG\n",
    "from castle.datasets import DAG, IIDSimulation\n",
    "from castle.algorithms import GOLEM\n",
    "\n",
    "# GOLEM learn\n",
    "#g = GOLEM(num_iter=2e4, lambda_2=1e4/X.shape[1], lambda_1=1e-2)\n",
    "#g = GOLEM(num_iter=2e4, lambda_2=0.0)  setting h_reg to 0 still allows model to be fit (no error thrown)\n",
    "g = GOLEM(num_iter=2e4, non_equal_variances=True) # F1 of 68%\n",
    "#g = GOLEM(num_iter=2e4, non_equal_variances=False) # F1 of 68%, default non_equal_variances=True\n",
    "g.learn(X)\n",
    "\n",
    "# plot est_dag and true_dag\n",
    "GraphDAG(g.causal_matrix, B_true)\n",
    "\n",
    "# calculate accuracy\n",
    "g_met = MetricsDAG(g.causal_matrix, B_true)\n",
    "print(g_met.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from castle.algorithms import Notears\n",
    "\n",
    "# notears learn\n",
    "nt = Notears(lambda1=1e-2, h_tol=1e-5, max_iter=5000) # default loss_type is 'l2', F1 of 74%\n",
    "#nt = Notears(lambda1=1e-2, h_tol=1e-5, max_iter=10000, loss_type='logistic')\n",
    "#nt = Notears(lambda1=1e-2, h_tol=1e-5, max_iter=10000, loss_type='poisson')\n",
    "nt.learn(X)\n",
    "\n",
    "# plot est_dag and true_dag\n",
    "GraphDAG(nt.causal_matrix, B_true)\n",
    "\n",
    "# calculate accuracy\n",
    "nt_met = MetricsDAG(nt.causal_matrix, B_true)\n",
    "print(nt_met.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from castle.algorithms import PC\n",
    "\n",
    "# A variant of PC-algorithm, one of [`original`, `stable`, `parallel`]\n",
    "pc = PC(variant='parallel', alpha=0.03, ci_test='fisherz') # F1 of 74%\n",
    "#pc = PC(variant='stable', alpha=0.03) # F1 of 69%\n",
    "#pc = PC(variant='original', alpha=0.03) # F1 of 58%\n",
    "pc.learn(X)\n",
    "\n",
    "# plot est_dag and true_dag\n",
    "GraphDAG(pc.causal_matrix, B_true)\n",
    "\n",
    "# calculate accuracy\n",
    "pc_met = MetricsDAG(pc.causal_matrix, B_true)\n",
    "print(pc_met.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from castle.algorithms.ges.ges import GES\n",
    "\n",
    "# GES learn\n",
    "# method is one of ['r2', 'scatter']\n",
    "ges = GES() # F1 of 47%\n",
    "#ges = GES(criterion='bic', method='scatter') # F1 of 47%\n",
    "#ges = GES(criterion='bic', method='r2') # F1 of 4%\n",
    "\n",
    "# learn the graph structure\n",
    "ges.learn(X)\n",
    "\n",
    "# plot est_dag and true_dag\n",
    "GraphDAG(ges.causal_matrix, B_true)\n",
    "\n",
    "# calculate accuracy\n",
    "ges_met = MetricsDAG(ges.causal_matrix, B_true)\n",
    "print(ges_met.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from castle.algorithms import DirectLiNGAM\n",
    "\n",
    "# measure : {'pwling', 'kernel'}, default='pwling'\n",
    "g_dlingam = DirectLiNGAM(measure='pwling') # F1 of 74%\n",
    "#g = DirectLiNGAM(measure='kernel') # F1 of nan - takes too long to run (even after 15 minutes not done)\n",
    "g_dlingam.learn(X)\n",
    "\n",
    "# plot est_dag and true_dag\n",
    "GraphDAG(g_dlingam.causal_matrix, B_true)\n",
    "\n",
    "# calculate accuracy\n",
    "g_dlingam_met = MetricsDAG(g_dlingam.causal_matrix, B_true)\n",
    "print(g_dlingam_met.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from castle.algorithms import ICALiNGAM\n",
    "\n",
    "# ICALiNGAM learn\n",
    "# max_iter : int, optional (default=1000)\n",
    "#g = ICALiNGAM(max_iter=1000) # F1 of 48%\n",
    "g_ICAlingam = ICALiNGAM(max_iter=100000) # F1 of 51%\n",
    "g_ICAlingam.learn(X)\n",
    "\n",
    "# plot est_dag and true_dag\n",
    "GraphDAG(g_ICAlingam.causal_matrix, B_true)\n",
    "\n",
    "# calculate accuracy\n",
    "g_ICAlingam_met = MetricsDAG(g_ICAlingam.causal_matrix, B_true)\n",
    "print(g_ICAlingam_met.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now use lingam to estimate total effect\n",
    "import lingam\n",
    "\n",
    "class CustomLingamObject(lingam.DirectLiNGAM):\n",
    "    def set_adjacency_matrix(self, W):\n",
    "        self._adjacency_matrix = W  # Manually set the adjacency matrix\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "def compute_causal_order(adjacency_matrix):\n",
    "    \"\"\"\n",
    "    Compute the causal order (topological order) of nodes from an adjacency matrix.\n",
    "\n",
    "    Parameters:\n",
    "        adjacency_matrix (np.ndarray): Adjacency matrix of the DAG.\n",
    "\n",
    "    Returns:\n",
    "        list: List of nodes in causal order (from roots to leaves).\n",
    "    \"\"\"\n",
    "    # Convert the adjacency matrix to a directed graph\n",
    "    G = nx.DiGraph(adjacency_matrix)  # Do NOT transpose the matrix\n",
    "    return list(nx.topological_sort(G))\n",
    "\n",
    "# Example usage\n",
    "# Compute the causal order for the adjacency matrix from the ICA-LiNGAM model\n",
    "lingam_true_object = CustomLingamObject()\n",
    "lingam_true_object.set_adjacency_matrix(W_true)\n",
    "\n",
    "# Compute topological causal order\n",
    "true_causal_order = compute_causal_order(lingam_true_object.adjacency_matrix_)\n",
    "\n",
    "# Set the true causal order in the true lingam object\n",
    "lingam_true_object._causal_order = true_causal_order\n",
    "\n",
    "true_total_effect = lingam_true_object.estimate_total_effect(X=X, from_index=0, to_index=11)\n",
    "print(\"The true total effect from node 0 to node 11 is: \", true_total_effect)\n",
    "\n",
    "# # Create an instance of the custom class\n",
    "# lingam_object_true = CustomLingamObject()\n",
    "# # Set the custom adjacency matrix \n",
    "# lingam_object_true.set_adjacency_matrix(W_true)\n",
    "# true_total_effect = lingam_object_true.estimate_total_effect(X=X, from_index=0, to_index=1)\n",
    "\n",
    "# # our method\n",
    "# lingam_object_our = CustomLingamObject()\n",
    "# lingam_object_our.set_adjacency_matrix(W_est)\n",
    "# total_effect_matrix_est_our = lingam_object_our.estimate_total_effect(X=X, from_index=0, to_index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################### EVALUATION METRICS #####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from causal_metrics import compute_SHD, compute_SID, compute_ancestor_AID, compute_TEE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indices for treatment (intervention) and target variables\n",
    "interv_node_idx = 0\n",
    "target_node_idx = 11\n",
    "\n",
    "# Compute Total Effect Estimation Error (TEE)\n",
    "true_total_effect, est_total_effect, TEE = compute_TEE(X, B_true, B_est, interv_node_idx, target_node_idx)\n",
    "print(f\"True Total Effect: {true_total_effect}\")\n",
    "print(f\"Estimated Total Effect: {est_total_effect}\")\n",
    "print(f\"Total Effect Estimation Error (TEE): {TEE}\")\n",
    "\n",
    "# Convert adjacency matrices to binary (int8) for compatibility with `gadjid` metrics\n",
    "B_true_binary = (B_true > 0).astype(np.int8)\n",
    "B_est_binary = (B_est > 0).astype(np.int8)\n",
    "\n",
    "# Compute Structural Hamming Distance (SHD)\n",
    "SHD_normalized, SHD_absolute = compute_SHD(B_true_binary, B_est_binary)\n",
    "print(f\"Structural Hamming Distance (SHD): {SHD_absolute}\")\n",
    "print(f\"Normalized SHD: {SHD_normalized}\")\n",
    "\n",
    "# Compute Parent Adjustment Distance (SID)\n",
    "SID_normalized, SID_absolute = compute_SID(B_true_binary, B_est_binary)\n",
    "print(f\"Structural Intervention Distance (SID): {SID_absolute}\")\n",
    "print(f\"Normalized SID: {SID_normalized}\")\n",
    "\n",
    "# Compute Ancestor Adjustment Distance (AID)\n",
    "AID_normalized, AID_absolute = compute_ancestor_AID(B_true_binary, B_est_binary)\n",
    "print(f\"Ancestor Adjustment Distance (AID): {AID_absolute}\")\n",
    "print(f\"Normalized AID: {AID_normalized}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pcax24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
