{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# choose the GPU to use\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "# disable preallocation of memory\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\n",
    "\n",
    "# pcx\n",
    "import pcx as px\n",
    "import pcx.predictive_coding as pxc\n",
    "import pcx.nn as pxnn\n",
    "import pcx.functional as pxf\n",
    "import pcx.utils as pxu\n",
    "\n",
    "# 3rd party\n",
    "import jax\n",
    "from jax import jit\n",
    "import jax.numpy as jnp\n",
    "import jax.numpy.linalg as jax_numpy_linalg # for expm()\n",
    "import jax.scipy.linalg as jax_scipy_linalg # for slogdet()\n",
    "\n",
    "import jax.random as random\n",
    "import optax\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.api.types import is_float_dtype\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import timeit\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# own\n",
    "import causal_helpers\n",
    "from causal_helpers import simulate_dag, simulate_parameter, simulate_linear_sem, simulate_linear_sem_cyclic\n",
    "from causal_helpers import load_adjacency_matrix, set_random_seed, plot_adjacency_matrices\n",
    "from causal_helpers import load_graph, load_adjacency_matrix\n",
    "from causal_metrics import compute_F1_directed, compute_F1_skeleton, compute_AUPRC, compute_AUROC, compute_cycle_F1\n",
    "from causal_metrics import compute_SHD, compute_SID, compute_ancestor_AID, compute_TEE\n",
    "from connectome_cyclic_data_generator import sample_cyclic_data\n",
    "\n",
    "# Set random seed\n",
    "seed = 1 # main seed for reproducibility\n",
    "set_random_seed(seed)\n",
    "\n",
    "# causal libraries\n",
    "import cdt, castle\n",
    "from castle.algorithms import GOLEM\n",
    "from castle.algorithms import Notears\n",
    "from castle.algorithms import PC\n",
    "from castle.algorithms.ges.ges import GES\n",
    "from castle.algorithms import DirectLiNGAM\n",
    "from castle.algorithms import ICALiNGAM\n",
    "\n",
    "import lingam\n",
    "\n",
    "# causal metrics\n",
    "from cdt.metrics import precision_recall, SHD, SID\n",
    "from castle.metrics import MetricsDAG\n",
    "from castle.common import GraphDAG\n",
    "from causallearn.graph.SHD import SHD as SHD_causallearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#   # fetch dataset \n",
    "# abalone = fetch_ucirepo(id=1) \n",
    "  \n",
    "# # data (as pandas dataframes) \n",
    "# X = abalone.data.features\n",
    "# y = abalone.data.targets\n",
    "  \n",
    "# # metadata \n",
    "# #print(abalone.metadata) \n",
    "\n",
    "# # now merge X and y to create a single dataframe and give the columns the correct names using abalone.variables.name.values\n",
    "# df = pd.concat([X, y], axis=1)\n",
    "# df.columns = abalone.variables.name.values.tolist()\n",
    "# print(df.head())\n",
    "# print()\n",
    "# # show # of unique values in each column\n",
    "# print(df.nunique())\n",
    "# # finally convert the Rings variable to a binary variable by setting the threshold to mean(rings)\n",
    "# df['Rings'] = df['Rings'] > df['Rings'].mean()\n",
    "# # then convert to integer\n",
    "# df['Rings'] = df['Rings'].astype(int)\n",
    "# # also replace the values in Sex with integers\n",
    "# df['Sex'] = df['Sex'].map({'M': 0, 'F': 1, 'I': 2})\n",
    "\n",
    "# # now show the first 5 rows of the dataframe\n",
    "# print(df.head())\n",
    "\n",
    "# # Create a boolean list for continuous variables (any float dtype)\n",
    "# is_cont_node = df.dtypes.map(is_float_dtype).tolist()\n",
    "\n",
    "# # Print the result\n",
    "# print(is_cont_node)\n",
    "\n",
    "# # plot the distribution of all variables in the dataframe\n",
    "# df.hist(figsize=(15, 10))\n",
    "# plt.show()\n",
    "\n",
    "# path to mixed_confounding data\n",
    "#path = '../data/custom_mixed_confounding/'\n",
    "path = '../data/custom_mixed_confounding_softplus/'\n",
    "\n",
    "# file name adjacency matrix in csv format\n",
    "mixed_confounding_adjacency_matrix = 'adj_matrix.csv'\n",
    "# file name observational data in csv format\n",
    "mixed_confounding_obs_data = 'train.csv'\n",
    "\n",
    "# load adjacency matrix and data as pandas dataframe, both files have no header\n",
    "adj_matrix = pd.read_csv(path + mixed_confounding_adjacency_matrix, header=None)\n",
    "data = pd.read_csv(path + mixed_confounding_obs_data, header=None)\n",
    "weighted_adj_matrix = pd.read_csv(path + 'W_adj_matrix.csv', header=None)\n",
    "\n",
    "B_true = adj_matrix.values\n",
    "X = data.values\n",
    "W_true = weighted_adj_matrix.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()\n",
    "# show unique values in each column\n",
    "print(data.nunique())\n",
    "# Determine if each variable is continuous or discrete based on the number of unique values\n",
    "is_cont_node = np.array([True if data[col].nunique() > 2 else False for col in data.columns])\n",
    "is_cont_node = is_cont_node.tolist()\n",
    "print(is_cont_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ## Load the actual connectome data\n",
    "\n",
    "# # %%\n",
    "# # load the weighted adjacency matrices for ER and connectome\n",
    "\n",
    "# # Specify the folder where the adjacency matrices were saved\n",
    "# folder = '../data/'\n",
    "\n",
    "# # Specify the folder where the acyclic positive integer weighted connectome data was saved\n",
    "# folder_cyclic = '/home/amine.mcharrak/connectome/data/'\n",
    "# # Specify the folder where the acyclic positive integer weighted connectome data was saved\n",
    "# folder_acyclic = '/home/amine.mcharrak/connectome/data/'\n",
    "\n",
    "# # Example usage to load the saved adjacency matrices\n",
    "# # G_A_init_t_ordered_adj_matrix = load_adjacency_matrix(os.path.join(folder, 'G_A_init_t_ordered_adj_matrix.npy'))\n",
    "# # G_A_init_t_ordered_dag_adj_matrix = load_adjacency_matrix(os.path.join(folder, 'G_A_init_t_ordered_dag_adj_matrix.npy'))\n",
    "# # ER = load_adjacency_matrix(os.path.join(folder, 'ER_adj_matrix.npy'))\n",
    "# # ER_dag = load_adjacency_matrix(os.path.join(folder, 'ER_dag_adj_matrix.npy'))\n",
    "\n",
    "# # Change name of the connectome adjacency matrix to C and C_dag\n",
    "# # C = G_A_init_t_ordered_adj_matrix\n",
    "# # C_dag = G_A_init_t_ordered_dag_adj_matrix\n",
    "\n",
    "# # Now ensure that both DAG adjacency matrices are binary, if they aren't already\n",
    "# # ER_dag_bin = (ER_dag != 0).astype(int)\n",
    "# # C_dag_bin = (C_dag != 0).astype(int)\n",
    "\n",
    "# # ER_true = ER_dag_bin\n",
    "# # C_true = C_dag_bin\n",
    "\n",
    "# # %% [markdown]\n",
    "# # ## Create data to debug and implement the pcax version of NOTEARS\n",
    "\n",
    "# # %%\n",
    "# #Â actual data\n",
    "# B_true = simulate_dag(d=10, s0=20, graph_type='ER') # ER2\n",
    "# # B_true = simulate_dag(d=100, s0=400, graph_type='ER') # ER4\n",
    "# # debugging data\n",
    "# # B_true = simulate_dag(d=10, s0=20, graph_type='ER') # ER2\n",
    "\n",
    "\n",
    "# # B_true = C_dag_bin # if you want to use the connectome-based DAG # best\n",
    "# #B_true = ER_dag_bin # if you want to use the ER-based DAG\n",
    "\n",
    "# #B_true = simulate_dag(d=5, s0=10, graph_type='ER') # ER2\n",
    "# #B_true = simulate_dag(d=10, s0=20, graph_type='ER') # ER2\n",
    "# #B_true = simulate_dag(d=50, s0=100, graph_type='ER') # ER2\n",
    "# #B_true = simulate_dag(d=100, s0=200, graph_type='ER') # ER2\n",
    "# #B_true = simulate_dag(d=279, s0=558, graph_type='ER') # ER2\n",
    "\n",
    "# # create SF2 graph and SF4 graph with d=10 nodes\n",
    "# #B_true = simulate_dag(d=10, s0=20, graph_type='SF') # SF2\n",
    "# #B_true = simulate_dag(d=10, s0=40, graph_type='SF') # SF4\n",
    "# #B_true = simulate_dag(d=100, s0=400, graph_type='SF') # SF4\n",
    "\n",
    "# # create ER2 and ER4 graphs with d=100 nodes\n",
    "# #B_true = simulate_dag(d=100, s0=200, graph_type='ER') # ER2\n",
    "# #B_true = simulate_dag(d=100, s0=400, graph_type='ER') # ER4\n",
    "\n",
    "# # create equivalent ER4 and ER6 graphs\n",
    "# #B_true = simulate_dag(d=279, s0=1116, graph_type='ER') # ER4\n",
    "# #B_true = simulate_dag(d=279, s0=1674, graph_type='ER') # ER6\n",
    "\n",
    "# # create equivalent SF4 and SF6 graphs\n",
    "# #B_true = simulate_dag(d=100, s0=600, graph_type='SF') # SF6\n",
    "# #B_true = simulate_dag(d=279, s0=1116, graph_type='SF') # SF4\n",
    "# #B_true = simulate_dag(d=279, s0=1674, graph_type='SF') # SF6\n",
    "\n",
    "# # create simple data using simulate_dag method from causal_helpers with expected number of edges (s0) and number of nodes (d)\n",
    "# #B_true = simulate_dag(d=100, s0=199, graph_type='ER') # we use pâ‰ˆ0.040226 for the connectome-based ER_dag graph. This means that the expected number of edges is 0.040226 * d * (d-1) / 2\n",
    "# # examples: d=50 -> s0=49 (works), d=100 -> s0=199, d=200 -> s0=800\n",
    "\n",
    "# # create the weighted adjacency matrix based on the binary adjacency matrix\n",
    "# #W_true = simulate_parameter(B_true, connectome=True)\n",
    "# #W_true = simulate_parameter(B_true)\n",
    "\n",
    "# # sample data from the linear SEM\n",
    "# #Â actual data\n",
    "# #X = simulate_linear_sem(W_true, n=10000, sem_type='gauss')\n",
    "# #X = simulate_linear_sem(W_true, n=10000, sem_type='uniform')\n",
    "# # for debugging\n",
    "# #X = simulate_linear_sem(W_true, n=1000, sem_type='gauss')\n",
    "# #X = simulate_linear_sem(W_true, n=2500, sem_type='gauss')\n",
    "# #X = simulate_linear_sem(W_true, n=6250, sem_type='gauss')\n",
    "# #X = simulate_linear_sem(W_true, n=50000, sem_type='gauss')\n",
    "# #X = simulate_linear_sem(W_true, n=100000, sem_type='gauss') # 1000*(279**2)/(20**2) = 194602\n",
    "\n",
    "# # load the cyclic integer weighted connectome data adjacency matrix\n",
    "# #B_true_weighted = load_adjacency_matrix(os.path.join(folder_cyclic, 'A_init_t_ordered_adj_matrix_with_cycles.npy'))\n",
    "# #X, W_true = sample_cyclic_data(B_true_weighted, n_samples=10000, noise_type='non-gaussian')\n",
    "# #B_true = (W_true != 0).astype(int)\n",
    "\n",
    "# # load the acyclic integer weighted connectome data adjacency matrix\n",
    "# # B_true_weighted = load_adjacency_matrix(os.path.join(folder_acyclic, 'A_init_t_ordered_adj_matrix_no_cycles.npy'))\n",
    "# # print(\"B_true_weighted:\\n\", np.array_str(B_true_weighted, precision=4, suppress_small=True))\n",
    "\n",
    "# # A: use this for regular DAGs\n",
    "# W_true = simulate_parameter(B_true)\n",
    "\n",
    "# # B: use this for connectome-based DAGs\n",
    "# #W_true = simulate_parameter(B_true_weighted, connectome=True)\n",
    "# #B_true = (W_true != 0).astype(int)\n",
    "\n",
    "# # some print statements to check the values of W_true\n",
    "# print(\"W_true:\\n\", np.array_str(W_true, precision=4, suppress_small=True))\n",
    "# print(\"Mean of W_true:\", np.mean(W_true))\n",
    "# print(\"Variance of W_true:\", np.var(W_true))\n",
    "# print(\"Max value in W_true:\", np.max(W_true))\n",
    "# print(\"Min value in W_true:\", np.min(W_true))\n",
    "\n",
    "# # sample data from the linear SEM\n",
    "# X = simulate_linear_sem(W_true, n=1000, sem_type='gauss')\n",
    "\n",
    "# # now standardized data, where each variable is normalized to unit variance\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# scaler = StandardScaler()\n",
    "# X_std = scaler.fit_transform(X)\n",
    "\n",
    "# # NOTE: you may not write positional arguments after keyword arguments. \n",
    "# # That is, the values that you are passing positionally have to come first!\n",
    "\n",
    "# # create a dataset using the simulated data\n",
    "# # NOTE: NOTEARS paper uses n=1000 for graph with d=20.\n",
    "# # NOTE: d... number of nodes, p=d^2... number of parameters, n... number of samples. Then: comparing p1=d1^2 vs p2=d2^2 we have that: n1/p1 must be equal to n2/p2\n",
    "# # Thus we have n2 = n1 * p2 / p1. For the case of d2=100 we have that n2 = (n1*p2)/p1 = 1000*(100^2)/(20^2) = 25000 \n",
    "# # we should expect to use that many samples actually to be able to learn the graph in a comparable way.\n",
    "# #dataset = IIDSimulation(W=W_true, n=25000, method='linear', sem_type='gauss')\n",
    "# #true_dag, X = dataset.B, dataset.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print how many non-zero entries are in the true DAG\n",
    "print(f\"Number of non-zero entries in the true DAG: {np.count_nonzero(B_true)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility and evaluation functions\n",
    "@jit\n",
    "def MAE(W_true, W):\n",
    "    \"\"\"This function returns the Mean Absolute Error for the difference between the true weighted adjacency matrix W_true and th estimated one, W.\"\"\"\n",
    "    MAE_ = jnp.mean(jnp.abs(W - W_true))\n",
    "    return MAE_\n",
    "\n",
    "@jax.jit\n",
    "def compute_h_reg(W):\n",
    "    \"\"\"\n",
    "    Compute the DAG constraint using the exponential trace-based acyclicity constraint.\n",
    "\n",
    "    This function calculates the value of the acyclicity constraint for a given\n",
    "    adjacency matrix using the formulation:\n",
    "    h_reg = trace(exp(W âŠ™ W)) - d\n",
    "    where âŠ™ represents the Hadamard (element-wise) product.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    W : jnp.ndarray\n",
    "        (d, d) adjacency matrix.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    h_reg : float\n",
    "        The value of the DAG constraint.\n",
    "    \"\"\"\n",
    "    # Dimensions of W\n",
    "    d = W.shape[0]\n",
    "\n",
    "    # Compute h_reg using the trace of the matrix exponential\n",
    "    h_reg = jnp.trace(jax_scipy_linalg.expm(jnp.multiply(W, W))) - d\n",
    "\n",
    "    return h_reg\n",
    "\n",
    "@jax.jit\n",
    "def notears_dag_constraint(W):\n",
    "    \"\"\"\n",
    "    Compute the NOTEARS DAG constraint using the exponential trace-based acyclicity constraint.\n",
    "\n",
    "    This function calculates the value of the acyclicity constraint for a given\n",
    "    adjacency matrix using the formulation:\n",
    "    h_reg = trace(exp(W âŠ™ W)) - d\n",
    "    where âŠ™ represents the Hadamard (element-wise) product.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    W : jnp.ndarray\n",
    "        (d, d) adjacency matrix.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    h_reg : float\n",
    "        The value of the DAG constraint.\n",
    "    \"\"\"\n",
    "    # Dimensions of W\n",
    "    d = W.shape[0]\n",
    "\n",
    "    # Compute h_reg using the trace of the matrix exponential\n",
    "    h_reg = jnp.trace(jax_scipy_linalg.expm(jnp.multiply(W, W))) - d\n",
    "\n",
    "    return h_reg\n",
    "\n",
    "@jax.jit\n",
    "def dagma_dag_constraint(W, s=1.0):\n",
    "    \"\"\"\n",
    "    Compute the DAG constraint using the logdet acyclicity constraint from DAGMA.\n",
    "    This function is JAX-jitted for improved performance.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    W : jnp.ndarray\n",
    "        (d, d) adjacency matrix.\n",
    "    s : float, optional\n",
    "        Controls the domain of M-matrices. Defaults to 1.0.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    h_reg : float\n",
    "        The value of the DAG constraint.\n",
    "    \"\"\"\n",
    "    # Dimensions of W\n",
    "    d = W.shape[0]\n",
    "\n",
    "    # Compute M-matrix for the logdet constraint\n",
    "    M = s * jnp.eye(d) - jnp.multiply(W, W)\n",
    "\n",
    "    # Compute the value of the logdet DAG constraint\n",
    "    h_reg = -jax_numpy_linalg.slogdet(M)[1] + d * jnp.log(s)\n",
    "\n",
    "    return h_reg\n",
    "\n",
    "\n",
    "# Define fucntion to compute h_reg based W with h_reg = jnp.trace(jax.scipy.linalg.expm(W * W)) - d, here * denotes the hadamard product\n",
    "def compute_h_reg(W):\n",
    "    \"\"\"This function computes the h_reg term based on the matrix W.\"\"\"\n",
    "    h_reg = jnp.trace(jax.scipy.linalg.expm(W * W)) - W.shape[0]\n",
    "    return h_reg\n",
    "\n",
    "def compute_binary_adjacency(W, threshold=0.3):\n",
    "    \"\"\"\n",
    "    Compute the binary adjacency matrix by thresholding the input matrix.\n",
    "\n",
    "    Args:\n",
    "    - W (array-like): The weighted adjacency matrix (can be a JAX array or a NumPy array).\n",
    "    - threshold (float): The threshold value to determine the binary matrix. Default is 0.3.\n",
    "\n",
    "    Returns:\n",
    "    - B_est (np.ndarray): The binary adjacency matrix where each element is True if the corresponding \n",
    "                          element in W is greater than the threshold, otherwise False.\n",
    "    \"\"\"\n",
    "    # Convert JAX array to NumPy array if necessary\n",
    "    if isinstance(W, jnp.ndarray):\n",
    "        W = np.array(W)\n",
    "\n",
    "    # Compute the binary adjacency matrix\n",
    "    B_est = np.array(np.abs(W) > threshold)\n",
    "    \n",
    "    return B_est\n",
    "\n",
    "def ensure_DAG(W):\n",
    "    \"\"\"\n",
    "    Ensure that the weighted adjacency matrix corresponds to a DAG.\n",
    "\n",
    "    Inputs:\n",
    "        W: numpy.ndarray - a weighted adjacency matrix representing a directed graph\n",
    "\n",
    "    Outputs:\n",
    "        W: numpy.ndarray - a weighted adjacency matrix without cycles (DAG)\n",
    "    \"\"\"\n",
    "    # Convert the adjacency matrix to a directed graph\n",
    "    g = nx.DiGraph(W)\n",
    "\n",
    "    # Make a copy of the graph to modify\n",
    "    gg = g.copy()\n",
    "\n",
    "    # Remove cycles by removing edges\n",
    "    while not nx.is_directed_acyclic_graph(gg):\n",
    "        h = gg.copy()\n",
    "\n",
    "        # Remove all the sources and sinks\n",
    "        while True:\n",
    "            finished = True\n",
    "\n",
    "            for node, in_degree in nx.in_degree_centrality(h).items():\n",
    "                if in_degree == 0:\n",
    "                    h.remove_node(node)\n",
    "                    finished = False\n",
    "\n",
    "            for node, out_degree in nx.out_degree_centrality(h).items():\n",
    "                if out_degree == 0:\n",
    "                    h.remove_node(node)\n",
    "                    finished = False\n",
    "\n",
    "            if finished:\n",
    "                break\n",
    "\n",
    "        # Find a cycle with a random walk starting at a random node\n",
    "        node = list(h.nodes)[0]\n",
    "        cycle = [node]\n",
    "        while True:\n",
    "            edges = list(h.out_edges(node))\n",
    "            _, node = edges[np.random.choice(len(edges))]\n",
    "\n",
    "            if node in cycle:\n",
    "                break\n",
    "\n",
    "            cycle.append(node)\n",
    "\n",
    "        # Extract the cycle path and adjust it to start at the first occurrence of the repeated node\n",
    "        cycle = np.array(cycle)\n",
    "        i = np.argwhere(cycle == node)[0][0]\n",
    "        cycle = cycle[i:]\n",
    "        cycle = cycle.tolist() + [node]\n",
    "\n",
    "        # Find edges in that cycle\n",
    "        edges = list(zip(cycle[:-1], cycle[1:]))\n",
    "\n",
    "        # Randomly pick an edge to remove\n",
    "        edge = edges[np.random.choice(len(edges))]\n",
    "        gg.remove_edge(*edge)\n",
    "\n",
    "    # Convert the modified graph back to a weighted adjacency matrix\n",
    "    W_acyclic = nx.to_numpy_array(gg)\n",
    "\n",
    "    return W_acyclic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Complete_Graph(pxc.EnergyModule):\n",
    "    def __init__(self, input_dim: int, n_nodes: int, has_bias: bool = False, is_cont_node: list = None) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = px.static(input_dim)  # Ensure input_dim is static\n",
    "        self.n_nodes = px.static(n_nodes)  # Keep n_nodes as a static value\n",
    "        self.has_bias = has_bias\n",
    "        self.is_cont_node = is_cont_node\n",
    "\n",
    "        # Initialize a single linear layer for the weights and wrap it in a list\n",
    "        self.layers = [pxnn.Linear(n_nodes, n_nodes, bias=has_bias)] # vanilla initialization is uniform(-stdv, stdv) with stdv = 1/sqrt(n_nodes), here n_nodes = 12, thus stdv = 1/sqrt(12) = 0.2887\n",
    "        \n",
    "        #stddev = jnp.sqrt(0.01) # this equals 0.1 (default would have been 0.2887)\n",
    "        stddev = 1/n_nodes\n",
    "        key = random.PRNGKey(0)\n",
    "        #new_weight_matrix = random.normal(key, shape=(n_nodes, n_nodes)) * stddev # option 1 using normal distribution\n",
    "        new_weight_matrix = random.uniform(key, shape=(n_nodes, n_nodes), minval=-stddev, maxval=stddev) # option 2 using uniform distribution\n",
    "\n",
    "        # Step 3: Replace diagonal elements with 0\n",
    "        for i in range(n_nodes):\n",
    "            new_weight_matrix = new_weight_matrix.at[i, i].set(0.0)\n",
    "\n",
    "        # Step 5: Update the weight matrix\n",
    "        self.layers[0].nn.weight.set(new_weight_matrix)\n",
    "\n",
    "        # Initialize vodes based on is_cont_node\n",
    "        if is_cont_node is None:\n",
    "            is_cont_node = [True] * n_nodes  # Default to all continuous nodes if not provided\n",
    "\n",
    "        self.vodes = []\n",
    "        for is_cont in is_cont_node:\n",
    "            if is_cont:\n",
    "                self.vodes.append(pxc.Vode())\n",
    "            else:\n",
    "                self.vodes.append(pxc.Vode(pxc.bce_energy))\n",
    "\n",
    "    def freeze_nodes(self, freeze=True):\n",
    "        \"\"\"Freeze or unfreeze all vodes in the model.\"\"\"\n",
    "        for vode in self.vodes:\n",
    "            vode.h.frozen = freeze\n",
    "\n",
    "    def are_vodes_frozen(self):\n",
    "        \"\"\"Check if all vodes in the model are frozen.\"\"\"\n",
    "        return all(hasattr(vode.h, 'frozen') and vode.h.frozen for vode in self.vodes)\n",
    "    \n",
    "    def get_W(self):\n",
    "        \"\"\"This function returns the weighted adjacency matrix based on the linear layer in the model.\"\"\"\n",
    "        W = self.layers[0].nn.weight.get()\n",
    "        W_T = W.T\n",
    "        return W_T\n",
    "\n",
    "    def __call__(self, x=None):\n",
    "        n_nodes = self.n_nodes.get()\n",
    "\n",
    "        if x is not None:\n",
    "            # Initialize nodes with given data\n",
    "            reshaped_x = x.reshape(n_nodes, -1)  # Infer input_dim from x\n",
    "            \n",
    "            # Print the shape of reshaped_x[0] when x is not None\n",
    "            print(\"The shape of reshaped_x[0] when x is not None is: \", reshaped_x[0].shape)\n",
    "\n",
    "            for i in range(n_nodes):\n",
    "                self.vodes[i](reshaped_x[i])\n",
    "\n",
    "        else:\n",
    "            # Stack current state of vodes into a matrix of shape (n_nodes, input_dim)\n",
    "            x_ = jnp.vstack([vode.get('h') for vode in self.vodes])\n",
    "\n",
    "            # Print the shape of x_ when x is None\n",
    "            print(\"The shape of x_ when x is None is: \", x_.shape)\n",
    "\n",
    "            # Apply the linear transformation\n",
    "            output = self.layers[0](x_)\n",
    "\n",
    "            # Print the shape of output when x is None\n",
    "            print(\"The shape of output when x is None is: \", output.shape)\n",
    "\n",
    "            # Update the vodes with the output\n",
    "            for i in range(n_nodes):\n",
    "                self.vodes[i](output[i])\n",
    "\n",
    "        # Stack the final state of vodes for output\n",
    "        output = jnp.vstack([vode.get('h') for vode in self.vodes])\n",
    "\n",
    "        # Print the shape of the output\n",
    "        print(\"The shape of the output is: \", output.shape)\n",
    "\n",
    "        return output\n",
    "\n",
    "# Usage\n",
    "input_dim = 1\n",
    "n_nodes = X.shape[1]\n",
    "#model = Complete_Graph(input_dim, n_nodes, has_bias=False)\n",
    "#model = Complete_Graph(input_dim, n_nodes, has_bias=True)\n",
    "model = Complete_Graph(input_dim, n_nodes, has_bias=True, is_cont_node=is_cont_node)\n",
    "# Get weighted adjacency matrix\n",
    "W = model.get_W()\n",
    "print(\"This is the weighted adjacency matrix:\\n\", W)\n",
    "print()\n",
    "print(\"The shape of the weighted adjacency matrix is: \", W.shape)\n",
    "print()\n",
    "#print(model)\n",
    "print()\n",
    "\n",
    "# Check if all nodes are frozen initially\n",
    "print(\"Initially, are all nodes frozen?:\", model.are_vodes_frozen())\n",
    "print()\n",
    "\n",
    "# Freezing all nodes\n",
    "print(\"Freezing all nodes...\")\n",
    "model.freeze_nodes(freeze=True)\n",
    "print()\n",
    "\n",
    "# Check if all nodes are frozen after freezing\n",
    "print(\"After freezing, are all nodes frozen?:\", model.are_vodes_frozen())\n",
    "print()\n",
    "\n",
    "# Unfreezing all nodes\n",
    "print(\"Unfreezing all nodes...\")\n",
    "model.freeze_nodes(freeze=False)\n",
    "print()\n",
    "\n",
    "# Check if all nodes are frozen after unfreezing\n",
    "print(\"After unfreezing, are all nodes frozen?:\", model.are_vodes_frozen())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.is_cont_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: make the below params global or input to the functions in which it is used.\n",
    "w_learning_rate = 1e-3 # Notes: 5e-1 is too high\n",
    "h_learning_rate = 1e-4\n",
    "T = 1\n",
    "\n",
    "nm_epochs = 2000 # not much happens after 2000 epochs\n",
    "every_n_epochs = 1\n",
    "batch_size = 128\n",
    "\n",
    "\n",
    "# {'fdr': 0.0667, 'tpr': 0.9333, 'fpr': 0.0196, 'shd': 1, 'nnz': 15, 'precision': 0.9333, 'recall': 0.9333, 'F1': 0.9333, 'gscore': 0.8667}\n",
    "# bs_128_lrw_0.001_lrh_0.0001_lamh_10000.0_laml1_1.0_epochs_2000 gives F1 score of 0.9333\n",
    "# NOTE THIS USES EPSILON=1e-8 FOR THE NORMALIZATION OF THE REGULARIZATION TERMS\n",
    "# lam_h = 1e4\n",
    "# lam_l1 = 1e0\n",
    "\n",
    "lam_h = 1e4\n",
    "lam_l1 = 1e0\n",
    "\n",
    "# Create a file name string for the hyperparameters\n",
    "exp_name = f\"bs_{batch_size}_lrw_{w_learning_rate}_lrh_{h_learning_rate}_lamh_{lam_h}_laml1_{lam_l1}_epochs_{nm_epochs}\"\n",
    "print(\"Name of the experiment: \", exp_name)\n",
    "\n",
    "# Training an1d evaluation functions\n",
    "@pxf.vmap(pxu.M(pxc.VodeParam | pxc.VodeParam.Cache).to((None, 0)), in_axes=(0,), out_axes=0)\n",
    "def forward(x, *, model: Complete_Graph):\n",
    "    return model(x)\n",
    "\n",
    "# @pxf.vmap(pxu.M(pxc.VodeParam | pxc.VodeParam.Cache).to((None, 0)), out_axes=(None, 0), axis_name=\"batch\") # if only one output\n",
    "@pxf.vmap(pxu.M(pxc.VodeParam | pxc.VodeParam.Cache).to((None, 0)), out_axes=(None, None, None, None, 0), axis_name=\"batch\") # if multiple outputs\n",
    "def energy(*, model: Complete_Graph):\n",
    "\n",
    "    print(\"Energy: Starting computation\")\n",
    "    x_ = model(None)\n",
    "    print(\"Energy: Got model output\")\n",
    "    \n",
    "    W = model.get_W()\n",
    "    # Dimensions of W\n",
    "    d = W.shape[0]\n",
    "    print(f\"Energy: Got W (shape: {W.shape}) and d: {d}\")\n",
    "\n",
    "    # PC energy term\n",
    "    pc_energy = jax.lax.pmean(model.energy(), axis_name=\"batch\")\n",
    "    print(f\"Energy: PC energy term: {pc_energy}\")\n",
    "\n",
    "    # L1 regularization using adjacency matrix (scaled by Frobenius norm)\n",
    "    l1_reg = jnp.sum(jnp.abs(W)) / (jnp.linalg.norm(W, ord='fro') + 1e-8)\n",
    "    #l1_reg = jnp.sum(jnp.abs(W)) / d\n",
    "    print(f\"Energy: L1 reg term: {l1_reg}\")\n",
    "\n",
    "    # DAG constraint (stable logarithmic form)\n",
    "    #h_reg = notears_dag_constraint(W)\n",
    "    #h_reg = notears_dag_constraint(W)/ (jnp.sqrt(d) + 1e-8)\n",
    "    #h_reg = notears_dag_constraint(W) / d  # with normalization\n",
    "\n",
    "    #h_reg = dagma_dag_constraint(W)\n",
    "    h_reg = dagma_dag_constraint(W) / (jnp.sqrt(d) + 1e-8)\n",
    "    #h_reg = dagma_dag_constraint(W) / d  # with normalization\n",
    "    print(f\"Energy: DAG constraint term: {h_reg}\")\n",
    "        \n",
    "    # Combined loss\n",
    "    obj = pc_energy + lam_h * h_reg + lam_l1 * l1_reg\n",
    "    print(f\"Energy: Final objective: {obj}\")\n",
    "\n",
    "    # Ensure obj is a scalar, not a (1,) array because JAX's grad and value_and_grad functions are designed\n",
    "    # to compute gradients of *scalar-output functions*\n",
    "    obj = obj.squeeze()  # explicitly converte the (1,) array (obj) to a scalar of shape ()  \n",
    "    \n",
    "    return obj, pc_energy, h_reg, l1_reg, x_\n",
    "\n",
    "@pxf.jit(static_argnums=0)\n",
    "def train_on_batch(T: int, x: jax.Array, *, model: Complete_Graph, optim_w: pxu.Optim, optim_h: pxu.Optim):\n",
    "    print(\"1. Starting train_on_batch\")  \n",
    "\n",
    "    model.train()\n",
    "    print(\"2. Model set to train mode\")\n",
    "\n",
    "    model.freeze_nodes(freeze=True)\n",
    "    print(\"3. Nodes frozen\")\n",
    "\n",
    "    # init step\n",
    "    with pxu.step(model, pxc.STATUS.INIT, clear_params=pxc.VodeParam.Cache):\n",
    "        print(\"4. Doing forward for initialization\")\n",
    "        forward(x, model=model)\n",
    "        print(\"5. After forward for initialization\")\n",
    "\n",
    "    \"\"\"\n",
    "    # The following code might not be needed as we are keeping the vodes frozen at all times\n",
    "    # Reinitialize the optimizer state between different batches\n",
    "    optim_h.init(pxu.M(pxc.VodeParam)(model))\n",
    "\n",
    "    for _ in range(T):\n",
    "        with pxu.step(model, clear_params=pxc.VodeParam.Cache):\n",
    "            (e, x_), g = pxf.value_and_grad(\n",
    "                pxu.M(pxu.m(pxc.VodeParam).has_not(frozen=True), [False, True]),\n",
    "                has_aux=True\n",
    "            )(energy)(model=model)\n",
    "        optim_h.step(model, g[\"model\"], True)\n",
    "    \"\"\"\n",
    "\n",
    "    with pxu.step(model, clear_params=pxc.VodeParam.Cache):\n",
    "        print(\"6. Before computing gradients\")\n",
    "        (obj, (pc_energy, h_reg, l1_reg, x_)), g = pxf.value_and_grad(\n",
    "            pxu.M(pxnn.LayerParam).to([False, True]), \n",
    "            has_aux=True\n",
    "        )(energy)(model=model) # pxf.value_and_grad returns a tuple structured as ((value, aux), grad), not as six separate outputs.\n",
    "        \n",
    "        print(\"7. After computing gradients\")\n",
    "        #print(\"Gradient structure:\", g)\n",
    "\n",
    "        print(\"8. Before zeroing out the diagonal gradients\")\n",
    "        # Zero out the diagonal gradients using jax.numpy.fill_diagonal\n",
    "        weight_grads = g[\"model\"].layers[0].nn.weight.get()\n",
    "        weight_grads = jax.numpy.fill_diagonal(weight_grads, 0.0, inplace=False)\n",
    "        # print the grad values using the syntax jax.debug.print(\"ðŸ¤¯ {x} ðŸ¤¯\", x=x)\n",
    "        #jax.debug.print(\"{weight_grads}\", weight_grads=weight_grads)\n",
    "        g[\"model\"].layers[0].nn.weight.set(weight_grads)\n",
    "        print(\"9. After zeroing out the diagonal gradients\")\n",
    "\n",
    "        \n",
    "    print(\"10. Before optimizer step\")\n",
    "    optim_w.step(model, g[\"model\"])\n",
    "    #optim_w.step(model, g[\"model\"], scale_by=1.0/x.shape[0])\n",
    "    print(\"11. After optimizer step\")\n",
    "\n",
    "    with pxu.step(model, clear_params=pxc.VodeParam.Cache):\n",
    "        print(\"12. Before final forward\")\n",
    "        forward(None, model=model)\n",
    "        e_avg_per_sample = model.energy()\n",
    "        print(\"13. After final forward\")\n",
    "\n",
    "    model.freeze_nodes(freeze=False)\n",
    "    print(\"14. Nodes unfrozen\")\n",
    "\n",
    "    return pc_energy, l1_reg, h_reg, obj\n",
    "\n",
    "def train(dl, T, *, model: Complete_Graph, optim_w: pxu.Optim, optim_h: pxu.Optim):\n",
    "    batch_pc_energies = []\n",
    "    batch_l1_regs = []\n",
    "    batch_h_regs = []\n",
    "    batch_objs = []\n",
    "    \n",
    "    for batch in dl:\n",
    "        pc_energy, l1_reg, h_reg, obj = train_on_batch(\n",
    "            T, batch, model=model, optim_w=optim_w, optim_h=optim_h\n",
    "        )\n",
    "        batch_pc_energies.append(pc_energy)\n",
    "        batch_l1_regs.append(l1_reg)\n",
    "        batch_h_regs.append(h_reg)\n",
    "        batch_objs.append(obj)\n",
    "\n",
    "    W = model.get_W()\n",
    "\n",
    "    # Compute epoch averages\n",
    "    epoch_pc_energy = jnp.mean(jnp.array(batch_pc_energies))\n",
    "    epoch_l1_reg = jnp.mean(jnp.array(batch_l1_regs))\n",
    "    epoch_h_reg = jnp.mean(jnp.array(batch_h_regs))\n",
    "    epoch_obj = jnp.mean(jnp.array(batch_objs))\n",
    "    \n",
    "    return W, epoch_pc_energy, epoch_l1_reg, epoch_h_reg, epoch_obj\n",
    "\n",
    "# %%\n",
    "# for reference compute the MAE, SID, and SHD between the true adjacency matrix and an all-zero matrix and then print it\n",
    "# this acts as a baseline for the MAE, SID, and SHD similar to how 1/K accuracy acts as a baseline for classification tasks where K is the number of classes\n",
    "\n",
    "W_zero = np.zeros_like(W_true)\n",
    "print(\"MAE between the true adjacency matrix and an all-zero matrix: \", MAE(W_true, W_zero))\n",
    "print(\"SHD between the true adjacency matrix and an all-zero matrix: \", SHD(B_true, compute_binary_adjacency(W_zero)))\n",
    "#print(\"SID between the true adjacency matrix and an all-zero matrix: \", SID(W_true, W_zero))\n",
    "\n",
    "# %%\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X):\n",
    "        self.X = X\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx]\n",
    "\n",
    "# This is a simple collate function that stacks numpy arrays used to interface\n",
    "# the PyTorch dataloader with JAX. In the future we hope to provide custom dataloaders\n",
    "# that are independent of PyTorch.\n",
    "def numpy_collate(batch):\n",
    "    if isinstance(batch[0], np.ndarray):\n",
    "        return np.stack(batch)\n",
    "    elif isinstance(batch[0], (tuple, list)):\n",
    "        transposed = zip(*batch)\n",
    "        return [numpy_collate(samples) for samples in transposed]\n",
    "    else:\n",
    "        return np.array(batch)\n",
    "\n",
    "# The dataloader assumes cuda is being used, as such it sets 'pin_memory = True' and\n",
    "# 'prefetch_factor = 2'. Note that the batch size should be constant during training, so\n",
    "# we set 'drop_last = True' to avoid having to deal with variable batch sizes.\n",
    "class TorchDataloader(torch.utils.data.DataLoader):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset,\n",
    "        batch_size=1,\n",
    "        shuffle=None,\n",
    "        sampler=None,\n",
    "        batch_sampler=None,\n",
    "        num_workers=1,\n",
    "        pin_memory=True,\n",
    "        timeout=0,\n",
    "        worker_init_fn=None,\n",
    "        persistent_workers=True,\n",
    "        prefetch_factor=2,\n",
    "    ):\n",
    "        super(self.__class__, self).__init__(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            sampler=sampler,\n",
    "            batch_sampler=batch_sampler,\n",
    "            num_workers=num_workers,\n",
    "            collate_fn=numpy_collate,\n",
    "            pin_memory=pin_memory,\n",
    "            drop_last=True if batch_sampler is None else None,\n",
    "            timeout=timeout,\n",
    "            worker_init_fn=worker_init_fn,\n",
    "            persistent_workers=persistent_workers,\n",
    "            prefetch_factor=prefetch_factor,\n",
    "        )\n",
    "\n",
    "\n",
    "# Create the custom dataset\n",
    "dataset = CustomDataset(X)\n",
    "\n",
    "# Create the custom dataset with standardized data\n",
    "#dataset_std = CustomDataset(X_std)\n",
    "\n",
    "# Create the dataloader\n",
    "dl = TorchDataloader(dataset, batch_size=batch_size, shuffle=True)\n",
    "######## OR ########\n",
    "#dl = TorchDataloader(dataset_std, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# %%\n",
    "# Initialize the model and optimizers\n",
    "with pxu.step(model, pxc.STATUS.INIT, clear_params=pxc.VodeParam.Cache):\n",
    "    forward(jnp.zeros((batch_size, model.n_nodes.get())), model=model)\n",
    "    optim_h = pxu.Optim(lambda: optax.sgd(h_learning_rate))\n",
    "\n",
    "    \"\"\"\n",
    "    optim_w = pxu.Optim(\n",
    "    optax.chain(\n",
    "        optax.clip_by_global_norm(clip_value),  # Clip gradients by global norm\n",
    "        optax.sgd(w_learning_rate)  # Apply SGD optimizer\n",
    "    ),\n",
    "    pxu.M(pxnn.LayerParam)(model)  # Masking the parameters of the model\n",
    ")\n",
    "    \"\"\"\n",
    "    #optim_w = pxu.Optim(lambda: optax.adam(w_learning_rate), pxu.M(pxnn.LayerParam)(model))\n",
    "    optim_w = pxu.Optim(lambda: optax.adamw(w_learning_rate, nesterov=True), pxu.M(pxnn.LayerParam)(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store differences and energies\n",
    "MAEs = []\n",
    "SHDs = []\n",
    "F1s = []\n",
    "pc_energies = []\n",
    "l1_regs = []\n",
    "h_regs = []\n",
    "objs = []\n",
    "\n",
    "# Calculate the initial MAE, SID, and SHD\n",
    "\n",
    "W_init = model.get_W()\n",
    "B_init = compute_binary_adjacency(W_init)\n",
    "\n",
    "MAE_init = MAE(W_true, W_init)\n",
    "print(f\"Start difference (cont.) between W_true and W_init: {MAE_init:.4f}\")\n",
    "\n",
    "SHD_init = SHD(B_true, B_init)\n",
    "print(f\"Start SHD between B_true and B_init: {SHD_init:.4f}\")\n",
    "\n",
    "F1_init = compute_F1_directed(B_true, B_init)\n",
    "print(f\"Start F1 between B_true and B_init: {F1_init:.4f}\")\n",
    "\n",
    "# print the values of the diagonal of the initial W\n",
    "print(\"The diagonal of the initial W: \", jnp.diag(W_init))\n",
    "\n",
    "# Start timing\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "# Training loop\n",
    "with tqdm(range(nm_epochs), position=0, leave=True) as pbar:\n",
    "    for epoch in pbar:\n",
    "        # Train for one epoch using the dataloader\n",
    "        W, epoch_pc_energy, epoch_l1_reg, epoch_h_reg, epoch_obj = train(dl, T=T, model=model, optim_w=optim_w, optim_h=optim_h)\n",
    "        \n",
    "        # Extract the weighted adjacency matrix W and compute the binary adjacency matrix B\n",
    "        W = np.array(W)\n",
    "        B = compute_binary_adjacency(W)\n",
    "\n",
    "        # Compute metrics every n epochs\n",
    "        if (epoch + 1) % every_n_epochs == 0 or epoch == 0:\n",
    "            MAEs.append(float(MAE(W_true, W)))\n",
    "            SHDs.append(float(SHD(B_true, compute_binary_adjacency(W), double_for_anticausal=False)))\n",
    "            F1s.append(float(compute_F1_directed(B_true, B)))\n",
    "            pc_energies.append(float(epoch_pc_energy))\n",
    "            l1_regs.append(float(epoch_l1_reg))\n",
    "            epoch_h_reg_raw = compute_h_reg(W)\n",
    "            h_regs.append(float(epoch_h_reg_raw))\n",
    "            objs.append(float(epoch_obj))\n",
    "\n",
    "            # Update progress bar with the current status\n",
    "            pbar.set_description(f\"MAE: {MAEs[-1]:.4f}, F1: {F1s[-1]:.4f}, SHD: {SHDs[-1]:.4f} || PC Energy: {pc_energies[-1]:.4f}, L1 Reg: {l1_regs[-1]:.4f}, H Reg: {h_regs[-1]:.4f}, Obj: {objs[-1]:.4f}\")\n",
    "\n",
    "\n",
    "# End timing\n",
    "end_time = timeit.default_timer()\n",
    "\n",
    "# Print the average time per epoch\n",
    "average_time_per_epoch = (end_time - start_time) / nm_epochs\n",
    "print(f\"An epoch (with compiling and testing) took on average: {average_time_per_epoch:.4f} seconds\")\n",
    "# print the values of the diagonal of the final W\n",
    "print(\"The diagonal of the final W: \", jnp.diag(model.get_W()))\n",
    "\n",
    "\n",
    "# print in big that training is done\n",
    "print(\"\\n\\n ###########################  Training is done  ########################### \\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define experiment name\n",
    "exp_name = f\"bs_{batch_size}_lrw_{w_learning_rate}_lrh_{h_learning_rate}_lamh_{lam_h}_laml1_{lam_l1}_epochs_{nm_epochs}\"\n",
    "\n",
    "# Create subdirectory in linear folder with the name stored in exp_name\n",
    "save_path = os.path.join('plots/linear_mixed_disc_x0', exp_name)\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "# Reset to default style and set seaborn style\n",
    "plt.style.use('default')\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Update matplotlib parameters\n",
    "plt.rcParams.update({\n",
    "    'figure.dpi': 300,\n",
    "    'savefig.dpi': 300,\n",
    "    'font.size': 12,\n",
    "    'axes.titlesize': 14,\n",
    "    'axes.labelsize': 12,\n",
    "    'xtick.labelsize': 10,\n",
    "    'ytick.labelsize': 10,\n",
    "    'legend.fontsize': 10,\n",
    "    'figure.titlesize': 16,\n",
    "    'axes.grid': True,\n",
    "    'grid.alpha': 0.3,\n",
    "    'axes.spines.top': False,\n",
    "    'axes.spines.right': False\n",
    "})\n",
    "\n",
    "# Create a figure and subplots using GridSpec\n",
    "fig = plt.figure(figsize=(20, 12))\n",
    "gs = fig.add_gridspec(2, 4, height_ratios=[1, 1], width_ratios=[1, 1, 1, 1])\n",
    "\n",
    "# Adjust layout to make more room for title and subtitle\n",
    "plt.subplots_adjust(top=0.85, hspace=0.4, wspace=0.3)\n",
    "\n",
    "# Create axes\n",
    "ax00 = fig.add_subplot(gs[0, 0])\n",
    "ax01 = fig.add_subplot(gs[0, 1])\n",
    "ax02 = fig.add_subplot(gs[0, 2])\n",
    "ax03 = fig.add_subplot(gs[0, 3])\n",
    "ax10 = fig.add_subplot(gs[1, 0])\n",
    "ax11 = fig.add_subplot(gs[1, 1])\n",
    "ax12 = fig.add_subplot(gs[1, 2:])\n",
    "# ax12 spans two columns\n",
    "\n",
    "axes = [ax00, ax01, ax02, ax03, ax10, ax11, ax12]\n",
    "\n",
    "# Plot configurations\n",
    "plot_configs = [\n",
    "    {'metric': MAEs, 'title': 'Mean Absolute Error', 'ylabel': 'MAE', 'color': '#2ecc71', 'ax': ax00},\n",
    "    {'metric': SHDs, 'title': 'Structural Hamming Distance', 'ylabel': 'SHD', 'color': '#e74c3c', 'ax': ax01},\n",
    "    {'metric': F1s, 'title': 'F1 Score', 'ylabel': 'F1', 'color': '#3498db', 'ax': ax02},\n",
    "    {'metric': pc_energies, 'title': 'PC Energy', 'ylabel': 'Energy', 'color': '#9b59b6', 'ax': ax03},\n",
    "    {'metric': l1_regs, 'title': 'L1 Regularization', 'ylabel': 'L1', 'color': '#f1c40f', 'ax': ax10},\n",
    "    {'metric': h_regs, 'title': 'DAG Constraint', 'ylabel': 'h(W)', 'color': '#e67e22', 'ax': ax11},\n",
    "    {'metric': objs, 'title': 'Total Objective', 'ylabel': 'Loss', 'color': '#1abc9c', 'ax': ax12}\n",
    "]\n",
    "\n",
    "# Create all subplots\n",
    "for config in plot_configs:\n",
    "    ax = config['ax']\n",
    "    \n",
    "    # Plot data with rolling average\n",
    "    epochs = range(len(config['metric']))\n",
    "    \n",
    "    # Determine if we should use log scale and/or scaling factor\n",
    "    use_log_scale = config['title'] in ['PC Energy', 'Total Objective']\n",
    "    scale_factor = 1e4 if config['title'] == 'DAG Constraint' else 1\n",
    "    \n",
    "    # Apply scaling and/or log transform to the metric\n",
    "    metric_values = np.array(config['metric'])\n",
    "    if use_log_scale:\n",
    "        # Add small constant to avoid log(0)\n",
    "        metric_values = np.log10(np.abs(metric_values) + 1e-10)\n",
    "    metric_values = metric_values * scale_factor\n",
    "    \n",
    "    # Plot raw data\n",
    "    raw_line = ax.plot(epochs, metric_values, \n",
    "                      alpha=0.3, \n",
    "                      color=config['color'], \n",
    "                      label='Raw')\n",
    "    \n",
    "    # Calculate and plot rolling average\n",
    "    window_size = 50\n",
    "    if len(metric_values) > window_size:\n",
    "        rolling_mean = np.convolve(metric_values, \n",
    "                                 np.ones(window_size)/window_size, \n",
    "                                 mode='valid')\n",
    "        ax.plot(range(window_size-1, len(metric_values)), \n",
    "                rolling_mean, \n",
    "                color=config['color'], \n",
    "                linewidth=2, \n",
    "                label='Moving Average')\n",
    "    \n",
    "    # Customize each subplot\n",
    "    ax.set_title(config['title'], pad=10)\n",
    "    ax.set_xlabel('Epoch', labelpad=10)\n",
    "    \n",
    "    # Adjust ylabel based on transformations\n",
    "    ylabel = config['ylabel']\n",
    "    if use_log_scale:\n",
    "        ylabel = f'log10({ylabel})'\n",
    "    if scale_factor != 1:\n",
    "        ylabel = f'{ylabel} (Ã—{int(scale_factor)})'\n",
    "    ax.set_ylabel(ylabel, labelpad=10)\n",
    "    \n",
    "    ax.grid(True, linestyle='--', alpha=0.7)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    \n",
    "    # Add legend if it's the total objective plot\n",
    "    if config['title'] == 'Total Objective':\n",
    "        ax.legend(loc='upper right')\n",
    "        \n",
    "    # Add note about scaling if applicable\n",
    "    if use_log_scale or scale_factor != 1:\n",
    "        transform_text = []\n",
    "        if use_log_scale:\n",
    "            transform_text.append('log scale')\n",
    "        if scale_factor != 1:\n",
    "            transform_text.append(f'Ã—{int(scale_factor)}')\n",
    "        ax.text(0.02, 0.98, f\"({', '.join(transform_text)})\", \n",
    "                transform=ax.transAxes, \n",
    "                fontsize=8, \n",
    "                verticalalignment='top',\n",
    "                bbox=dict(facecolor='white', alpha=0.8, edgecolor='none'))\n",
    "\n",
    "# Add overall title and subtitle with adjusted positions\n",
    "fig.suptitle('Training Metrics Over Time', \n",
    "            fontsize=16, \n",
    "            weight='bold', \n",
    "            y=0.98)\n",
    "\n",
    "subtitle = f'Î»_h = {lam_h}, Î»_l1 = {lam_l1}, Weights Learning Rate = {w_learning_rate}'\n",
    "fig.text(0.5, 0.93, \n",
    "         subtitle, \n",
    "         horizontalalignment='center',\n",
    "         fontsize=12,\n",
    "         style='italic')\n",
    "\n",
    "# Save and show the figure as a .pdf file at the specified location\n",
    "plt.savefig(os.path.join(save_path, 'training_metrics.pdf'), \n",
    "            bbox_inches='tight', \n",
    "            dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Create a separate figure for the adjacency matrices comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Use a better colormap - options:\n",
    "# 'YlOrBr' - Yellow-Orange-Brown (good for sparse matrices)\n",
    "# 'viridis' - Perceptually uniform, colorblind-friendly\n",
    "# 'Greys' - Black and white, professional\n",
    "# 'YlGnBu' - Yellow-Green-Blue, professional\n",
    "cmap = 'YlOrBr'  # Choose one of the above\n",
    "\n",
    "# Plot estimated adjacency matrix (now on the left)\n",
    "im1 = ax1.imshow(compute_binary_adjacency(W), cmap=cmap, interpolation='nearest')\n",
    "ax1.set_title('Estimated DAG', pad=10)\n",
    "plt.colorbar(im1, ax=ax1, fraction=0.046, pad=0.04)\n",
    "ax1.set_xlabel('Node', labelpad=10)\n",
    "ax1.set_ylabel('Node', labelpad=10)\n",
    "\n",
    "# Plot true adjacency matrix (now on the right)\n",
    "im2 = ax2.imshow(B_true, cmap=cmap, interpolation='nearest')\n",
    "ax2.set_title('True DAG', pad=10)\n",
    "plt.colorbar(im2, ax=ax2, fraction=0.046, pad=0.04)\n",
    "ax2.set_xlabel('Node', labelpad=10)\n",
    "ax2.set_ylabel('Node', labelpad=10)\n",
    "\n",
    "# Add overall title\n",
    "fig.suptitle('Estimated vs True DAG Structure', \n",
    "             fontsize=16, \n",
    "             weight='bold', \n",
    "             y=1.05)\n",
    "\n",
    "# Add grid lines to better separate the nodes\n",
    "for ax in [ax1, ax2]:\n",
    "    ax.set_xticks(np.arange(-.5, B_true.shape[0], 1), minor=True)\n",
    "    ax.set_yticks(np.arange(-.5, B_true.shape[0], 1), minor=True)\n",
    "    ax.grid(which=\"minor\", color=\"w\", linestyle='-', linewidth=0.3)\n",
    "    ax.tick_params(which=\"minor\", bottom=False, left=False)\n",
    "\n",
    "# Improve layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the comparison plot as a .pdf file at the specified location\n",
    "plt.savefig(os.path.join(save_path, 'dag_comparison.pdf'), \n",
    "            bbox_inches='tight', \n",
    "            dpi=300,\n",
    "            facecolor='white',\n",
    "            edgecolor='none')\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# Now use a threshold of 0.3 to binarize the weighted adjacency matrix W\n",
    "W_est = np.array(model.get_W())\n",
    "B_est = compute_binary_adjacency(W_est, threshold=0.3)\n",
    "\n",
    "# %%\n",
    "# Check if B_est is indeed a DAG\n",
    "def is_dag(adjacency_matrix):\n",
    "    \"\"\"\n",
    "    Check if a given adjacency matrix represents a Directed Acyclic Graph (DAG).\n",
    "    \n",
    "    Parameters:\n",
    "        adjacency_matrix (numpy.ndarray): A square matrix representing the adjacency of a directed graph.\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if the graph is a DAG, False otherwise.\n",
    "    \"\"\"\n",
    "    # Create a directed graph from the adjacency matrix\n",
    "    graph = nx.DiGraph(adjacency_matrix)\n",
    "    \n",
    "    # Check if the graph is a DAG\n",
    "    return nx.is_directed_acyclic_graph(graph)\n",
    "\n",
    "# Check if the estimated binary adjacency matrix B_est is a DAG\n",
    "is_dag_B_est = is_dag(B_est)\n",
    "print(f\"Is the estimated binary adjacency matrix a DAG? {is_dag_B_est}\")\n",
    "\n",
    "\n",
    "# Compute the h_reg term for the true weighted adjacency matrix W_true\n",
    "h_reg_true = compute_h_reg(W_true)\n",
    "print(f\"The h_reg term for the true weighted adjacency matrix W_true is: {h_reg_true:.4f}\")\n",
    "\n",
    "# Compute the h_reg term for the estimated weighted adjacency matrix W_est\n",
    "h_reg_est = compute_h_reg(W_est)\n",
    "print(f\"The h_reg term for the estimated weighted adjacency matrix W_est is: {h_reg_est:.4f}\")\n",
    "\n",
    "# print first 5 rows and columsn of W_est and round values to 4 decimal places and show as non-scientific notation\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "print(\"The first 5 rows and columns of the estimated weighted adjacency matrix W_est\\n{}\".format(W_est[:5, :5]))\n",
    "\n",
    "# now show the adjacency matrix of the true graph and the estimated graph side by side\n",
    "plot_adjacency_matrices(true_matrix=B_true, est_matrix=B_est, save_path=os.path.join(save_path, 'adjacency_matrices.png'))\n",
    "\n",
    "# print the number of edges in the true graph and the estimated graph\n",
    "print(f\"The number of edges in the true graph: {np.sum(B_true)}\")\n",
    "print(f\"The number of edges in the estimated graph: {np.sum(B_est)}\")\n",
    "\n",
    "# %%\n",
    "# plot est_dag and true_dag\n",
    "GraphDAG(B_est, B_true, save_name=os.path.join(save_path, 'est_dag_true_dag.png'))\n",
    "# calculate accuracy\n",
    "met_pcx = MetricsDAG(B_est, B_true)\n",
    "print(met_pcx.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the style and color palette\n",
    "sns.set(style=\"whitegrid\")\n",
    "palette = sns.color_palette(\"tab10\")\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(18, 5))  # Adjusting layout to 1 row and 3 columns\n",
    "fig.suptitle('Performance Metrics Over Epochs', fontsize=16, weight='bold')\n",
    "\n",
    "# Plot the MAE\n",
    "sns.lineplot(x=range(len(MAEs)), y=MAEs, ax=axs[0], color=palette[0])\n",
    "axs[0].set_title(\"Mean Absolute Error (MAE)\", fontsize=14)\n",
    "axs[0].set_xlabel(\"Epoch\", fontsize=12)\n",
    "axs[0].set_ylabel(\"MAE\", fontsize=12)\n",
    "axs[0].grid(True)\n",
    "\n",
    "# Plot the SHD\n",
    "sns.lineplot(x=range(len(SHDs)), y=SHDs, ax=axs[1], color=palette[2])\n",
    "axs[1].set_title(\"Structural Hamming Distance (SHD)\", fontsize=14)\n",
    "axs[1].set_xlabel(\"Epoch\", fontsize=12)\n",
    "axs[1].set_ylabel(\"SHD\", fontsize=12)\n",
    "axs[1].grid(True)\n",
    "\n",
    "# Plot the Energy\n",
    "sns.lineplot(x=range(len(pc_energies)), y=pc_energies, ax=axs[2], color=palette[3])\n",
    "axs[2].set_title(\"Energy\", fontsize=14)\n",
    "axs[2].set_xlabel(\"Epoch\", fontsize=12)\n",
    "axs[2].set_ylabel(\"Energy\", fontsize=12)\n",
    "axs[2].grid(True)\n",
    "\n",
    "# Improve layout and show the plot\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjust layout to fit the suptitle\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print first 5 rows and columsn of W_est and round values to 4 decimal places and show as non-scientific notation\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "print(\"The first 4 rows and columns of the estimated weighted adjacency matrix W_est\\n{}\".format(W_est[:11, :11]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now show the adjacency matrix of the true graph and the estimated graph side by side\n",
    "plot_adjacency_matrices(B_true, B_est)\n",
    "\n",
    "# print the number of edges in the true graph and the estimated graph\n",
    "print(f\"The number of edges in the true graph: {np.sum(B_true)}\")\n",
    "print(f\"The number of edges in the estimated graph: {np.sum(B_est)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot est_dag and true_dag\n",
    "GraphDAG(B_est, B_true)\n",
    "# calculate accuracy\n",
    "met_pcax = MetricsDAG(B_est, B_true)\n",
    "print(met_pcax.metrics)\n",
    "\n",
    "# print experiment name\n",
    "print(exp_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GOLEM learn\n",
    "#g = GOLEM(num_iter=2e4, lambda_2=1e4/X.shape[1], lambda_1=1e-2)\n",
    "#g = GOLEM(num_iter=2e4, lambda_2=0.0)  setting h_reg to 0 still allows model to be fit (no error thrown)\n",
    "g = GOLEM(num_iter=1e4, equal_variances=True, non_equal_variances=False, device_type='gpu') # F1 of 68%\n",
    "#g = GOLEM(num_iter=2e4, non_equal_variances=False) # F1 of 68%, default non_equal_variances=True\n",
    "g.learn(X)\n",
    "\n",
    "# plot est_dag and true_dag\n",
    "GraphDAG(g.causal_matrix, B_true)\n",
    "\n",
    "# calculate accuracy\n",
    "g_met = MetricsDAG(g.causal_matrix, B_true)\n",
    "print(g_met.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notears learn\n",
    "#nt = Notears(lambda1=0.1, h_tol=1e-5, max_iter=500, loss_type='l2') # default loss_type is 'l2', F1 of 74%\n",
    "nt = Notears(lambda1=0.1, h_tol=1e-5, max_iter=500, loss_type='laplace') # default loss_type is 'l2', F1 of 74%\n",
    "#nt = Notears(lambda1=1e-2, h_tol=1e-5, max_iter=5000) # default loss_type is 'l2', F1 of 74%\n",
    "#nt = Notears(lambda1=1e-2, h_tol=1e-5, max_iter=10000, loss_type='logistic')\n",
    "#nt = Notears(lambda1=1e-2, h_tol=1e-5, max_iter=10000, loss_type='poisson')\n",
    "nt.learn(X)\n",
    "\n",
    "# plot est_dag and true_dag\n",
    "GraphDAG(nt.causal_matrix, B_true)\n",
    "\n",
    "# calculate accuracy\n",
    "nt_met = MetricsDAG(nt.causal_matrix, B_true)\n",
    "print(nt_met.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peter & Clark (PC)\n",
    "# A variant of PC-algorithm, one of [`original`, `stable`, `parallel`]\n",
    "pc = PC(variant='parallel', alpha=0.03, ci_test='fisherz') # F1 of 74%\n",
    "#pc = PC(variant='stable', alpha=0.03) # F1 of 69%\n",
    "#pc = PC(variant='original', alpha=0.03) # F1 of 58%\n",
    "pc.learn(X)\n",
    "\n",
    "# plot est_dag and true_dag\n",
    "GraphDAG(pc.causal_matrix, B_true)\n",
    "\n",
    "# calculate accuracy\n",
    "pc_met = MetricsDAG(pc.causal_matrix, B_true)\n",
    "print(pc_met.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GES learn\n",
    "# method is one of ['r2', 'scatter']\n",
    "ges = GES() # F1 of 47%\n",
    "#ges = GES(criterion='bic', method='scatter') # F1 of 47%\n",
    "#ges = GES(criterion='bic', method='r2') # F1 of 4%\n",
    "\n",
    "# learn the graph structure\n",
    "ges.learn(X)\n",
    "\n",
    "# plot est_dag and true_dag\n",
    "GraphDAG(ges.causal_matrix, B_true)\n",
    "\n",
    "# calculate accuracy\n",
    "ges_met = MetricsDAG(ges.causal_matrix, B_true)\n",
    "print(ges_met.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DirectLiNGAM learn\n",
    "# measure : {'pwling', 'kernel'}, default='pwling'\n",
    "g_dlingam = DirectLiNGAM(measure='pwling') # F1 of 74%\n",
    "#g = DirectLiNGAM(measure='kernel') # F1 of nan - takes too long to run (even after 15 minutes not done)\n",
    "g_dlingam.learn(X)\n",
    "\n",
    "# plot est_dag and true_dag\n",
    "GraphDAG(g_dlingam.causal_matrix, B_true)\n",
    "\n",
    "# calculate accuracy\n",
    "g_dlingam_met = MetricsDAG(g_dlingam.causal_matrix, B_true)\n",
    "print(g_dlingam_met.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ICALiNGAM learn\n",
    "# max_iter : int, optional (default=1000)\n",
    "#g = ICALiNGAM(max_iter=1000) # F1 of 48%\n",
    "g_ICAlingam = ICALiNGAM(max_iter=1000) # F1 of 51%\n",
    "g_ICAlingam.learn(X)\n",
    "\n",
    "# plot est_dag and true_dag\n",
    "GraphDAG(g_ICAlingam.causal_matrix, B_true)\n",
    "\n",
    "# calculate accuracy\n",
    "g_ICAlingam_met = MetricsDAG(g_ICAlingam.causal_matrix, B_true)\n",
    "print(g_ICAlingam_met.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LiM from lingam library\n",
    "LiM = lingam.LiM(w_threshold=0.3, max_iter=500, h_tol=1e-5, rho_max=1e12, lambda1=0.3)\n",
    "# create array of shape (1, n_features) which indicates which columns/variables in data.values are discrete or continuous variables, where \"1\" indicates a continuous variable, while \"0\" a discrete variable.\n",
    "is_cont = np.array([(1 if data[col].nunique() > 2 else 0) for col in data.columns]).reshape(1, -1)\n",
    "#LiM.fit(X=data.values, dis_con=is_cont, only_global=False) # does not finish, even fater 375 minutes\n",
    "LiM.fit(X=data.values, dis_con=is_cont, only_global=True)\n",
    "\n",
    "# plot est_dag and true_dag\n",
    "LiM_W_est = LiM.adjacency_matrix_\n",
    "LiM_B_est = compute_binary_adjacency(LiM_W_est)\n",
    "\n",
    "GraphDAG(LiM_B_est, B_true)\n",
    "\n",
    "# calculate accuracy\n",
    "LiM_met = MetricsDAG(LiM_B_est, B_true)\n",
    "print(LiM_met.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### imports\n",
    "import os\n",
    "import csv\n",
    "import logging\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from datetime import datetime\n",
    "from itertools import combinations, product, chain\n",
    "from math import floor\n",
    "from multiprocessing import Pool, RawArray\n",
    "from scipy.spatial import KDTree\n",
    "from scipy.special import digamma\n",
    "from scipy.stats import rankdata\n",
    "\n",
    "### manual added implementation of mCMIkNN\n",
    "\n",
    "class IndependenceTest(ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def test_params(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def compute_pval(self, x, y, z):\n",
    "        pass\n",
    "    \n",
    "class mCMIkNN(IndependenceTest):\n",
    "    '''\n",
    "        An independence test class that provides acces to the following non-parametric methods described in Huegle et al. (2022)\n",
    "        - compute_mi: non-parametric estimator for mutual information I(X;Y)\n",
    "        - compute_cmi: non-parametric estimator for conditional mutual information I(X;Y|Z)\n",
    "        - compute_pval_mi: non-parametric independence test returning p value for H_0: X _||_ Y\n",
    "        - compute_pval: non-parametric conditional independence test returing p value for H_0: X _||_ Y | Z\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 kcmi=25,\n",
    "                 kperm=5,\n",
    "                 Mperm=100,\n",
    "                 subsample=None,\n",
    "                 transform=None,\n",
    "                 log_warning=False):\n",
    "\n",
    "        # Required parameters with defaults\n",
    "        self.Mperm = Mperm\n",
    "        self.kcmi = kcmi\n",
    "        self.kperm = kperm\n",
    "        # Persisted values\n",
    "        self.cmi_val = None\n",
    "        self.null_distribution = None\n",
    "        self.permutation = None\n",
    "        self.pval = None\n",
    "        # Options\n",
    "        self.transform = transform\n",
    "        self.dis = 10\n",
    "        self.subsample = subsample\n",
    "        self.leafsize = 16\n",
    "        self.log_warning = log_warning\n",
    "\n",
    "    def test_params(self):\n",
    "        return {\n",
    "            'kcmi': self.kcmi,\n",
    "            'kperm':self.kperm,\n",
    "            'Mperm': self.Mperm,\n",
    "            'Transformation': self.transform,\n",
    "        }\n",
    "\n",
    "    def rank_transform(self, x, y, z=None):\n",
    "        '''\n",
    "            Rank Transform all variables while preserving rank of discrete points.\n",
    "        '''\n",
    "        x_transformed = rankdata(x, method='dense', axis=0).astype(np.float32)\n",
    "        y_transformed = rankdata(y, method='dense', axis=0).astype(np.float32)\n",
    "        z_transformed = None if np.all(z) == None else rankdata(z, method='dense', axis=0).astype(np.float32)\n",
    "        return (x_transformed, y_transformed, z_transformed)\n",
    "\n",
    "    def uniform_transform(self, x, y, z=None):\n",
    "        '''\n",
    "            Transform all variables to take values in [0,1] with equal distances while preserving discrete points (i.e., normalized rank transformed variables).\n",
    "        '''\n",
    "        x_transformed = self.normalize(rankdata(x, method='dense', axis=0).astype(np.float32))\n",
    "        y_transformed = self.normalize(rankdata(y, method='dense', axis=0).astype(np.float32))\n",
    "        z_transformed = None if np.all(z) == None else self.normalize(rankdata(z, method='dense', axis=0).astype(np.float32))\n",
    "        return (x_transformed, y_transformed, z_transformed)\n",
    "\n",
    "    def standardize(self, x):\n",
    "        x_mean = np.mean(x, axis=0)\n",
    "        x_std = np.std(x, axis=0)\n",
    "        return (x - x_mean) / x_std\n",
    "\n",
    "    def standard_transform(self, x, y, z=None):\n",
    "        '''\n",
    "            Standardize all continuous variables (unique values > 10) to std. normal distribution,\n",
    "            i.e., (x-mean(x))/std(x)\n",
    "        '''\n",
    "        res = []\n",
    "        # Standardize Xa if unique(Xa) > 10\n",
    "        xN = []\n",
    "        for a in x.T:\n",
    "            if np.unique(a, axis=0).shape[0] > self.dis:\n",
    "                xN.append(self.standardize(a))\n",
    "            else:\n",
    "                xN.append(a)\n",
    "        res.append(np.asarray(xN).T)\n",
    "\n",
    "        # Standardize Ya if unique(Ya) > 10\n",
    "        yN = []\n",
    "        for a in y.T:\n",
    "            if np.unique(a, axis=0).shape[0] > self.dis:\n",
    "                yN.append(self.standardize(a))\n",
    "            else:\n",
    "                yN.append(a)\n",
    "        res.append(np.asarray(yN).T)\n",
    "\n",
    "        # Standardize Za if unique(Za) > 10\n",
    "        zN = []\n",
    "        if np.all(z) != None:\n",
    "            for a in z.T:\n",
    "                if np.unique(a, axis=0).shape[0] > self.dis:\n",
    "                    zN.append(self.standardize(a))\n",
    "                else:\n",
    "                    zN.append(a)\n",
    "            res.append(np.asarray(zN).T)\n",
    "        else:\n",
    "            res.append(None)\n",
    "        return res\n",
    "\n",
    "    def normalize(self, x):\n",
    "        x_min = np.min(x, axis=0)\n",
    "        x_max = np.max(x, axis=0)\n",
    "        return (x-x_min)/(x_max-x_min)\n",
    "\n",
    "    def normal_transform(self, x, y, z=None):\n",
    "        '''\n",
    "            Normalize all continuous variables (unique values > 10) to take values in [0,1],\n",
    "            i.e., (x-min(x))/(max(x)-min(x))\n",
    "        '''\n",
    "        res = []\n",
    "        # Normalize Xa if unique(Xa) > 10\n",
    "        xN = []\n",
    "        for a in x.T:\n",
    "            if np.unique(a, axis=0).shape[0] > self.dis:\n",
    "                xN.append(self.normalize(a))\n",
    "            else:\n",
    "                xN.append(a)\n",
    "        res.append(np.asarray(xN).T)\n",
    "\n",
    "        # Normalize Ya if unique(Ya) > 10\n",
    "        yN = []\n",
    "        for a in y.T:\n",
    "            if np.unique(a, axis=0).shape[0] > self.dis:\n",
    "                yN.append(self.normalize(a))\n",
    "            else:\n",
    "                yN.append(a)\n",
    "        res.append(np.asarray(yN).T)\n",
    "\n",
    "        # Normalize Za if unique(Za) > 10\n",
    "        zN = []\n",
    "        if np.all(z) != None:\n",
    "            for a in z.T:\n",
    "                if np.unique(a, axis=0).shape[0] > self.dis:\n",
    "                    zN.append(self.normalize(a))\n",
    "                else:\n",
    "                    zN.append(a)\n",
    "            res.append(np.asarray(zN).T)\n",
    "        else:\n",
    "            res.append(None)\n",
    "        return res\n",
    "\n",
    "    def transform_data(self, x, y, z=None):\n",
    "        if self.transform == 'rank':\n",
    "            return self.rank_transform(x, y, z)\n",
    "        elif self.transform == 'standardize':\n",
    "            return self.standard_transform(x, y, z)\n",
    "        elif self.transform == 'normalize':\n",
    "            return self.normal_transform(x, y, z)\n",
    "        elif self.transform == 'uniform':\n",
    "            return self.uniform_transform(x, y, z)\n",
    "        return (x, y, z)\n",
    "\n",
    "    def count_NN(self, tree, points, rho):\n",
    "        '''\n",
    "            Count all nearest neighbors with distance smaller or equal to rho. (Note, this does not include point itself.)\n",
    "        '''\n",
    "        return tree.query_ball_point(points, rho, p=np.inf, return_length=True)-1\n",
    "\n",
    "\n",
    "    def return_NN(self, tree, points, sigma):\n",
    "        '''\n",
    "            Return all nearest neighbors with distance smaller or equal to sigma. (Note, excludes point itself.)\n",
    "        '''\n",
    "        # includes points itself\n",
    "        neighbors = tree.query_ball_point(points, sigma, p=np.inf)\n",
    "        # exclude points withing neighborhood\n",
    "        for i in range(len(points)):\n",
    "            neighbors[i].remove(i)\n",
    "\n",
    "        return neighbors\n",
    "\n",
    "\n",
    "    def compute_mi(self, x, y):\n",
    "        '''\n",
    "            Estimate the mutual information I(X;Y) of X and Y from n samples (x_i, y_i)_{i=1}^n\n",
    "            using Alg. 1 which relates to the *Mixed-KSG* mutual information estimator of Gao et al. (2017)\n",
    "\n",
    "            Note: Using digamma instead of log according to Mesner et al. (2021)\n",
    "\n",
    "            Input:  x: 2D array of size n*dz (or 1D list of size n if dx = 1)\n",
    "                    y: 2D array of size n*dz (or 1D list of size n if dy = 1)\n",
    "                    (self.kcmi: k-nearest neighbor parameter)\n",
    "\n",
    "            Output: ^I_n(X;Y)\n",
    "        '''\n",
    "        assert len(x) == len(y), \"x and y should have the same number of observations\"\n",
    "        n = len(x)\n",
    "        assert self.kcmi <= n-1, \"Set kcmi smaller than number of observations - 1\"\n",
    "\n",
    "        x =  x.reshape((n, 1)).astype(np.float32) if (x.shape == (n,)) else x.astype(np.float32)\n",
    "        y =  y.reshape((n, 1)).astype(np.float32) if (y.shape == (n,)) else y.astype(np.float32)\n",
    "        xy = np.concatenate((x, y), axis=1)\n",
    "\n",
    "        # build k-d trees\n",
    "        tree_xy = KDTree(xy, leafsize=self.leafsize)\n",
    "        tree_x = KDTree(x, leafsize=self.leafsize)\n",
    "        tree_y = KDTree(y, leafsize=self.leafsize)\n",
    "\n",
    "        # compute k-NN distances, using k+1 as this includes dist to self\n",
    "        rho = tree_xy.query(xy, self.kcmi+1, p=np.inf)[0][:, self.kcmi]\n",
    "\n",
    "        # if continous  -> k_tilde = k_cmi\n",
    "        # if discrete or mixed -> k_tilde = number of samples with distance rho\n",
    "        k_tilde = self.count_NN(tree_xy, xy, rho)\n",
    "\n",
    "        # entropy estimates - i.e., count points withon distance rho\n",
    "        nx = self.count_NN(tree_x, x, rho)\n",
    "        ny = self.count_NN(tree_y, y, rho)\n",
    "\n",
    "        mi = np.mean(digamma(k_tilde) + digamma(n) - digamma(nx) - digamma(ny))\n",
    "        return max(0,mi)\n",
    "\n",
    "    def compute_cmi(self, x, y, z):\n",
    "        '''\n",
    "            Estimate the conditional mutual information I(X;Y|Z) of X and Y given a dz-dimensional variable Z from samples (x_i, y_i,z_i)_{i=1}^n\n",
    "            Using Alg. 1 which relates to the *Mixed-KSG* mutual information estimator of Mesner et al. (2021)\n",
    "\n",
    "            Input:  x: 2D array of size n*dz (or 1D list of size n if dx = 1)\n",
    "                    y: 2D array of size n*dz (or 1D list of size n if dy = 1)\n",
    "                    z: 2D array of size n*dz (or 1D list of size n if dz = 1)\n",
    "                    (self.kcmi: k-nearest neighbor parameter)\n",
    "\n",
    "            Output: ^I_n(X;Y|Z)\n",
    "        '''\n",
    "        assert len(x) == len(y) == len(z), \"x, y, and z should have same number of observations\"\n",
    "        n=len(x)\n",
    "        assert self.kcmi <= n-1, \"Set kcmi smaller than number of observations - 1\"\n",
    "\n",
    "        x =  x.reshape((n, 1)).astype(np.float32) if (x.shape == (n,)) else x.astype(np.float32)\n",
    "        y =  y.reshape((n, 1)).astype(np.float32) if (y.shape == (n,)) else y.astype(np.float32)\n",
    "        z =  z.reshape((n, 1)).astype(np.float32) if (z.shape == (n,)) else z.astype(np.float32)\n",
    "\n",
    "        yz = np.concatenate((y, z), axis=1)\n",
    "        xyz = np.concatenate((x, yz), axis=1)\n",
    "        xz = np.concatenate((x, z), axis=1)\n",
    "\n",
    "        # build k-d trees\n",
    "        tree_xyz = KDTree(xyz, leafsize= self.leafsize)\n",
    "        tree_xz = KDTree(xz, leafsize= self.leafsize)\n",
    "        tree_yz = KDTree(yz, leafsize= self.leafsize)\n",
    "        tree_z = KDTree(z, leafsize= self.leafsize)\n",
    "\n",
    "        # compute k-NN distances, using k+1 as this includes dist to self\n",
    "        rho = tree_xyz.query(xyz, self.kcmi+1, p=np.inf)[0][:, self.kcmi]\n",
    "\n",
    "        # if continous  -> k_tilde = k_cmi\n",
    "        # if discrete or mixed -> k_tilde = number of samples with distance rho\n",
    "        k_tilde = self.count_NN(tree_xyz, xyz, rho)\n",
    "\n",
    "        # entropy estimates - i.e., count neighbors within distance rho\n",
    "        nxz = self.count_NN(tree_xz, xz, rho)\n",
    "        nyz = self.count_NN(tree_yz, yz, rho)\n",
    "        nz = self.count_NN(tree_z, z, rho)\n",
    "\n",
    "        cmi = np.mean(digamma(k_tilde) - digamma(nxz) - digamma(nyz) + digamma(nz))\n",
    "        return max(0,cmi)\n",
    "\n",
    "    def compute_pval_mi(self, x, y):\n",
    "        '''\n",
    "            Returns the p value returning p value for H_0: X _||_ Y estimated from n samples (x_i, y_i)_{i=1}^n\n",
    "            using Alg. 2 of Huegle et al. (2022), i.e., comparing the present MI against MIs for shuffled samples of X under H_0.\n",
    "\n",
    "            H_0: X and Y are independent\n",
    "            H_1: X and Y are dependent\n",
    "\n",
    "            Note: Using rank transformation for k-NN searches according to Runge (2017) but preserving ties\n",
    "\n",
    "            Input:  x: 2D array of size n*dz (or 1D list of size n if dx = 1)\n",
    "                    y: 2D array of size n*dz (or 1D list of size n if dy = 1)\n",
    "                    (self.Mperm: number of permutations)\n",
    "                    (self.kcmi: k used for MI estimation)\n",
    "\n",
    "            Output: p_perm,n\n",
    "        '''\n",
    "\n",
    "        assert len(x) == len(y), \"x and y should have same number of observations\"\n",
    "\n",
    "        if self.subsample is not None:\n",
    "            sample = np.random.choice(np.arange(len(x)), min(len(x), self.subsample), replace=False)\n",
    "            x, y = x[sample], y[sample]\n",
    "\n",
    "        n = len(x)\n",
    "\n",
    "        x =  x.reshape((n, 1)).astype(np.float32) if (x.shape == (n,)) else x.astype(np.float32)\n",
    "        y =  y.reshape((n, 1)).astype(np.float32) if (y.shape == (n,)) else y.astype(np.float32)\n",
    "\n",
    "        x, y, _ = self.transform_data(x,y)\n",
    "\n",
    "        n = len(x)\n",
    "        if self.kperm == 0:\n",
    "            self.kperm = np.floor(np.sqrt(n)).astype(int)\n",
    "        elif 0 < self.kperm <= 1:\n",
    "            self.kperm = np.floor(np.nextafter(self.kperm,0) * n).astype(int) # ensure kperm < n (n-1 equals shuffling without considering z)\n",
    "\n",
    "        if self.kcmi == 0:\n",
    "            self.kcmi = np.floor(np.sqrt(n)).astype(int)\n",
    "        elif 0 < self.kcmi <= 1:\n",
    "            self.kcmi = np.floor(np.nextafter(self.kcmi,0) * n).astype(int) # ensure kcmi < n\n",
    "\n",
    "        # estimate present MI value\n",
    "        self.cmi_val = self.compute_mi(x, y)\n",
    "\n",
    "        # estimate Mperm MIs for shuffled X under H_0\n",
    "        null_dist = np.zeros(self.Mperm)\n",
    "        for m in range(self.Mperm):\n",
    "             # Generate random shuffled x\n",
    "            x_shuffled = x[np.random.default_rng().permutation(n)]\n",
    "            null_dist[m] = self.compute_mi(x_shuffled, y)\n",
    "\n",
    "        # estimate pvalue comparing MI against MIs for shuffled X\n",
    "        self.null_distribution = null_dist\n",
    "        self.pval = (1+np.sum(null_dist >= self.cmi_val))/(1+self.Mperm)\n",
    "        return self.pval\n",
    "\n",
    "    def compute_pval(self, x, y, z=None):\n",
    "        '''\n",
    "            Returns the p value returning p value for H_0: X _||_ Y | Z estimated from n samples (x_i, y_i,z_i)_{i=1}^n\n",
    "            using Alg. 2 of Huegle et al. (2022), i.e., comparing the present MI against MIs for shuffled samples of X under H_0.\n",
    "\n",
    "            H_0: X and Y are independent given dz dimensional Z\n",
    "            H_1: X and Y are dependent given dz dimensional Z\n",
    "\n",
    "            Note: Using rank transformation for k-NN searches according to Runge (2017) but preserving ties\n",
    "\n",
    "            Input:  x: 1D 2D array of size n*dz (or 1D list of size n if dx = 1)\n",
    "                    y: 1D 2D array of size n*dz (or 1D list of size n if dy = 1)\n",
    "                    z: 2D array of size n*dz (or 1D list of size n if dz = 1)\n",
    "                    (self.Mperm: number of permutations)\n",
    "                    (self.kperm: k used for local permuation scheme)\n",
    "                    (self.kcmi: k used for MI estimation)\n",
    "\n",
    "            Output: p_perm,n\n",
    "        '''\n",
    "\n",
    "        # for empty z calculate return p value according to H_0: X _||_ Y\n",
    "        if z is None:\n",
    "            return self.compute_pval_mi(x, y)\n",
    "\n",
    "        assert len(x) == len(y) == len(z), \"x, y, and z should have same number of observations\"\n",
    "\n",
    "        if self.subsample is not None:\n",
    "            sample = np.random.choice(np.arange(len(x)), min(len(x), self.subsample), replace=False)\n",
    "            x, y, z = x[sample], y[sample], z[sample]\n",
    "\n",
    "        n = len(x)\n",
    "\n",
    "        x =  x.reshape((n, 1)).astype(np.float32) if (x.shape == (n,)) else x.astype(np.float32)\n",
    "        y =  y.reshape((n, 1)).astype(np.float32) if (y.shape == (n,)) else y.astype(np.float32)\n",
    "        z =  z.reshape((n, 1)).astype(np.float32) if (z.shape == (n,)) else z.astype(np.float32)\n",
    "        x, y, z = self.transform_data(x, y, z)\n",
    "\n",
    "        if self.kperm == 0:\n",
    "            self.kperm = np.floor(np.sqrt(n)).astype(int)\n",
    "        elif 0 < self.kperm <= 1:\n",
    "            self.kperm = np.floor(np.nextafter(self.kperm,0) * n).astype(int) # ensure kperm < n (n-1 equals shuffling without considering z)\n",
    "\n",
    "        if self.kcmi == 0:\n",
    "            self.kcmi = np.floor(np.sqrt(n)).astype(int)\n",
    "        elif 0 < self.kcmi <= 1:\n",
    "            self.kcmi = np.floor(np.nextafter(self.kcmi,0) * n).astype(int) # ensure kcmi < n\n",
    "\n",
    "        # estimate present CMI value\n",
    "        self.cmi_val = self.compute_cmi(x, y, z)\n",
    "\n",
    "        # Get nearest neighbors around each sample point in Z\n",
    "        tree_z = KDTree(z, leafsize= self.leafsize)\n",
    "\n",
    "        # compute k-NN distances in Z, using k+1 as this includes dist to self\n",
    "        sigma = tree_z.query(z, self.kperm+1, p=np.inf)[0][:, self.kperm]\n",
    "\n",
    "        # if continuous -> k points distance smaller or equal to sigma excluding the point itself\n",
    "        # if discrete or mixed -> all points with distance smaller or euqla to the k-NN distance sigma excluding the point itself\n",
    "        neighbors = self.return_NN(tree_z, z, sigma)\n",
    "\n",
    "        # estimate Mperm CMIs for shuffled X under H_0 while preserving marginal distributions\n",
    "        null_dist = np.zeros(self.Mperm)\n",
    "        for m in range(self.Mperm):\n",
    "            # compose local permutations of nearest neighbors to receive a restricted permutation of the whole index list\n",
    "            permutation = np.arange(n)\n",
    "            for i in range(n-1,-1,-1):\n",
    "                permutation[neighbors[i,]]=permutation[np.random.default_rng().permutation(neighbors[i,])]\n",
    "            x_shuffled = x[permutation]\n",
    "            null_dist[m] = self.compute_cmi(x_shuffled, y, z)\n",
    "\n",
    "        self.null_distribution = null_dist\n",
    "        self.pval = (1+np.sum(null_dist >= self.cmi_val))/(1+self.Mperm)\n",
    "        return self.pval\n",
    "\n",
    "\n",
    "### parallel pc alg\n",
    "# A global dictionary storing the variables passed from the initializer.\n",
    "var_dict = {}\n",
    "\n",
    "\n",
    "def _init_worker(data, data_shape, graph, vertices, test, alpha):\n",
    "    # Using a dictionary is not strictly necessary. You can also\n",
    "    # use global variables.\n",
    "    var_dict['data'] = data\n",
    "    var_dict['data_shape'] = data_shape\n",
    "\n",
    "    var_dict['graph'] = graph\n",
    "    var_dict['vertices'] = vertices\n",
    "\n",
    "    var_dict['alpha'] = alpha\n",
    "    var_dict['test'] = test\n",
    "\n",
    "\n",
    "def _test_worker(i, j, lvl):\n",
    "    test = var_dict['test']\n",
    "    alpha = var_dict['alpha']\n",
    "    data_arr = np.frombuffer(var_dict['data']).reshape(var_dict['data_shape'])\n",
    "    graph = np.frombuffer(var_dict['graph'], dtype=\"int32\").reshape((var_dict['vertices'],\n",
    "                                                                     var_dict['vertices']))\n",
    "    \n",
    "    # unconditional\n",
    "    if lvl < 1:\n",
    "        p_val = test.compute_pval(data_arr[:, [i]], data_arr[:, [j]], z=None)\n",
    "        if (p_val > alpha):\n",
    "            return (i, j, p_val, [])\n",
    "    # conditional\n",
    "    else:\n",
    "        candidates_1 = np.arange(var_dict['vertices'])[(graph[i] == 1)]\n",
    "        candidates_1 = np.delete(candidates_1, np.argwhere((candidates_1==i) | (candidates_1==j)))\n",
    "\n",
    "        if (len(candidates_1) < lvl):\n",
    "            return None\n",
    "        \n",
    "        for S in [list(c) for c in combinations(candidates_1, lvl)]:\n",
    "            p_val = test.compute_pval(data_arr[:, [i]], data_arr[:, [j]], z=data_arr[:, list(S)])\n",
    "            if (p_val > alpha):\n",
    "                return (i, j, p_val, list(S))\n",
    "            \n",
    "    return None\n",
    "\n",
    "\n",
    "def _unid(g, i, j):\n",
    "    return g.has_edge(i, j) and not g.has_edge(j, i)\n",
    "\n",
    "\n",
    "def _bid(g, i, j):\n",
    "    return g.has_edge(i, j) and g.has_edge(j, i)\n",
    "\n",
    "\n",
    "def _adj(g, i, j):\n",
    "    return g.has_edge(i, j) or g.has_edge(j, i)\n",
    "\n",
    "\n",
    "def rule1(g, j, k):\n",
    "    for i in g.predecessors(j):\n",
    "        # i -> j s.t. i not adjacent to k\n",
    "        if _unid(g, i, j) and not _adj(g, i, k):\n",
    "            g.remove_edge(k, j)\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def rule2(g, i, j):\n",
    "    for k in g.successors(i):\n",
    "        # i -> k -> j\n",
    "        if _unid(g, k, j) and _unid(g, i, k):\n",
    "            g.remove_edge(j, i)\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def rule3(g, i, j):\n",
    "    for k, l in combinations(g.predecessors(j), 2):\n",
    "        # i <-> k -> j and i <-> l -> j s.t. k not adjacent to l\n",
    "        if (not _adj(g, k, l) and _bid(g, i, k) and _bid(g, i, l) and _unid(g, l, j) and _unid(g, k, j)):\n",
    "            g.remove_edge(j, i)\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def rule4(g, i, j):\n",
    "    for l in g.predecessors(j):\n",
    "        for k in g.predecessors(l):\n",
    "            # i <-> k -> l -> j s.t. k not adjacent to j and i adjacent to l\n",
    "            if (not _adj(g, k, j) and _adj(g, i, l) and _unid(g, k, l) and _unid(g, l, j) and _bid(g, i, k)):\n",
    "                g.remove_edge(j, i)\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def _direct_edges(graph, sepsets):\n",
    "    digraph = nx.DiGraph(graph)\n",
    "    for i in graph.nodes():\n",
    "        for j in nx.non_neighbors(graph, i):\n",
    "            for k in nx.common_neighbors(graph, i, j):\n",
    "                sepset = sepsets[(i, j)] if (i, j) in sepsets else []\n",
    "                if k not in sepset:\n",
    "                    if (k, i) in digraph.edges() and (i, k) in digraph.edges():\n",
    "                        digraph.remove_edge(k, i)\n",
    "                    if (k, j) in digraph.edges() and (j, k) in digraph.edges():\n",
    "                        digraph.remove_edge(k, j)\n",
    "\n",
    "    bidirectional_edges = [(i, j) for i, j in digraph.edges if digraph.has_edge(j, i)]\n",
    "    for i, j in bidirectional_edges:\n",
    "        if _bid(digraph, i, j):\n",
    "            continue\n",
    "        if (rule1(digraph, i, j) or rule2(digraph, i, j) or rule3(digraph, i, j) or rule4(digraph, i, j)):\n",
    "            continue\n",
    "\n",
    "    return digraph\n",
    "\n",
    "\n",
    "def parallel_stable_pc(data, estimator, alpha=0.05, processes=1, max_level=None):\n",
    "    cols = data.columns\n",
    "    cols_map = np.arange(len(cols))\n",
    "\n",
    "    data_raw = RawArray('d', data.shape[0] * data.shape[1])\n",
    "    # Wrap X as an numpy array so we can easily manipulates its data.\n",
    "    data_arr = np.frombuffer(data_raw).reshape(data.shape)\n",
    "    # Copy data to our shared array.\n",
    "    np.copyto(data_arr, data.values)\n",
    "\n",
    "    # same magic as for data\n",
    "    vertices = len(cols)\n",
    "    graph_raw = RawArray('i', np.ones(vertices*vertices).astype(int))\n",
    "    graph = np.frombuffer(graph_raw, dtype=\"int32\").reshape((vertices, vertices))\n",
    "    sepsets = {}\n",
    "\n",
    "    lvls = range((len(cols) - 1) if max_level is None else min(len(cols)-1, max_level+1))\n",
    "    for lvl in lvls:\n",
    "        configs = [(i, j, lvl) for i, j in product(cols_map, cols_map) if i != j and graph[i][j] == 1]\n",
    "\n",
    "        logging.info(f'Starting level {lvl} pool with {len(configs)} remaining edges at {datetime.now()}')\n",
    "        with Pool(processes=processes, initializer=_init_worker,\n",
    "                  initargs=(data_raw, data.shape, graph_raw, vertices, estimator, alpha)) as pool:\n",
    "            result = pool.starmap(_test_worker, configs)\n",
    "\n",
    "        for r in result:\n",
    "            if r is not None:\n",
    "                graph[r[0]][r[1]] = 0\n",
    "                graph[r[1]][r[0]] = 0\n",
    "                sepsets[(r[0], r[1])] = {'p_val': r[2], 'sepset': r[3]}\n",
    "\n",
    "    nx_graph = nx.from_numpy_array(graph)\n",
    "    nx_graph.remove_edges_from(nx.selfloop_edges(nx_graph))\n",
    "    nx_digraph = _direct_edges(nx_graph, sepsets)\n",
    "    nx.relabel_nodes(nx_digraph, lambda i: cols[i], copy=False)\n",
    "    sepsets = {(cols[k[0]], cols[k[1]]): {'p_val': v['p_val'], 'sepset': [cols[e] for e in v['sepset']]}\n",
    "               for k, v in sepsets.items()}\n",
    "\n",
    "    return nx_digraph, sepsets\n",
    "\n",
    "\n",
    "### parameters\n",
    "# set alpha\n",
    "alpha = 0.05 # significance level, increasing it will increase the number of edges\n",
    "# set number of cores\n",
    "processes = 200\n",
    "# set mCMIkNN parameters\n",
    "kcmi = 25 # number of nearest neighbors to find for mutual information estimation\n",
    "kperm = 5 # neighborhood size for local permutation scheme\n",
    "Mperm = 100 # linear reduction in computational cost, total number of permutations to compute for p-value estimation\n",
    "#subsample = 1000 # 20 mins\n",
    "subsample = 500\n",
    "# set maximum level of pcalg (None == infinite)\n",
    "max_level = None # limits the maximum size of the conditioning set\n",
    "\n",
    "# Create an instance of the mCMIkNN class\n",
    "indep_test = mCMIkNN(kcmi=kcmi, kperm=kperm, Mperm=Mperm, subsample=subsample)\n",
    "\n",
    "# Run the parallel_stable_pc algorithm\n",
    "#graph, sepsets = parallel_stable_pc(data, indep_test, alpha=alpha, processes=processes, max_level=max_level)\n",
    "# convert X to a pandas DataFrame\n",
    "X_df = pd.DataFrame(X)\n",
    "graph, sepsets = parallel_stable_pc(X_df, indep_test, alpha=alpha, processes=processes, max_level=max_level)\n",
    "\n",
    "# plot est_dag and true_dag\n",
    "mCMIkNN_B_est = nx.to_numpy_array(graph)\n",
    "\n",
    "GraphDAG(mCMIkNN_B_est, B_true)\n",
    "\n",
    "# calculate accuracy\n",
    "mCMIkNN_met = MetricsDAG(mCMIkNN_B_est, B_true)\n",
    "\n",
    "print(mCMIkNN_met.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################### EVALUATION METRICS #####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(B_true, B_est, interv_node_idx, target_node_idx, model_name):\n",
    "\n",
    "    # Convert adjacency matrices to binary (int8) for compatibility with `gadjid` metrics\n",
    "    B_true_binary = (B_true > 0).astype(np.int8)\n",
    "    B_est_binary = (B_est > 0).astype(np.int8)\n",
    "\n",
    "    # Compute Structural Hamming Distance (SHD)\n",
    "    SHD_normalized, SHD_absolute = compute_SHD(B_true_binary, B_est_binary)\n",
    "    print(f\"{model_name} - Structural Hamming Distance (SHD): {SHD_absolute}\")\n",
    "    print(f\"{model_name} - Normalized SHD: {SHD_normalized}\")\n",
    "\n",
    "    # Compute Parent Adjustment Distance (SID)\n",
    "    SID_normalized, SID_absolute = compute_SID(B_true_binary, B_est_binary)\n",
    "    print(f\"{model_name} - Structural Intervention Distance (SID): {SID_absolute}\")\n",
    "    print(f\"{model_name} - Normalized SID: {SID_normalized}\")\n",
    "\n",
    "    # Compute Ancestor Adjustment Distance (AID)\n",
    "    AID_normalized, AID_absolute = compute_ancestor_AID(B_true_binary, B_est_binary)\n",
    "    print(f\"{model_name} - Ancestor Adjustment Distance (AID): {AID_absolute}\")\n",
    "    print(f\"{model_name} - Normalized AID: {AID_normalized}\")\n",
    "\n",
    "    # Compute F1 score\n",
    "    F1_our = compute_F1_directed(B_true, B_est)\n",
    "    print(f\"{model_name} - F1 Score: {F1_our}\")\n",
    "\n",
    "def evaluate_effect(W_true, W_est, interv_node_idx, target_node_idx, model_name):\n",
    "    # Compute Total Effect Estimation Error (TEE)\n",
    "    true_total_effect, est_total_effect, TEE = compute_TEE(W_true, W_est, interv_node_idx, target_node_idx)\n",
    "    _B_true = (np.abs(W_true) > 0).astype(int)\n",
    "    auroc, auprc = compute_AUROC(_B_true, W_est)[-1], compute_AUPRC(_B_true, W_est)[-1]\n",
    "    print(f\"{model_name} - AUROC: {auroc}\")\n",
    "    print(f\"{model_name} - AUPRC: {auprc}\")\n",
    "    print(f\"{model_name} - True Total Effect: {true_total_effect}\")\n",
    "    print(f\"{model_name} - Estimated Total Effect: {est_total_effect}\")\n",
    "    print(f\"{model_name} - Total Effect Estimation Error (TEE): {TEE}\")\n",
    "\n",
    "# Example usage\n",
    "# Indices for treatment (intervention) and target variables\n",
    "interv_node_idx = 0\n",
    "target_node_idx = 11\n",
    "evaluate_model(B_true, B_est, interv_node_idx, target_node_idx, \"Our (PC)\")\n",
    "print()\n",
    "evaluate_effect(W_true, W_est, interv_node_idx, target_node_idx, \"Our (PC)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now do same for PC model which has its B_est stored in pc.causal_matrix\n",
    "evaluate_model(B_true, pc.causal_matrix, interv_node_idx, target_node_idx, \"Peter Clark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTEARS with nt.causal_matrix\n",
    "evaluate_model(B_true, nt.causal_matrix, interv_node_idx, target_node_idx, \"NOTEARS\")\n",
    "print()\n",
    "evaluate_effect(W_true, nt.weight_causal_matrix, interv_node_idx, target_node_idx, \"NOTEARS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ICALiNGAM with g_ICAlingam.causal_matrix\n",
    "evaluate_model(B_true, g_ICAlingam.causal_matrix, interv_node_idx, target_node_idx, \"ICALiNGAM\")\n",
    "print()\n",
    "evaluate_effect(W_true, g_ICAlingam.weight_causal_matrix, interv_node_idx, target_node_idx, \"ICALiNGAM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LiM with LiM_B_est\n",
    "evaluate_model(B_true, LiM_B_est, interv_node_idx, target_node_idx, \"LiM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mCMIkNN with graph, note that graph is a networkx graph so we need to convert it to adjacency matrix\n",
    "evaluate_model(B_true, mCMIkNN_B_est, interv_node_idx, target_node_idx, \"mCMIkNN\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pcax24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
