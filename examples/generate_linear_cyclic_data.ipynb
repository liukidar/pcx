{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "print(\"Current Working Directory:\", os.getcwd())\n",
    "\n",
    "# Add the examples directory to sys.path so we can import the set_random_seed function and other utilities from causal_helpers\n",
    "examples_dir = os.path.abspath(os.path.join(os.getcwd(), '..', 'examples'))\n",
    "sys.path.append(examples_dir)\n",
    "# Now import set_random_seed directly from causal_helpers\n",
    "from causal_helpers import set_random_seed\n",
    "from cyclic_obs_data_generator import sample_ER_dcg, sample_SF_dcg, sample_NWS_dcg\n",
    "from cyclic_obs_data_generator import sample_W, sample_data\n",
    "\n",
    "# Add the data directory to sys.path so we can save and load data files\n",
    "data_dir = os.path.abspath(os.path.join(examples_dir, '..', 'data'))\n",
    "\n",
    "# Use the function\n",
    "#seed = 41 # main seed\n",
    "seed = 22\n",
    "set_random_seed(seed)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Current Working Directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "num_samples = 5000 # number of samples to be sampled from [100, 500, 1000, 2000, 5000]\n",
    "d = 10 # number of variables in the graph to be sampled from [10, 15, 20]\n",
    "#max_degree = int(d // 2) # ensure that no node has more than half the number of nodes as neighbors\n",
    "max_degree = d - 1 # ensure that no node has more than d - 1 neighbors\n",
    "max_cycle = 3 # maximum cycle length in the graph\n",
    "p_density = 0.3 # density of the graph to be sampled from [0.3,0.5,0.7] uese these for nws only\n",
    "e_to_d_ratio = 2 # ratio of edges to nodes to be sampled from [2,4,6] use these for er and sf only\n",
    "\n",
    "n_edges = e_to_d_ratio * d # number of edges in the graph\n",
    "\n",
    "# SELECT 1 NOISE TYPE to be sampled from [\"GAUSS-EV\", \"SOFTPLUS\", \"EXP\", \"UNIFORM\"]\n",
    "noise_type = \"GAUSS-EV\"\n",
    "#noise_type = \"SOFTPLUS\"\n",
    "#noise_type = \"EXP\"\n",
    "#noise_type = \"UNIFORM\"\n",
    "\n",
    "\n",
    "# SELECT 1 GRAPH TYPE to be sampled from [\"ER\", \"SF\", \"NWS\"]\n",
    "#graph_type = \"SF\"\n",
    "graph_type = \"ER\"\n",
    "#graph_type = \"NWS\"\n",
    "\n",
    "# Generate data\n",
    "\n",
    "#B = sample_ER_dcg(d=d, max_degree=max_degree, max_cycle=max_cycle, p=None, n_edges=n_edges)\n",
    "#B = sample_SF_dcg(d=d, max_degree=max_degree, max_cycle=max_cycle, p=None, n_edges=n_edges)\n",
    "B = sample_NWS_dcg(d=d, max_degree=max_degree, max_cycle=max_cycle, p=p_density)\n",
    "print(\"B created\")\n",
    "\n",
    "#W, noise_scales = sample_W(B)\n",
    "W, _ = sample_W(B)\n",
    "print(\"W created\")\n",
    "\n",
    "scales = np.ones(d, dtype=float)\n",
    "X, prec_matrix = sample_data(W=W, scales=scales, num_samples=num_samples, noise_type=noise_type)\n",
    "print(\"X created\")\n",
    "\n",
    "data = pd.DataFrame(X, columns=[f\"X{i}\" for i in range(d)])\n",
    "\n",
    "# Display the adjacency matrix and a preview of the data\n",
    "print(\"Adjacency Matrix:\")\n",
    "print(B)\n",
    "print(\"\\nGenerated Data:\")\n",
    "print(data.head())\n",
    "print(\"\\nTrue Weights:\")\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_cont = np.array([True if data[col].nunique() > 2 else False for col in data.columns])\n",
    "is_cont.shape\n",
    "print(is_cont.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "G = nx.DiGraph(B)\n",
    "\n",
    "print(\"Is the graph directed?\", G.is_directed())\n",
    "\n",
    "is_cyclic = False  # Initialize is_cyclic to False\n",
    "try:\n",
    "    cycles = nx.find_cycle(G)\n",
    "    is_cyclic = True  # Set to True if a cycle is found\n",
    "    print(\"Is the graph cyclic?\", is_cyclic)\n",
    "    print(f\"There are {len(list(nx.simple_cycles(G)))} cycles in the graph, including: {cycles} for example.\")\n",
    "except nx.NetworkXNoCycle:\n",
    "    print(\"Is the graph cyclic?\", is_cyclic)  # Use the boolean variable directly\n",
    "\n",
    "print(f\"Number of edges: {G.number_of_edges()}\")\n",
    "print(f\"Number of nodes: {G.number_of_nodes()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all the data to a directory\n",
    "\n",
    "# in the end we only store n_vars and n_edges in the directory name because we are interested about the density of the graph using the notation 10ER2, 10SF4, 10NSW3 and so on\n",
    "dir_name = os.path.join(\n",
    "    data_dir,\n",
    "    f\"linear_cyclic_{noise_type}_{graph_type}_nvars_{d}_n_edges_{G.number_of_edges()}_max_degree_{max_degree}_max_cycle_{max_cycle}_seed_{seed}_n_samples_{num_samples}\"\n",
    ")\n",
    "\n",
    "os.makedirs(dir_name, exist_ok=True)\n",
    "\n",
    "# store the adjacency matrix as a csv file named \"adj_matrix.csv\" without header\n",
    "adj_matrix_df = pd.DataFrame(B)\n",
    "adj_matrix_df.to_csv(f\"{dir_name}/adj_matrix.csv\", header=False, index=False)\n",
    "\n",
    "# store the data as a csv file named \"train.csv\" without header\n",
    "data.to_csv(f\"{dir_name}/train.csv\", header=False, index=False)\n",
    "\n",
    "# store the weighted adjacency matrix as a csv file named \"W.csv\" without header\n",
    "W_df = pd.DataFrame(W)\n",
    "W_df.to_csv(f\"{dir_name}/W_adj_matrix.csv\", header=False, index=False)\n",
    "\n",
    "# store the precision matrix as a csv file named \"prec_matrix.csv\" without header\n",
    "if prec_matrix is not None:\n",
    "    prec_matrix_df = pd.DataFrame(prec_matrix)\n",
    "    prec_matrix_df.to_csv(f\"{dir_name}/prec_matrix.csv\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the distribution of all variables against each other and save the plot as \"pairplot.pdf\"\n",
    "import seaborn as sns\n",
    "sns.pairplot(data)\n",
    "plt.savefig(f\"{dir_name}/pairplot.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################## MISC ##################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for the uniform distribution\n",
    "low, high = -1, 1  # Range of the uniform distribution\n",
    "size = 100000  # Number of samples\n",
    "\n",
    "# Generate uniform samples\n",
    "uniform_samples = np.random.uniform(0.5, 1.5, size)\n",
    "\n",
    "# Apply the power transformation\n",
    "power = 5\n",
    "transformed_samples = uniform_samples**power\n",
    "\n",
    "# Plot the original and transformed distributions\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Original uniform distribution\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(uniform_samples, bins=100, density=True, alpha=0.7, color='blue')\n",
    "plt.title(\"Original Uniform Distribution\")\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Density\")\n",
    "\n",
    "# Transformed distribution\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(transformed_samples, bins=100, density=True, alpha=0.7, color='orange')\n",
    "plt.title(f\"Transformed Distribution (x**{power})\")\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Density\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_var = 5\n",
    "noise_scales = np.random.uniform(0.5, 1.5, size=n_var)\n",
    "noise = (np.random.uniform(-np.array(noise_scales).reshape(-1, 1), np.array(noise_scales).reshape(-1, 1), size=(n_var, 5000)).T)**4\n",
    "\n",
    "# Plot histograms for each row\n",
    "for i in range(n_var):\n",
    "    plt.figure()\n",
    "    plt.hist(noise[:, i], bins=50, alpha=0.75)\n",
    "    plt.title(f'Distribution of Row {i+1}')\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Generate samples from transformed Gaussian distribution\n",
    "np.random.seed(42)\n",
    "x = np.random.normal(0, 1, 100000)  # Original Gaussian samples\n",
    "y = np.sign(x) * (x ** 2)          # Apply transformation\n",
    "\n",
    "# Theoretical PDF calculation\n",
    "def theoretical_pdf(y):\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        pdf = 1 / (2 * np.sqrt(2 * np.pi * np.abs(y))) * np.exp(-np.abs(y)/2)\n",
    "    return np.nan_to_num(pdf)\n",
    "\n",
    "# Create visualization\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(y, bins=100, stat='density', alpha=0.6, color='skyblue', label='Empirical Distribution')\n",
    "\n",
    "# Plot theoretical distribution\n",
    "x_range = np.linspace(-10, 10, 1000)\n",
    "plt.plot(x_range, theoretical_pdf(x_range), 'r-', lw=2, label='Theoretical PDF')\n",
    "\n",
    "plt.title('Noise Distribution: sign(x) · x² where x ∼ N(0,1)')\n",
    "plt.xlabel('Transformed Value')\n",
    "plt.ylabel('Probability Density')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.xlim(-10, 10)  # Focus on central region\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HERE WE PLOT THE NOISE DISTRIBUTION USED IN LACERDA ET. AL 2008 ICA BASED CYCLIC STRUCTURE LEARNING AT EQUILIBRIUM ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Number of samples\n",
    "num_samples = 10000\n",
    "\n",
    "# Sample from Gaussian(0,1)\n",
    "samples = np.random.normal(0, 1, size=num_samples)\n",
    "\n",
    "# Apply transformation: square and restore sign\n",
    "transformed_samples = np.sign(samples) * (samples ** 2)\n",
    "\n",
    "# Plot the distribution\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.histplot(transformed_samples, bins=100, kde=True)\n",
    "plt.xlabel(\"Transformed Samples\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Distribution of Transformed Gaussian Samples\")\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pcax24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
