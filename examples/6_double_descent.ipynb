{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "# Core dependencies\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import equinox as eqx\n",
    "import optax\n",
    "\n",
    "# pcax\n",
    "import pcax as px\n",
    "import pcax.predictive_coding as pxc\n",
    "import pcax.nn as pxnn\n",
    "import pcax.functional as pxf\n",
    "import pcax.utils as pxu\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition\n",
    "class TwoLayerNN(pxc.EnergyModule):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int, act_fn: Callable[[jax.Array], jax.Array]) -> None:\n",
    "        super().__init__()\n",
    "        self.act_fn = px.static(act_fn)\n",
    "        self.layers = [\n",
    "            pxnn.Linear(input_dim, hidden_dim),\n",
    "            pxnn.Linear(hidden_dim, output_dim)\n",
    "        ]\n",
    "\n",
    "        # create a glorot uniform initializer\n",
    "        initializer = jax.nn.initializers.glorot_uniform()\n",
    "        # now apply glorot uniform initialization to the weights only\n",
    "        # the basic syntax is: model.layers[i].nn.weight.set(initializer(key, model.layers[i].nn.weight.shape))\n",
    "        for l in self.layers:\n",
    "            l.nn.weight.set(initializer(px.RKG(), l.nn.weight.shape))\n",
    "        \n",
    "        self.vodes = [\n",
    "            pxc.Vode((hidden_dim,)),\n",
    "            pxc.Vode((output_dim,), pxc.ce_energy)\n",
    "        ]\n",
    "        self.vodes[-1].h.frozen = True\n",
    "\n",
    "    def __call__(self, x, y):\n",
    "        for v, l in zip(self.vodes[:-1], self.layers[:-1]):\n",
    "            x = v(self.act_fn(l(x)))\n",
    "        x = self.vodes[-1](self.layers[-1](x))\n",
    "        if y is not None:\n",
    "            self.vodes[-1].set(\"h\", y)\n",
    "        return self.vodes[-1].get(\"u\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a simple collate function that stacks numpy arrays used to interface\n",
    "# the PyTorch dataloader with JAX. In the future we hope to provide custom dataloaders\n",
    "# that are independent of PyTorch.\n",
    "def numpy_collate(batch):\n",
    "    if isinstance(batch[0], np.ndarray):\n",
    "        return np.stack(batch)\n",
    "    elif isinstance(batch[0], (tuple, list)):\n",
    "        transposed = zip(*batch)\n",
    "        return [numpy_collate(samples) for samples in transposed]\n",
    "    else:\n",
    "        return np.array(batch)\n",
    "\n",
    "# The dataloader assumes cuda is being used, as such it sets 'pin_memory = True' and\n",
    "# 'prefetch_factor = 2'. Note that the batch size should be constant during training, so\n",
    "# we set 'drop_last = True' to avoid having to deal with variable batch sizes.\n",
    "class TorchDataloader(torch.utils.data.DataLoader):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset,\n",
    "        batch_size=1,\n",
    "        shuffle=None,\n",
    "        sampler=None,\n",
    "        batch_sampler=None,\n",
    "        num_workers=16,\n",
    "        pin_memory=True,\n",
    "        timeout=0,\n",
    "        worker_init_fn=None,\n",
    "        persistent_workers=True,\n",
    "        prefetch_factor=2,\n",
    "    ):\n",
    "        super(self.__class__, self).__init__(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            sampler=sampler,\n",
    "            batch_sampler=batch_sampler,\n",
    "            num_workers=num_workers,\n",
    "            collate_fn=numpy_collate,\n",
    "            pin_memory=pin_memory,\n",
    "            drop_last=True if batch_sampler is None else None,\n",
    "            timeout=timeout,\n",
    "            worker_init_fn=worker_init_fn,\n",
    "            persistent_workers=persistent_workers,\n",
    "            prefetch_factor=prefetch_factor,\n",
    "        )\n",
    "\n",
    "# This function returns the MNIST dataloaders for training and testing.\n",
    "def get_dataloaders(batch_size: int):\n",
    "    t = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,)),\n",
    "        lambda x: x.view(-1).numpy()  # Flatten the image to a vector\n",
    "    ])\n",
    "\n",
    "    train_dataset = torchvision.datasets.MNIST(\n",
    "        \"~/tmp/mnist/\",\n",
    "        transform=t,\n",
    "        download=True,\n",
    "        train=True,\n",
    "    )\n",
    "\n",
    "    train_dataloader = TorchDataloader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=16,\n",
    "    )\n",
    "\n",
    "    test_dataset = torchvision.datasets.MNIST(\n",
    "        \"~/tmp/mnist/\",\n",
    "        transform=t,\n",
    "        download=True,\n",
    "        train=False,\n",
    "    )\n",
    "        \n",
    "    test_dataloader = TorchDataloader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=16,\n",
    "    )\n",
    "\n",
    "    return train_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and evaluation functions\n",
    "@pxf.vmap(pxu.Mask(pxc.VodeParam | pxc.VodeParam.Cache, (None, 0)), in_axes=(0, 0), out_axes=0)\n",
    "def forward(x, y, *, model: TwoLayerNN):\n",
    "    return model(x, y)\n",
    "\n",
    "@pxf.vmap(pxu.Mask(pxc.VodeParam | pxc.VodeParam.Cache, (None, 0)), in_axes=(0,), out_axes=(None, 0), axis_name=\"batch\")\n",
    "def energy(x, *, model: TwoLayerNN):\n",
    "    y_ = model(x, None)\n",
    "    return jax.lax.pmean(model.energy().sum(), \"batch\"), y_\n",
    "\n",
    "@pxf.jit(static_argnums=0)\n",
    "def train_on_batch(T: int, x: jax.Array, y: jax.Array, *, model: TwoLayerNN, optim_w: pxu.Optim, optim_h: pxu.Optim):\n",
    "    model.train()\n",
    "    with pxu.step(model, pxc.STATUS.INIT, clear_params=pxc.VodeParam.Cache):\n",
    "        forward(x, y, model=model)\n",
    "\n",
    "    for _ in range(T):\n",
    "        with pxu.step(model, clear_params=pxc.VodeParam.Cache):\n",
    "            _, g = pxf.value_and_grad(pxu.Mask(pxu.m(pxc.VodeParam).has_not(frozen=True), [False, True]), has_aux=True)(energy)(x, model=model)\n",
    "        optim_h.step(model, g[\"model\"], True)\n",
    "\n",
    "    with pxu.step(model, clear_params=pxc.VodeParam.Cache):\n",
    "        _, g = pxf.value_and_grad(pxu.Mask(pxnn.LayerParam, [False, True]), has_aux=True)(energy)(x, model=model)\n",
    "    optim_w.step(model, g[\"model\"])\n",
    "\n",
    "@pxf.jit()\n",
    "def eval_on_batch(x: jax.Array, y: jax.Array, *, model: TwoLayerNN):\n",
    "    model.eval()\n",
    "    with pxu.step(model, pxc.STATUS.INIT, clear_params=pxc.VodeParam.Cache):\n",
    "        y_ = forward(x, None, model=model).argmax(axis=-1)\n",
    "    return (y_ == y).mean(), y_\n",
    "\n",
    "def train(dl, T, *, model: TwoLayerNN, optim_w: pxu.Optim, optim_h: pxu.Optim):\n",
    "    for x, y in dl:\n",
    "        #print(f\"x shape: {x.shape}, y shape: {y.shape}\")  # Debugging line\n",
    "        train_on_batch(T, x, jax.nn.one_hot(y, 10), model=model, optim_w=optim_w, optim_h=optim_h)\n",
    "\n",
    "def eval(dl, *, model: TwoLayerNN):\n",
    "    acc = []\n",
    "    ys_ = []\n",
    "    for x, y in dl:\n",
    "        a, y_ = eval_on_batch(x, y, model=model)\n",
    "        acc.append(a)\n",
    "        ys_.append(y_)\n",
    "    return np.mean(acc), np.concatenate(ys_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training and evaluation\n",
    "batch_size = 128\n",
    "nm_epochs = 10\n",
    "model = TwoLayerNN(input_dim=784, hidden_dim=128, output_dim=10, act_fn=jax.nn.relu)\n",
    "\n",
    "# Initialize the model and optimizers\n",
    "with pxu.step(model, pxc.STATUS.INIT, clear_params=pxc.VodeParam.Cache):\n",
    "    forward(jax.numpy.zeros((batch_size, 784)), None, model=model)\n",
    "    optim_h = pxu.Optim(optax.sgd(0.1), pxu.Mask(pxc.VodeParam)(model))\n",
    "    optim_w = pxu.Optim(optax.adamw(1e-3), pxu.Mask(pxnn.LayerParam)(model))\n",
    "\n",
    "train_dataloader, test_dataloader = get_dataloaders(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(nm_epochs):\n",
    "    train(train_dataloader, T=10, model=model, optim_w=optim_w, optim_h=optim_h)\n",
    "    a, y = eval(test_dataloader, model=model)\n",
    "    print(f\"Epoch {e + 1}/{nm_epochs} - Test Accuracy: {a * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initializer = jax.nn.initializers.glorot_uniform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the mean of weights of model.layers[0].nn.weight before initialization\n",
    "model.layers[0].nn.weight.get().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].nn.weight.set(initializer(px.RKG(), model.layers[0].nn.weight.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the weights mean of model.layers[0].nn.weight after initialization\n",
    "model.layers[0].nn.weight.get().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pcax24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
