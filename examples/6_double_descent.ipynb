{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-25 21:39:35.951874: W external/xla/xla/service/gpu/nvptx_compiler.cc:760] The NVIDIA driver's CUDA version is 12.2 which is older than the ptxas CUDA version (12.5.40). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n"
     ]
    }
   ],
   "source": [
    "from typing import Callable\n",
    "\n",
    "# Core dependencies\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import equinox as eqx\n",
    "import optax\n",
    "\n",
    "# pcax\n",
    "import pcax as px\n",
    "import pcax.predictive_coding as pxc\n",
    "import pcax.nn as pxnn\n",
    "import pcax.functional as pxf\n",
    "import pcax.utils as pxu\n",
    "\n",
    "import torchvision\n",
    "#import torchvision.transforms as transforms\n",
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition\n",
    "class TwoLayerNN(pxc.EnergyModule):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int, act_fn: Callable[[jax.Array], jax.Array]) -> None:\n",
    "        super().__init__()\n",
    "        self.act_fn = px.static(act_fn)\n",
    "        self.layers = [\n",
    "            pxnn.Linear(input_dim, hidden_dim),\n",
    "            pxnn.Linear(hidden_dim, output_dim)\n",
    "        ]\n",
    "\n",
    "        # create a glorot uniform initializer\n",
    "        initializer = jax.nn.initializers.glorot_uniform()\n",
    "        # now apply glorot uniform initialization to the weights only\n",
    "        # the basic syntax is: model.layers[i].nn.weight.set(initializer(key, model.layers[i].nn.weight.shape))\n",
    "        for l in self.layers:\n",
    "            l.nn.weight.set(initializer(px.RKG(), l.nn.weight.shape))\n",
    "        \n",
    "        self.vodes = [\n",
    "            pxc.Vode((hidden_dim,)),\n",
    "            pxc.Vode((output_dim,), pxc.ce_energy)\n",
    "        ]\n",
    "        self.vodes[-1].h.frozen = True\n",
    "\n",
    "    def __call__(self, x, y):\n",
    "        for v, l in zip(self.vodes[:-1], self.layers[:-1]):\n",
    "            x = v(self.act_fn(l(x)))\n",
    "        x = self.vodes[-1](self.layers[-1](x))\n",
    "        if y is not None:\n",
    "            self.vodes[-1].set(\"h\", y)\n",
    "        return self.vodes[-1].get(\"u\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset = namedtuple(\"Dataset\", [\"train_loader\", \"val_loader\", \"test_loader\"])\n",
    "\n",
    "# This is a simple collate function that stacks numpy arrays used to interface\n",
    "# the PyTorch dataloader with JAX. In the future we hope to provide custom dataloaders\n",
    "# that are independent of PyTorch.\n",
    "def numpy_collate(batch):\n",
    "    if isinstance(batch[0], np.ndarray):\n",
    "        return np.stack(batch)\n",
    "    elif isinstance(batch[0], (tuple, list)):\n",
    "        transposed = zip(*batch)\n",
    "        return [numpy_collate(samples) for samples in transposed]\n",
    "    else:\n",
    "        return np.array(batch)\n",
    "\n",
    "# The dataloader assumes cuda is being used, as such it sets 'pin_memory = True' and\n",
    "# 'prefetch_factor = 2'. Note that the batch size should be constant during training, so\n",
    "# we set 'drop_last = True' to avoid having to deal with variable batch sizes.\n",
    "class TorchDataloader(torch.utils.data.DataLoader):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset,\n",
    "        batch_size=1,\n",
    "        shuffle=None,\n",
    "        sampler=None,\n",
    "        batch_sampler=None,\n",
    "        num_workers=16,\n",
    "        pin_memory=True,\n",
    "        timeout=0,\n",
    "        worker_init_fn=None,\n",
    "        persistent_workers=True,\n",
    "        prefetch_factor=2,\n",
    "    ):\n",
    "        super(self.__class__, self).__init__(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            sampler=sampler,\n",
    "            batch_sampler=batch_sampler,\n",
    "            num_workers=num_workers,\n",
    "            collate_fn=numpy_collate,\n",
    "            pin_memory=pin_memory,\n",
    "            drop_last=True if batch_sampler is None else None,\n",
    "            timeout=timeout,\n",
    "            worker_init_fn=worker_init_fn,\n",
    "            persistent_workers=persistent_workers,\n",
    "            prefetch_factor=prefetch_factor,\n",
    "        )\n",
    "\n",
    "# Function to add noise to the labels in the dataset\n",
    "def add_label_noise(dataset, noise_level=0.2):\n",
    "    targets = np.array(dataset.targets)\n",
    "    num_classes = len(np.unique(targets))\n",
    "    num_noisy = int(noise_level * len(targets))\n",
    "    noisy_indices = np.random.choice(len(targets), num_noisy, replace=False)\n",
    "\n",
    "    for idx in noisy_indices:\n",
    "        original_label = targets[idx]\n",
    "        new_label = original_label\n",
    "        while new_label == original_label:\n",
    "            new_label = np.random.randint(0, num_classes)\n",
    "        targets[idx] = new_label\n",
    "\n",
    "    dataset.targets = torch.tensor(targets)\n",
    "    return dataset\n",
    "\n",
    "# Function to get the dataloaders\n",
    "def get_dataloaders(dataset_name, train_subset_size, batch_size, noise_level=0.2):\n",
    "    if dataset_name.lower() == \"mnist\":\n",
    "        ds = datasets.MNIST\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Dataset {dataset_name} isn't available\")\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,)),\n",
    "        transforms.Lambda(lambda x: x.view(-1).numpy())  # Flatten the image to a vector\n",
    "    ])\n",
    "\n",
    "    train_set = ds(root='./data', download=True, train=True, transform=transform)\n",
    "    train_set = add_label_noise(train_set, noise_level=noise_level)\n",
    "\n",
    "    val_subset_size = int(0.2 * train_subset_size)\n",
    "    random_train_indices = np.random.choice(len(train_set), size=train_subset_size, replace=False)\n",
    "    remaining_indices = np.setdiff1d(np.arange(len(train_set)), random_train_indices)\n",
    "    random_val_indices = np.random.choice(remaining_indices, size=val_subset_size, replace=False)\n",
    "\n",
    "    train_loader = TorchDataloader(\n",
    "        train_set, batch_size=batch_size, num_workers=16,\n",
    "        sampler=torch.utils.data.sampler.SubsetRandomSampler(random_train_indices))\n",
    "    val_loader = TorchDataloader(\n",
    "        train_set, batch_size=batch_size, num_workers=16,\n",
    "        sampler=torch.utils.data.sampler.SubsetRandomSampler(random_val_indices))\n",
    "\n",
    "    test_set = ds(root='./data', download=True, train=False, transform=transform)\n",
    "    test_loader = TorchDataloader(\n",
    "        test_set, batch_size=batch_size, shuffle=False, num_workers=16)\n",
    "\n",
    "    return Dataset(train_loader=train_loader, val_loader=val_loader, test_loader=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# This is a simple collate function that stacks numpy arrays used to interface\\n# the PyTorch dataloader with JAX. In the future we hope to provide custom dataloaders\\n# that are independent of PyTorch.\\ndef numpy_collate(batch):\\n    if isinstance(batch[0], np.ndarray):\\n        return np.stack(batch)\\n    elif isinstance(batch[0], (tuple, list)):\\n        transposed = zip(*batch)\\n        return [numpy_collate(samples) for samples in transposed]\\n    else:\\n        return np.array(batch)\\n\\n# The dataloader assumes cuda is being used, as such it sets \\'pin_memory = True\\' and\\n# \\'prefetch_factor = 2\\'. Note that the batch size should be constant during training, so\\n# we set \\'drop_last = True\\' to avoid having to deal with variable batch sizes.\\nclass TorchDataloader(torch.utils.data.DataLoader):\\n    def __init__(\\n        self,\\n        dataset,\\n        batch_size=1,\\n        shuffle=None,\\n        sampler=None,\\n        batch_sampler=None,\\n        num_workers=16,\\n        pin_memory=True,\\n        timeout=0,\\n        worker_init_fn=None,\\n        persistent_workers=True,\\n        prefetch_factor=2,\\n    ):\\n        super(self.__class__, self).__init__(\\n            dataset,\\n            batch_size=batch_size,\\n            shuffle=shuffle,\\n            sampler=sampler,\\n            batch_sampler=batch_sampler,\\n            num_workers=num_workers,\\n            collate_fn=numpy_collate,\\n            pin_memory=pin_memory,\\n            drop_last=True if batch_sampler is None else None,\\n            timeout=timeout,\\n            worker_init_fn=worker_init_fn,\\n            persistent_workers=persistent_workers,\\n            prefetch_factor=prefetch_factor,\\n        )\\n\\n# This function returns the MNIST dataloaders for training and testing.\\ndef get_dataloaders(batch_size: int):\\n    t = transforms.Compose([\\n        transforms.ToTensor(),\\n        transforms.Normalize((0.1307,), (0.3081,)),\\n        transforms.Lambda(lambda x: x.view(-1).numpy())  # Flatten the image to a vector\\n    ])\\n\\n    train_dataset = torchvision.datasets.MNIST(\\n        \"~/tmp/mnist/\",\\n        transform=t,\\n        download=True,\\n        train=True,\\n    )\\n\\n    train_dataloader = TorchDataloader(\\n        train_dataset,\\n        batch_size=batch_size,\\n        shuffle=True,\\n        num_workers=16,\\n    )\\n\\n    test_dataset = torchvision.datasets.MNIST(\\n        \"~/tmp/mnist/\",\\n        transform=t,\\n        download=True,\\n        train=False,\\n    )\\n        \\n    test_dataloader = TorchDataloader(\\n        test_dataset,\\n        batch_size=batch_size,\\n        shuffle=False,\\n        num_workers=16,\\n    )\\n\\n    return train_dataloader, test_dataloader\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# This is a simple collate function that stacks numpy arrays used to interface\n",
    "# the PyTorch dataloader with JAX. In the future we hope to provide custom dataloaders\n",
    "# that are independent of PyTorch.\n",
    "def numpy_collate(batch):\n",
    "    if isinstance(batch[0], np.ndarray):\n",
    "        return np.stack(batch)\n",
    "    elif isinstance(batch[0], (tuple, list)):\n",
    "        transposed = zip(*batch)\n",
    "        return [numpy_collate(samples) for samples in transposed]\n",
    "    else:\n",
    "        return np.array(batch)\n",
    "\n",
    "# The dataloader assumes cuda is being used, as such it sets 'pin_memory = True' and\n",
    "# 'prefetch_factor = 2'. Note that the batch size should be constant during training, so\n",
    "# we set 'drop_last = True' to avoid having to deal with variable batch sizes.\n",
    "class TorchDataloader(torch.utils.data.DataLoader):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset,\n",
    "        batch_size=1,\n",
    "        shuffle=None,\n",
    "        sampler=None,\n",
    "        batch_sampler=None,\n",
    "        num_workers=16,\n",
    "        pin_memory=True,\n",
    "        timeout=0,\n",
    "        worker_init_fn=None,\n",
    "        persistent_workers=True,\n",
    "        prefetch_factor=2,\n",
    "    ):\n",
    "        super(self.__class__, self).__init__(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            sampler=sampler,\n",
    "            batch_sampler=batch_sampler,\n",
    "            num_workers=num_workers,\n",
    "            collate_fn=numpy_collate,\n",
    "            pin_memory=pin_memory,\n",
    "            drop_last=True if batch_sampler is None else None,\n",
    "            timeout=timeout,\n",
    "            worker_init_fn=worker_init_fn,\n",
    "            persistent_workers=persistent_workers,\n",
    "            prefetch_factor=prefetch_factor,\n",
    "        )\n",
    "\n",
    "# This function returns the MNIST dataloaders for training and testing.\n",
    "def get_dataloaders(batch_size: int):\n",
    "    t = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,)),\n",
    "        transforms.Lambda(lambda x: x.view(-1).numpy())  # Flatten the image to a vector\n",
    "    ])\n",
    "\n",
    "    train_dataset = torchvision.datasets.MNIST(\n",
    "        \"~/tmp/mnist/\",\n",
    "        transform=t,\n",
    "        download=True,\n",
    "        train=True,\n",
    "    )\n",
    "\n",
    "    train_dataloader = TorchDataloader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=16,\n",
    "    )\n",
    "\n",
    "    test_dataset = torchvision.datasets.MNIST(\n",
    "        \"~/tmp/mnist/\",\n",
    "        transform=t,\n",
    "        download=True,\n",
    "        train=False,\n",
    "    )\n",
    "        \n",
    "    test_dataloader = TorchDataloader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=16,\n",
    "    )\n",
    "\n",
    "    return train_dataloader, test_dataloader\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and evaluation functions\n",
    "@pxf.vmap(pxu.Mask(pxc.VodeParam | pxc.VodeParam.Cache, (None, 0)), in_axes=(0, 0), out_axes=0)\n",
    "def forward(x, y, *, model: TwoLayerNN):\n",
    "    return model(x, y)\n",
    "\n",
    "@pxf.vmap(pxu.Mask(pxc.VodeParam | pxc.VodeParam.Cache, (None, 0)), in_axes=(0,), out_axes=(None, 0), axis_name=\"batch\")\n",
    "def energy(x, *, model: TwoLayerNN):\n",
    "    y_ = model(x, None)\n",
    "    return jax.lax.pmean(model.energy().sum(), \"batch\"), y_\n",
    "\n",
    "@pxf.jit(static_argnums=0)\n",
    "def train_on_batch(T: int, x: jax.Array, y: jax.Array, *, model: TwoLayerNN, optim_w: pxu.Optim, optim_h: pxu.Optim):\n",
    "    model.train()\n",
    "    with pxu.step(model, pxc.STATUS.INIT, clear_params=pxc.VodeParam.Cache):\n",
    "        forward(x, y, model=model)\n",
    "\n",
    "    for _ in range(T):\n",
    "        with pxu.step(model, clear_params=pxc.VodeParam.Cache):\n",
    "            _, g = pxf.value_and_grad(pxu.Mask(pxu.m(pxc.VodeParam).has_not(frozen=True), [False, True]), has_aux=True)(energy)(x, model=model)\n",
    "        optim_h.step(model, g[\"model\"], True)\n",
    "\n",
    "    with pxu.step(model, clear_params=pxc.VodeParam.Cache):\n",
    "        _, g = pxf.value_and_grad(pxu.Mask(pxnn.LayerParam, [False, True]), has_aux=True)(energy)(x, model=model)\n",
    "    optim_w.step(model, g[\"model\"])\n",
    "\n",
    "@pxf.jit()\n",
    "def eval_on_batch(x: jax.Array, y: jax.Array, *, model: TwoLayerNN):\n",
    "    model.eval()\n",
    "    with pxu.step(model, pxc.STATUS.INIT, clear_params=pxc.VodeParam.Cache):\n",
    "        y_ = forward(x, None, model=model).argmax(axis=-1)\n",
    "    return (y_ == y).mean(), y_\n",
    "\n",
    "def train(dl, T, *, model: TwoLayerNN, optim_w: pxu.Optim, optim_h: pxu.Optim):\n",
    "    for x, y in dl:\n",
    "        #print(f\"x shape: {x.shape}, y shape: {y.shape}\")  # Debugging line\n",
    "        train_on_batch(T, x, jax.nn.one_hot(y, 10), model=model, optim_w=optim_w, optim_h=optim_h)\n",
    "\n",
    "def eval(dl, *, model: TwoLayerNN):\n",
    "    acc = []\n",
    "    ys_ = []\n",
    "    for x, y in dl:\n",
    "        a, y_ = eval_on_batch(x, y, model=model)\n",
    "        acc.append(a)\n",
    "        ys_.append(y_)\n",
    "    return np.mean(acc), np.concatenate(ys_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 4000 samples\n",
      "Validation set: 800 samples\n",
      "Test set: 10000 samples\n"
     ]
    }
   ],
   "source": [
    "# Model training and evaluation\n",
    "batch_size = 128\n",
    "nm_epochs = 500\n",
    "model = TwoLayerNN(input_dim=784, hidden_dim=128, output_dim=10, act_fn=jax.nn.relu)\n",
    "\n",
    "# Initialize the model and optimizers\n",
    "with pxu.step(model, pxc.STATUS.INIT, clear_params=pxc.VodeParam.Cache):\n",
    "    forward(jax.numpy.zeros((batch_size, 784)), None, model=model)\n",
    "    optim_h = pxu.Optim(optax.sgd(0.1), pxu.Mask(pxc.VodeParam)(model))\n",
    "    optim_w = pxu.Optim(optax.sgd(0.01, momentum=0.95), pxu.Mask(pxnn.LayerParam)(model))\n",
    "\n",
    "# Assuming dataset is the namedtuple with the dataloaders\n",
    "dataset = get_dataloaders(\"mnist\", train_subset_size=4000, batch_size=batch_size, noise_level=0.0)\n",
    "# Check the sizes of the datasets\n",
    "print(f\"Training set: {len(dataset.train_loader.sampler)} samples\")\n",
    "print(f\"Validation set: {len(dataset.val_loader.sampler)} samples\")\n",
    "print(f\"Test set: {len(dataset.test_loader.dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500 - Training Accuracy: 100.00% - Validation Accuracy: 93.62% - Test Accuracy: 94.04%\n",
      "Epoch 2/500 - Training Accuracy: 100.00% - Validation Accuracy: 93.36% - Test Accuracy: 94.04%\n",
      "Epoch 3/500 - Training Accuracy: 100.00% - Validation Accuracy: 93.36% - Test Accuracy: 94.04%\n",
      "Epoch 4/500 - Training Accuracy: 100.00% - Validation Accuracy: 93.49% - Test Accuracy: 94.04%\n",
      "Epoch 5/500 - Training Accuracy: 100.00% - Validation Accuracy: 93.10% - Test Accuracy: 94.04%\n",
      "Epoch 6/500 - Training Accuracy: 100.00% - Validation Accuracy: 93.10% - Test Accuracy: 94.04%\n",
      "Epoch 7/500 - Training Accuracy: 100.00% - Validation Accuracy: 93.36% - Test Accuracy: 94.04%\n",
      "Epoch 8/500 - Training Accuracy: 100.00% - Validation Accuracy: 93.36% - Test Accuracy: 94.04%\n",
      "Epoch 9/500 - Training Accuracy: 100.00% - Validation Accuracy: 93.88% - Test Accuracy: 94.04%\n",
      "Epoch 10/500 - Training Accuracy: 100.00% - Validation Accuracy: 93.23% - Test Accuracy: 94.04%\n",
      "Epoch 11/500 - Training Accuracy: 100.00% - Validation Accuracy: 93.23% - Test Accuracy: 94.04%\n",
      "Epoch 12/500 - Training Accuracy: 100.00% - Validation Accuracy: 93.36% - Test Accuracy: 94.04%\n",
      "Epoch 13/500 - Training Accuracy: 100.00% - Validation Accuracy: 93.36% - Test Accuracy: 94.04%\n",
      "Epoch 14/500 - Training Accuracy: 100.00% - Validation Accuracy: 93.75% - Test Accuracy: 94.04%\n",
      "Epoch 15/500 - Training Accuracy: 100.00% - Validation Accuracy: 93.36% - Test Accuracy: 94.04%\n",
      "Epoch 16/500 - Training Accuracy: 100.00% - Validation Accuracy: 93.49% - Test Accuracy: 94.04%\n",
      "Epoch 17/500 - Training Accuracy: 100.00% - Validation Accuracy: 93.36% - Test Accuracy: 94.04%\n",
      "Epoch 18/500 - Training Accuracy: 100.00% - Validation Accuracy: 93.10% - Test Accuracy: 94.04%\n",
      "Epoch 19/500 - Training Accuracy: 100.00% - Validation Accuracy: 93.23% - Test Accuracy: 94.04%\n",
      "Epoch 20/500 - Training Accuracy: 100.00% - Validation Accuracy: 93.23% - Test Accuracy: 94.04%\n",
      "Epoch 21/500 - Training Accuracy: 100.00% - Validation Accuracy: 93.36% - Test Accuracy: 94.04%\n",
      "Epoch 22/500 - Training Accuracy: 100.00% - Validation Accuracy: 93.36% - Test Accuracy: 94.04%\n",
      "Epoch 23/500 - Training Accuracy: 100.00% - Validation Accuracy: 93.75% - Test Accuracy: 94.04%\n",
      "Epoch 24/500 - Training Accuracy: 100.00% - Validation Accuracy: 93.23% - Test Accuracy: 94.04%\n",
      "Epoch 25/500 - Training Accuracy: 100.00% - Validation Accuracy: 93.62% - Test Accuracy: 94.04%\n",
      "Epoch 26/500 - Training Accuracy: 100.00% - Validation Accuracy: 93.23% - Test Accuracy: 94.04%\n",
      "Epoch 27/500 - Training Accuracy: 100.00% - Validation Accuracy: 93.10% - Test Accuracy: 94.04%\n",
      "Epoch 28/500 - Training Accuracy: 100.00% - Validation Accuracy: 93.36% - Test Accuracy: 94.04%\n",
      "Epoch 29/500 - Training Accuracy: 100.00% - Validation Accuracy: 93.75% - Test Accuracy: 94.04%\n",
      "Epoch 30/500 - Training Accuracy: 100.00% - Validation Accuracy: 93.23% - Test Accuracy: 94.04%\n",
      "Epoch 31/500 - Training Accuracy: 100.00% - Validation Accuracy: 93.49% - Test Accuracy: 94.04%\n",
      "Epoch 32/500 - Training Accuracy: 100.00% - Validation Accuracy: 93.62% - Test Accuracy: 94.04%\n",
      "Epoch 33/500 - Training Accuracy: 100.00% - Validation Accuracy: 93.10% - Test Accuracy: 94.04%\n",
      "Epoch 34/500 - Training Accuracy: 100.00% - Validation Accuracy: 93.36% - Test Accuracy: 94.04%\n",
      "Epoch 35/500 - Training Accuracy: 100.00% - Validation Accuracy: 93.23% - Test Accuracy: 94.04%\n",
      "Epoch 36/500 - Training Accuracy: 100.00% - Validation Accuracy: 93.10% - Test Accuracy: 94.04%\n",
      "Epoch 37/500 - Training Accuracy: 100.00% - Validation Accuracy: 93.23% - Test Accuracy: 94.04%\n",
      "Epoch 38/500 - Training Accuracy: 100.00% - Validation Accuracy: 93.10% - Test Accuracy: 94.04%\n",
      "Epoch 39/500 - Training Accuracy: 100.00% - Validation Accuracy: 93.49% - Test Accuracy: 94.04%\n",
      "Epoch 40/500 - Training Accuracy: 100.00% - Validation Accuracy: 93.49% - Test Accuracy: 94.04%\n",
      "Epoch 41/500 - Training Accuracy: 100.00% - Validation Accuracy: 93.36% - Test Accuracy: 94.04%\n",
      "Epoch 42/500 - Training Accuracy: 100.00% - Validation Accuracy: 93.75% - Test Accuracy: 94.04%\n",
      "Epoch 43/500 - Training Accuracy: 100.00% - Validation Accuracy: 93.36% - Test Accuracy: 94.04%\n",
      "Epoch 44/500 - Training Accuracy: 100.00% - Validation Accuracy: 93.88% - Test Accuracy: 94.04%\n",
      "Epoch 45/500 - Training Accuracy: 100.00% - Validation Accuracy: 93.49% - Test Accuracy: 94.04%\n",
      "Epoch 46/500 - Training Accuracy: 100.00% - Validation Accuracy: 93.10% - Test Accuracy: 94.04%\n",
      "Epoch 47/500 - Training Accuracy: 100.00% - Validation Accuracy: 93.23% - Test Accuracy: 94.04%\n",
      "Epoch 48/500 - Training Accuracy: 100.00% - Validation Accuracy: 93.75% - Test Accuracy: 94.04%\n",
      "Epoch 49/500 - Training Accuracy: 100.00% - Validation Accuracy: 93.10% - Test Accuracy: 94.04%\n",
      "Epoch 50/500 - Training Accuracy: 100.00% - Validation Accuracy: 93.23% - Test Accuracy: 94.04%\n",
      "Epoch 51/500 - Training Accuracy: 100.00% - Validation Accuracy: 93.10% - Test Accuracy: 94.04%\n",
      "Epoch 52/500 - Training Accuracy: 100.00% - Validation Accuracy: 93.23% - Test Accuracy: 94.04%\n",
      "Epoch 53/500 - Training Accuracy: 100.00% - Validation Accuracy: 93.49% - Test Accuracy: 94.04%\n",
      "Epoch 54/500 - Training Accuracy: 100.00% - Validation Accuracy: 93.23% - Test Accuracy: 94.04%\n",
      "Epoch 55/500 - Training Accuracy: 100.00% - Validation Accuracy: 93.62% - Test Accuracy: 94.04%\n",
      "Epoch 56/500 - Training Accuracy: 100.00% - Validation Accuracy: 93.75% - Test Accuracy: 94.04%\n",
      "Epoch 57/500 - Training Accuracy: 100.00% - Validation Accuracy: 93.23% - Test Accuracy: 94.04%\n",
      "Epoch 58/500 - Training Accuracy: 100.00% - Validation Accuracy: 93.49% - Test Accuracy: 94.04%\n",
      "Epoch 59/500 - Training Accuracy: 100.00% - Validation Accuracy: 93.62% - Test Accuracy: 94.04%\n",
      "Epoch 60/500 - Training Accuracy: 100.00% - Validation Accuracy: 93.10% - Test Accuracy: 94.04%\n",
      "Epoch 61/500 - Training Accuracy: 100.00% - Validation Accuracy: 93.36% - Test Accuracy: 94.04%\n",
      "Epoch 62/500 - Training Accuracy: 100.00% - Validation Accuracy: 93.10% - Test Accuracy: 94.04%\n",
      "Epoch 63/500 - Training Accuracy: 100.00% - Validation Accuracy: 93.49% - Test Accuracy: 94.04%\n",
      "Epoch 64/500 - Training Accuracy: 100.00% - Validation Accuracy: 93.10% - Test Accuracy: 94.04%\n",
      "Epoch 65/500 - Training Accuracy: 100.00% - Validation Accuracy: 93.36% - Test Accuracy: 94.04%\n",
      "Epoch 66/500 - Training Accuracy: 100.00% - Validation Accuracy: 93.23% - Test Accuracy: 94.04%\n",
      "Epoch 67/500 - Training Accuracy: 100.00% - Validation Accuracy: 93.23% - Test Accuracy: 94.04%\n",
      "Epoch 68/500 - Training Accuracy: 100.00% - Validation Accuracy: 93.49% - Test Accuracy: 94.04%\n",
      "Epoch 69/500 - Training Accuracy: 100.00% - Validation Accuracy: 93.49% - Test Accuracy: 94.04%\n"
     ]
    }
   ],
   "source": [
    "for e in range(nm_epochs):\n",
    "    train(dataset.train_loader, T=10, model=model, optim_w=optim_w, optim_h=optim_h)\n",
    "    # show the training accuracy, validation accuracy, and test accuracy all in one line together with the epoch number\n",
    "    a_train, _ = eval(dataset.train_loader, model=model)\n",
    "    a_test, _ = eval(dataset.test_loader, model=model)\n",
    "    a_val, _ = eval(dataset.val_loader, model=model)\n",
    "\n",
    "    print(f\"Epoch {e + 1}/{nm_epochs} - Training Accuracy: {a_train * 100:.2f}% - Validation Accuracy: {a_val * 100:.2f}% - Test Accuracy: {a_test * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initializer = jax.nn.initializers.glorot_uniform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the mean of weights of model.layers[0].nn.weight before initialization\n",
    "model.layers[0].nn.weight.get().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].nn.weight.set(initializer(px.RKG(), model.layers[0].nn.weight.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the weights mean of model.layers[0].nn.weight after initialization\n",
    "model.layers[0].nn.weight.get().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pcax24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
