{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial #0b: Backpropagation with PCX\n",
    "\n",
    "PCX can be used to train standard artificial aeural networks with backpropagation be removing the `vodes` from a model and adjusting the training loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies not included in the base requirements.txt\n",
    "\n",
    "!pip install scikit-learn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "# These are the default import names used in tutorials and documentation.\n",
    "import jax\n",
    "import pcx as px\n",
    "import pcx.nn as pxnn\n",
    "import pcx.functional as pxf\n",
    "import pcx.utils as pxu\n",
    "\n",
    "# px.RKG is the default key generator used in pcx, which is used as default\n",
    "# source of randomness within pcx. Here we set its seed to 0 for more reproducibility.\n",
    "# By default it is initialised with the system time.\n",
    "px.RKG.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note how we inherit from Module and not pxc.EnergyModule as we do not have an energy function anymore.\n",
    "class Model(px.Module):\n",
    "    def __init__(\n",
    "        self, input_dim: int, hidden_dim: int, output_dim: int, nm_layers: int, act_fn: Callable[[jax.Array], jax.Array]\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.act_fn = px.static(act_fn)\n",
    "\n",
    "        self.layers = (\n",
    "            [pxnn.Linear(input_dim, hidden_dim)]\n",
    "            + [pxnn.Linear(hidden_dim, hidden_dim) for _ in range(nm_layers - 2)]\n",
    "            + [pxnn.Linear(hidden_dim, output_dim)]\n",
    "        )\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = self.act_fn(layer(x))\n",
    "\n",
    "        x = self.layers[-1](x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ce_loss(output, one_hot_label):\n",
    "    return (-one_hot_label * jax.nn.log_softmax(output)).sum()\n",
    "\n",
    "# We specify that the keyword argument 'model' has no parameters to be vmaped over.\n",
    "# Also, we do not provide any `y` value as there is no vode for it.\n",
    "@pxf.vmap({\"model\": None}, in_axes=0, out_axes=0)\n",
    "def forward(x, *, model: Model):\n",
    "    return model(x)\n",
    "\n",
    "\n",
    "# Instead, we provide the label `y` in the loss function, as normally done in deep learning.\n",
    "@pxf.vmap({\"model\": None}, in_axes=(0, 0), out_axes=(None, 0), axis_name=\"batch\")\n",
    "def loss(x, y, *, model: Model):\n",
    "    y_ = model(x)\n",
    "    return jax.lax.pmean(ce_loss(y_, y), \"batch\"), y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We removed unnnecessary parameters from the function signature. This also requires removing the `static_argnums` argument.\n",
    "@pxf.jit()\n",
    "def train_on_batch(\n",
    "    x: jax.Array,\n",
    "    y: jax.Array,\n",
    "    *,\n",
    "    model: Model,\n",
    "    optim_w: pxu.Optim\n",
    "):\n",
    "    print(\"Training!\")  # this will come in handy later\n",
    "\n",
    "    # This only sets an internal flag to be \"train\" (instead of \"eval\")\n",
    "    model.train()\n",
    "    \n",
    "    # No need for a forward pass as there is no vode to initialise.\n",
    "    \n",
    "    # No need for inference steps as well!\n",
    "    \n",
    "    # Weight update step\n",
    "    # The `pxu.step` function is actually not doing anything right now, but we keep it anyway for consistency\n",
    "    # (also in the future it may do something, who knows?).\n",
    "    with pxu.step(model):\n",
    "        (e, y_), g = pxf.value_and_grad(pxu.M(pxnn.LayerParam).to([False, True]), has_aux=True)(loss)(x, y, model=model)\n",
    "    \n",
    "    # Note there is not gradient scaling as we have already taken the mean of the loss, not its sum.\n",
    "    optim_w.step(model, g[\"model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Not much to say here, very similar as with PC, since also in PC we used a forward pass to compute the output.\n",
    "@pxf.jit()\n",
    "def eval_on_batch(x: jax.Array, y: jax.Array, *, model: Model):\n",
    "    model.eval()\n",
    "    \n",
    "    with pxu.step(model):\n",
    "        y_ = forward(x, model=model).argmax(axis=-1)\n",
    "    \n",
    "    return (y_ == y).mean(), y_\n",
    "\n",
    "\n",
    "# Standard training loop\n",
    "def train(dl, *, model: Model, optim_w: pxu.Optim):\n",
    "    for x, y in dl:\n",
    "        train_on_batch(x, jax.nn.one_hot(y, 2), model=model, optim_w=optim_w)\n",
    "\n",
    "# Standard evaluation loop\n",
    "def eval(dl, *, model: Model):\n",
    "    acc = []\n",
    "    ys_ = []\n",
    "    \n",
    "    for x, y in dl:\n",
    "        a, y_ = eval_on_batch(x, y, model=model)\n",
    "        acc.append(a)\n",
    "        ys_.append(y_)\n",
    "    \n",
    "    return np.mean(acc), np.concatenate(ys_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "model = Model(\n",
    "    input_dim=2,\n",
    "    hidden_dim=32,\n",
    "    output_dim=2,\n",
    "    nm_layers=3,\n",
    "    act_fn=jax.nn.leaky_relu\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim_w = pxu.Optim(lambda: optax.adamw(1e-2), pxu.M(pxnn.LayerParam)(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# this is unrelated to pcax: we generate and display the training set.\n",
    "nm_elements = 1024\n",
    "X, y = make_moons(n_samples=batch_size * (nm_elements // batch_size), noise=0.2, random_state=42)\n",
    "\n",
    "# Plot the dataset\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', edgecolor='k')\n",
    "plt.title(\"Two Moons Dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we split the dataset in training batches and do the same for the generated test set.\n",
    "train_dl = list(zip(X.reshape(-1, batch_size, 2), y.reshape(-1, batch_size)))\n",
    "\n",
    "X_test, y_test = make_moons(n_samples=batch_size * (nm_elements // batch_size) // 2, noise=0.2, random_state=0)\n",
    "test_dl = tuple(zip(X_test.reshape(-1, batch_size, 2), y_test.reshape(-1, batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nm_epochs = 256 // (nm_elements // batch_size)\n",
    "\n",
    "# Note how the text \"Training!\" appears only once. This is because 'train_on_batch' is executed only once,\n",
    "# and then its compiled equivalent is instead used (which only cares about what happens to jax.Arrays and\n",
    "# discards all python code).\n",
    "\n",
    "for e in range(nm_epochs):\n",
    "    train(train_dl, model=model, optim_w=optim_w)\n",
    "    a, y = eval(test_dl, model=model)\n",
    "    \n",
    "    # We print the average shift of the first vode during the inference steps. Note that it does not depend on\n",
    "    # the choice for the batch_size (feel free to play around with it, remember to reset the notebook if you\n",
    "    # you change it). This is because we multiply the learning rate of 'optim_h' by the batch_size. This is \n",
    "    # because the total energy is averaged over the batch dimension (as required for the weight updates),\n",
    "    # so we need to scale the learning rate accordingly for the vode updates.\n",
    "    print(f\"Epoch {e + 1}/{nm_epochs} - Test Accuracy: {a * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model on the grid of points in the range [-2.5, 2.5]x[-2.5, 2.5]\n",
    "X_grid = jax.numpy.stack(np.meshgrid(np.linspace(-2.5, 2.5, 96), np.linspace(-2.0, 2.0, 96))).reshape(2, -1).T\n",
    "with pxu.step(model):\n",
    "    y_grid = forward(X_grid, model=model).argmax(axis=-1)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.scatter(X_grid[:, 0], X_grid[:, 1], c=y_grid, cmap='viridis', s=14, marker='o', linewidths=0, alpha=0.2)\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], c=y, cmap='viridis', edgecolor='k')\n",
    "plt.title(\"Prediction on Two Moons Dataset\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
