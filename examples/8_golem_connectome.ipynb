{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here we run benchmark causal discovery algorithms on the big ER graph and connectome graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# choose the GPU to use\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "# disable preallocation of memory\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\n",
    "\n",
    "# pcax\n",
    "import pcx as px\n",
    "import pcx.predictive_coding as pxc\n",
    "import pcx.nn as pxnn\n",
    "import pcx.functional as pxf\n",
    "import pcx.utils as pxu\n",
    "\n",
    "# 3rd party\n",
    "import jax\n",
    "from jax import jit\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "import numpy as np\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import timeit\n",
    "\n",
    "# own\n",
    "import causal_helpers\n",
    "from causal_helpers import simulate_dag, simulate_parameter, simulate_linear_sem, simulate_linear_sem_cyclic\n",
    "from causal_helpers import load_adjacency_matrix, set_random_seed, plot_adjacency_matrices\n",
    "\n",
    "# Set random seed\n",
    "seed = 23\n",
    "set_random_seed(seed)\n",
    "\n",
    "# causal libraries\n",
    "import cdt, castle\n",
    "\n",
    "# causal metrics\n",
    "from cdt.metrics import precision_recall, SHD, SID\n",
    "from castle.metrics import MetricsDAG\n",
    "from castle.common import GraphDAG\n",
    "from causallearn.graph.SHD import SHD as SHD_causallearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the actual connectome data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the weighted adjacency matrices for ER and connectome\n",
    "\n",
    "# Specify the folder where the adjacency matrices were saved\n",
    "folder = '../data/'\n",
    "\n",
    "# Example usage to load the saved adjacency matrices\n",
    "G_A_init_t_ordered_adj_matrix = load_adjacency_matrix(os.path.join(folder, 'G_A_init_t_ordered_adj_matrix.npy'))\n",
    "G_A_init_t_ordered_dag_adj_matrix = load_adjacency_matrix(os.path.join(folder, 'G_A_init_t_ordered_dag_adj_matrix.npy'))\n",
    "ER = load_adjacency_matrix(os.path.join(folder, 'ER_adj_matrix.npy'))\n",
    "ER_dag = load_adjacency_matrix(os.path.join(folder, 'ER_dag_adj_matrix.npy'))\n",
    "\n",
    "# Change name of the connectome adjacency matrix to C and C_dag\n",
    "C = G_A_init_t_ordered_adj_matrix\n",
    "C_dag = G_A_init_t_ordered_dag_adj_matrix\n",
    "\n",
    "# Now ensure that both DAG adjacency matrices are binary, if they aren't already\n",
    "ER_dag_bin = (ER_dag != 0).astype(int)\n",
    "C_dag_bin = (C_dag != 0).astype(int)\n",
    "\n",
    "ER_true = ER_dag_bin\n",
    "C_true = C_dag_bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data to debug and implement the pcax version of NOTEARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â actual data\n",
    "#B_true = simulate_dag(d=100, s0=400, graph_type='ER') # ER4\n",
    "# debugging data\n",
    "B_true = simulate_dag(d=5, s0=10, graph_type='ER') # ER2\n",
    "\n",
    "\n",
    "#B_true = C_dag_bin # if you want to use the connectome-based DAG # best performance so far with 200,000 samples: 0.06 \n",
    "#B_true = ER_dag_bin # if you want to use the ER-based DAG\n",
    "\n",
    "\n",
    "#B_true = simulate_dag(d=50, s0=100, graph_type='ER') # ER2\n",
    "#B_true = simulate_dag(d=100, s0=200, graph_type='ER') # ER2\n",
    "#B_true = simulate_dag(d=279, s0=558, graph_type='ER') # ER2\n",
    "\n",
    "# create equivalent ER4 and ER6 graphs\n",
    "#B_true = simulate_dag(d=279, s0=1116, graph_type='ER') # ER4\n",
    "#B_true = simulate_dag(d=279, s0=1674, graph_type='ER') # ER6\n",
    "\n",
    "# create equivalent SF4 and SF6 graphs\n",
    "#B_true = simulate_dag(d=100, s0=600, graph_type='SF') # SF6\n",
    "#B_true = simulate_dag(d=279, s0=1116, graph_type='SF') # SF4\n",
    "#B_true = simulate_dag(d=279, s0=1674, graph_type='SF') # SF6\n",
    "\n",
    "\n",
    "# create simple data using simulate_dag method from causal_helpers with expected number of edges (s0) and number of nodes (d)\n",
    "#B_true = simulate_dag(d=100, s0=199, graph_type='ER') # we use pâ‰ˆ0.040226 for the connectome-based ER_dag graph. This means that the expected number of edges is 0.040226 * d * (d-1) / 2\n",
    "# examples: d=50 -> s0=49 (works), d=100 -> s0=199, d=200 -> s0=800\n",
    "W_true = simulate_parameter(B_true)\n",
    "\n",
    "# sample data from the linear SEM\n",
    "#Â actual data\n",
    "#X = simulate_linear_sem(W_true, n=25000, sem_type='gauss')\n",
    "# for debugging\n",
    "X = simulate_linear_sem(W_true, n=1000, sem_type='gauss')\n",
    "#X = simulate_linear_sem(W_true, n=2500, sem_type='gauss')\n",
    "#X = simulate_linear_sem(W_true, n=6250, sem_type='gauss')\n",
    "#X = simulate_linear_sem(W_true, n=50000, sem_type='gauss')\n",
    "#X = simulate_linear_sem(W_true, n=100000, sem_type='gauss') # 1000*(279**2)/(20**2) = 194602\n",
    "\n",
    "# now standardized data, where each variable is normalized to unit variance\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_std = scaler.fit_transform(X)\n",
    "\n",
    "# NOTE: you may not write positional arguments after keyword arguments. \n",
    "# That is, the values that you are passing positionally have to come first!\n",
    "\n",
    "# create a dataset using the simulated data\n",
    "# NOTE: NOTEARS paper uses n=1000 for graph with d=20.\n",
    "# NOTE: d... number of nodes, p=d^2... number of parameters, n... number of samples. Then: comparing p1=d1^2 vs p2=d2^2 we have that: n1/p1 must be equal to n2/p2\n",
    "# Thus we have n2 = n1 * p2 / p1. For the case of d2=100 we have that n2 = (n1*p2)/p1 = 1000*(100^2)/(20^2) = 25000 \n",
    "# we should expect to use that many samples actually to be able to learn the graph in a comparable way.\n",
    "#dataset = IIDSimulation(W=W_true, n=25000, method='linear', sem_type='gauss')\n",
    "#true_dag, X = dataset.B, dataset.X\n",
    "\n",
    "# print how many non-zero entries are in the true DAG\n",
    "print(f\"Number of non-zero entries in the true DAG: {np.count_nonzero(B_true)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the PCAX version of NOTEARS ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v1: single vode\n",
    "class Complete_Graph(pxc.EnergyModule):\n",
    "    def __init__(self, input_dim: int, n_nodes: int, has_bias: bool = False) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = px.static(input_dim)  # Ensure input_dim is static\n",
    "        self.n_nodes = px.static(n_nodes)  # Keep n_nodes as a static value\n",
    "        self.has_bias = has_bias\n",
    "\n",
    "        # Initialize a single linear layer for the weights and wrap it in a list\n",
    "        self.layers = [pxnn.Linear(n_nodes * input_dim, n_nodes * input_dim, bias=has_bias)]\n",
    "        \n",
    "        # Zero out the diagonal weights to avoid self-loops\n",
    "        weight_matrix = self.layers[0].nn.weight.get()\n",
    "        weight_matrix = weight_matrix.reshape(n_nodes, input_dim, n_nodes, input_dim)\n",
    "        for i in range(n_nodes):\n",
    "            weight_matrix = weight_matrix.at[i, :, i, :].set(jnp.zeros((input_dim, input_dim)))\n",
    "        self.layers[0].nn.weight.set(weight_matrix.reshape(n_nodes * input_dim, n_nodes * input_dim))\n",
    "\n",
    "        # Initialize vodes as a list containing a single matrix\n",
    "        self.vodes = [pxc.Vode((n_nodes, input_dim))]\n",
    "\n",
    "    def freeze_nodes(self, freeze=True):\n",
    "        self.vodes[0].h.frozen = freeze\n",
    "\n",
    "    def are_vodes_frozen(self):\n",
    "        \"\"\"Check if all vodes in the model are frozen.\"\"\"\n",
    "        return self.vodes[0].h.frozen\n",
    "    \n",
    "    def get_W(self):\n",
    "        \"\"\"This function returns the weighted adjacency matrix based on the linear layer in the model.\"\"\"\n",
    "        W = self.layers[0].nn.weight.get()\n",
    "        W_T = W.T\n",
    "        return W_T\n",
    "\n",
    "    def __call__(self, x=None):\n",
    "        n_nodes = self.n_nodes.get()\n",
    "        input_dim = self.input_dim.get()\n",
    "        if x is not None:\n",
    "            # print the shape of x before reshaping when x is not None\n",
    "            #print(\"The shape of x before reshaping when x is not None: \", x.shape)\n",
    "\n",
    "            # Initialize nodes with given data\n",
    "            reshaped_x = x.reshape(n_nodes, input_dim)\n",
    "\n",
    "            # print the shape of reshaped_x when x is not None\n",
    "            #print(\"The shape of reshaped_x when x is not None: \", reshaped_x.shape)\n",
    "\n",
    "            self.vodes[0](reshaped_x)\n",
    "        else:\n",
    "            # Perform forward pass using stored values\n",
    "            #x_ = self.vodes[0].get('h').reshape(n_nodes * input_dim, 1)\n",
    "\n",
    "            x_ = self.vodes[0].get('h')\n",
    "            # print the shape of x_ when x is None before reshaping\n",
    "            #print(\"The shape of x_ when x is None before reshaping: \", x_.shape)\n",
    "\n",
    "            x_ = x_.reshape(n_nodes * input_dim, 1)\n",
    "            # print the shape of x_ when x is None after reshaping\n",
    "            #print(\"The shape of x_ when x is None after reshaping: \", x_.shape)\n",
    "\n",
    "            # Perform the matrix-matrix multiplication\n",
    "            #output = self.layers[0](x_).reshape(n_nodes, input_dim)\n",
    "\n",
    "            output = self.layers[0](x_)\n",
    "            # print the shape of output before reshaping\n",
    "            #print(\"The shape of output before reshaping: \", output.shape)\n",
    "            #output = output.reshape(n_nodes, input_dim)\n",
    "            # print the shape of output after reshaping\n",
    "            #print(\"The shape of output after reshaping: \", output.shape)\n",
    "\n",
    "            # Set the new values in vodes\n",
    "            self.vodes[0](output)\n",
    "\n",
    "        # Return the output directly\n",
    "        return self.vodes[0].get('h')\n",
    "\n",
    "# Usage\n",
    "input_dim = 1\n",
    "n_nodes = X.shape[1]\n",
    "model = Complete_Graph(input_dim, n_nodes, has_bias=False)\n",
    "\n",
    "# Get weighted adjacency matrix\n",
    "W = model.get_W()\n",
    "print(W)\n",
    "print()\n",
    "print(W.shape)\n",
    "print(model)\n",
    "\n",
    "\n",
    "# Freezing all nodes\n",
    "model.freeze_nodes(freeze=True)\n",
    "\n",
    "# Check if all nodes are frozen\n",
    "print(model.are_vodes_frozen())\n",
    "\n",
    "# Unfreezing all nodes\n",
    "model.freeze_nodes(freeze=False)\n",
    "\n",
    "# Check if all nodes are frozen\n",
    "print(model.are_vodes_frozen())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: make the below params global or input to the functions in which it is used.\n",
    "w_learning_rate = 5e-3 # Notes: 5e-1 is too high\n",
    "h_learning_rate = 5e-4\n",
    "T = 1\n",
    "\n",
    "nm_epochs = 5000\n",
    "batch_size = 128\n",
    "\n",
    "lam_h = 5e2 # 2e2 -> 5e2 # this move works well! FIRST MOVE\n",
    "lam_l1 = 3e-2 # 1e-2 -> 3e-2 # this move works well! SECOND MOVE\n",
    "# TODO: check if one can start with 5e-2 for lam_l1 and 5e3 for lam_h directly instead (run for at least 300.000 epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and evaluation functions\n",
    "@pxf.vmap(pxu.M(pxc.VodeParam | pxc.VodeParam.Cache).to((None, 0)), in_axes=(0,), out_axes=0)\n",
    "def forward(x, *, model: Complete_Graph):\n",
    "    return model(x)\n",
    "\n",
    "@pxf.vmap(pxu.M(pxc.VodeParam | pxc.VodeParam.Cache).to((None, 0)), out_axes=(None, 0), axis_name=\"batch\") # if only one output\n",
    "def energy(*, model: Complete_Graph):\n",
    "    x_ = model(None)\n",
    "    \n",
    "    W = model.get_W()\n",
    "    d = model.n_nodes.get()\n",
    "\n",
    "    # compute the PC energy term (loss) - equivalent to negative log-likelihood\n",
    "    # thus no need to consider the logdet term: -jnp.linalg.slogdet(jnp.eye(d) - W)[1]\n",
    "    energy = model.energy()\n",
    "\n",
    "    # compute L1 regularization term of W (not normalized by batch size)\n",
    "    l1_reg = jnp.sum(jnp.abs(W))\n",
    "\n",
    "    # compute the DAG penalty term using the matrix exponential (h_reg)\n",
    "    #h_reg = jnp.trace(jax.scipy.linalg.expm(W * W)) - d\n",
    "    h_reg = jnp.trace(jax.scipy.linalg.expm(jnp.multiply(W, W))) - d\n",
    "    \n",
    "    # Combine loss, soft DAG constraint, and L1 regularization\n",
    "    obj = jax.lax.pmean(energy, axis_name=\"batch\") + lam_h * h_reg + lam_l1 * l1_reg\n",
    "\n",
    "    return obj, x_\n",
    "\n",
    "@pxf.jit(static_argnums=0)\n",
    "def train_on_batch(T: int, x: jax.Array, *, model: Complete_Graph, optim_w: pxu.Optim, optim_h: pxu.Optim):\n",
    "\n",
    "    print(\"Training!\")  # this will come in handy later\n",
    "\n",
    "    # This only sets an internal flag to be \"train\" (instead of \"eval\")\n",
    "    model.train()\n",
    "\n",
    "    # freeze nodes at the start of training\n",
    "    model.freeze_nodes(freeze=True)\n",
    "\n",
    "    # init step\n",
    "    with pxu.step(model, pxc.STATUS.INIT, clear_params=pxc.VodeParam.Cache):\n",
    "        forward(x, model=model)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    # The following code might not be needed as we are keeping the vodes frozen at all times\n",
    "    # Reinitialize the optimizer state between different batches\n",
    "    optim_h.init(pxu.M(pxc.VodeParam)(model))\n",
    "\n",
    "    for _ in range(T):\n",
    "        with pxu.step(model, clear_params=pxc.VodeParam.Cache):\n",
    "            (e, x_), g = pxf.value_and_grad(\n",
    "                pxu.M(pxu.m(pxc.VodeParam).has_not(frozen=True), [False, True]),\n",
    "                has_aux=True\n",
    "            )(energy)(model=model)\n",
    "        optim_h.step(model, g[\"model\"], True)\n",
    "    \"\"\"\n",
    "        \n",
    "    with pxu.step(model, clear_params=pxc.VodeParam.Cache):\n",
    "        _, g = pxf.value_and_grad(pxu.M_hasnot(pxc.VodeParam, frozen=True).to([False, True]), has_aux=True)(energy)(model=model) # pxf.value_and_grad returns a tuple structured as ((value, aux), grad), not as six separate outputs.\n",
    "        \n",
    "        # Zero out the diagonal gradients using jax.numpy.fill_diagonal\n",
    "        weight_grads = g[\"model\"].layers[0].nn.weight.get()\n",
    "        weight_grads = jax.numpy.fill_diagonal(weight_grads, 0.0, inplace=False)\n",
    "        # print the grad values using the syntax jax.debug.print(\"ðŸ¤¯ {x} ðŸ¤¯\", x=x)\n",
    "        #jax.debug.print(\"{weight_grads}\", weight_grads=weight_grads)\n",
    "        g[\"model\"].layers[0].nn.weight.set(weight_grads)\n",
    "\n",
    "    optim_w.step(model, g[\"model\"])\n",
    "\n",
    "    with pxu.step(model, clear_params=pxc.VodeParam.Cache):\n",
    "        forward(None, model=model)\n",
    "        e_avg_per_sample = model.energy() # this returns average value of objective per sample in the batch\n",
    "\n",
    "    # unfreeze nodes at the end of training\n",
    "    model.freeze_nodes(freeze=False)\n",
    "\n",
    "    return e_avg_per_sample\n",
    "\n",
    "def train(dl, T, *, model: Complete_Graph, optim_w: pxu.Optim, optim_h: pxu.Optim):\n",
    "    e_avg_per_sample_energies = []\n",
    "    for batch in dl:\n",
    "\n",
    "        e_avg_per_sample = train_on_batch(T, batch, model=model, optim_w=optim_w, optim_h=optim_h)\n",
    "        e_avg_per_sample_energies.append(e_avg_per_sample)\n",
    "\n",
    "    W = model.get_W()\n",
    "\n",
    "    # compute epoch energy\n",
    "    epoch_energy = jnp.mean(jnp.array(e_avg_per_sample_energies))\n",
    "    return W, epoch_energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def MAE(W_true, W):\n",
    "    \"\"\"This function returns the Mean Absolute Error for the difference between the true weighted adjacency matrix W_true and th estimated one, W.\"\"\"\n",
    "    MAE_ = jnp.mean(jnp.abs(W - W_true))\n",
    "    return MAE_\n",
    "\n",
    "def compute_binary_adjacency(W, threshold=0.3):\n",
    "    \"\"\"\n",
    "    Compute the binary adjacency matrix by thresholding the input matrix.\n",
    "\n",
    "    Args:\n",
    "    - W (array-like): The weighted adjacency matrix (can be a JAX array or a NumPy array).\n",
    "    - threshold (float): The threshold value to determine the binary matrix. Default is 0.3.\n",
    "\n",
    "    Returns:\n",
    "    - B_est (np.ndarray): The binary adjacency matrix where each element is True if the corresponding \n",
    "                          element in W is greater than the threshold, otherwise False.\n",
    "    \"\"\"\n",
    "    # Convert JAX array to NumPy array if necessary\n",
    "    if isinstance(W, jnp.ndarray):\n",
    "        W = np.array(W)\n",
    "\n",
    "    # Compute the binary adjacency matrix\n",
    "    B_est = np.array(np.abs(W) > threshold)\n",
    "    \n",
    "    return B_est\n",
    "\n",
    "\n",
    "def ensure_DAG(W):\n",
    "    \"\"\"\n",
    "    Ensure that the weighted adjacency matrix corresponds to a DAG.\n",
    "\n",
    "    Inputs:\n",
    "        W: numpy.ndarray - a weighted adjacency matrix representing a directed graph\n",
    "\n",
    "    Outputs:\n",
    "        W: numpy.ndarray - a weighted adjacency matrix without cycles (DAG)\n",
    "    \"\"\"\n",
    "    # Convert the adjacency matrix to a directed graph\n",
    "    g = nx.DiGraph(W)\n",
    "\n",
    "    # Make a copy of the graph to modify\n",
    "    gg = g.copy()\n",
    "\n",
    "    # Remove cycles by removing edges\n",
    "    while not nx.is_directed_acyclic_graph(gg):\n",
    "        h = gg.copy()\n",
    "\n",
    "        # Remove all the sources and sinks\n",
    "        while True:\n",
    "            finished = True\n",
    "\n",
    "            for node, in_degree in nx.in_degree_centrality(h).items():\n",
    "                if in_degree == 0:\n",
    "                    h.remove_node(node)\n",
    "                    finished = False\n",
    "\n",
    "            for node, out_degree in nx.out_degree_centrality(h).items():\n",
    "                if out_degree == 0:\n",
    "                    h.remove_node(node)\n",
    "                    finished = False\n",
    "\n",
    "            if finished:\n",
    "                break\n",
    "\n",
    "        # Find a cycle with a random walk starting at a random node\n",
    "        node = list(h.nodes)[0]\n",
    "        cycle = [node]\n",
    "        while True:\n",
    "            edges = list(h.out_edges(node))\n",
    "            _, node = edges[np.random.choice(len(edges))]\n",
    "\n",
    "            if node in cycle:\n",
    "                break\n",
    "\n",
    "            cycle.append(node)\n",
    "\n",
    "        # Extract the cycle path and adjust it to start at the first occurrence of the repeated node\n",
    "        cycle = np.array(cycle)\n",
    "        i = np.argwhere(cycle == node)[0][0]\n",
    "        cycle = cycle[i:]\n",
    "        cycle = cycle.tolist() + [node]\n",
    "\n",
    "        # Find edges in that cycle\n",
    "        edges = list(zip(cycle[:-1], cycle[1:]))\n",
    "\n",
    "        # Randomly pick an edge to remove\n",
    "        edge = edges[np.random.choice(len(edges))]\n",
    "        gg.remove_edge(*edge)\n",
    "\n",
    "    # Convert the modified graph back to a weighted adjacency matrix\n",
    "    W_acyclic = nx.to_numpy_array(gg)\n",
    "\n",
    "    return W_acyclic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reference compute the MAE, SID, and SHD between the true adjacency matrix and an all-zero matrix and then print it\n",
    "# this acts as a baseline for the MAE, SID, and SHD similar to how 1/K accuracy acts as a baseline for classification tasks where K is the number of classes\n",
    "W_zero = np.zeros_like(W_true)\n",
    "print(\"MAE between the true adjacency matrix and an all-zero matrix: \", MAE(W_true, W_zero))\n",
    "print(\"SHD between the true adjacency matrix and an all-zero matrix: \", SHD(B_true, compute_binary_adjacency(W_zero)))\n",
    "#print(\"SID between the true adjacency matrix and an all-zero matrix: \", SID(W_true, W_zero))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X):\n",
    "        self.X = X\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx]\n",
    "\n",
    "# This is a simple collate function that stacks numpy arrays used to interface\n",
    "# the PyTorch dataloader with JAX. In the future we hope to provide custom dataloaders\n",
    "# that are independent of PyTorch.\n",
    "def numpy_collate(batch):\n",
    "    if isinstance(batch[0], np.ndarray):\n",
    "        return np.stack(batch)\n",
    "    elif isinstance(batch[0], (tuple, list)):\n",
    "        transposed = zip(*batch)\n",
    "        return [numpy_collate(samples) for samples in transposed]\n",
    "    else:\n",
    "        return np.array(batch)\n",
    "\n",
    "# The dataloader assumes cuda is being used, as such it sets 'pin_memory = True' and\n",
    "# 'prefetch_factor = 2'. Note that the batch size should be constant during training, so\n",
    "# we set 'drop_last = True' to avoid having to deal with variable batch sizes.\n",
    "class TorchDataloader(torch.utils.data.DataLoader):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset,\n",
    "        batch_size=1,\n",
    "        shuffle=None,\n",
    "        sampler=None,\n",
    "        batch_sampler=None,\n",
    "        num_workers=1,\n",
    "        pin_memory=True,\n",
    "        timeout=0,\n",
    "        worker_init_fn=None,\n",
    "        persistent_workers=True,\n",
    "        prefetch_factor=2,\n",
    "    ):\n",
    "        super(self.__class__, self).__init__(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            sampler=sampler,\n",
    "            batch_sampler=batch_sampler,\n",
    "            num_workers=num_workers,\n",
    "            collate_fn=numpy_collate,\n",
    "            pin_memory=pin_memory,\n",
    "            drop_last=True if batch_sampler is None else None,\n",
    "            timeout=timeout,\n",
    "            worker_init_fn=worker_init_fn,\n",
    "            persistent_workers=persistent_workers,\n",
    "            prefetch_factor=prefetch_factor,\n",
    "        )\n",
    "\n",
    "\n",
    "# Create the custom dataset\n",
    "dataset = CustomDataset(X)\n",
    "# Create the custom dataset with standardized data\n",
    "dataset_std = CustomDataset(X_std)\n",
    "\n",
    "# Create the dataloader\n",
    "dl = TorchDataloader(dataset, batch_size=batch_size, shuffle=True)\n",
    "######## OR ########\n",
    "#dl = TorchDataloader(dataset_std, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model and optimizers\n",
    "# Initialize the model and optimizers\n",
    "with pxu.step(model, pxc.STATUS.INIT, clear_params=pxc.VodeParam.Cache):\n",
    "    forward(jnp.zeros((batch_size, model.n_nodes.get())), model=model)\n",
    "    optim_h = pxu.Optim(lambda: optax.sgd(h_learning_rate))\n",
    "\n",
    "    \"\"\"\n",
    "    optim_w = pxu.Optim(\n",
    "    optax.chain(\n",
    "        optax.clip_by_global_norm(clip_value),  # Clip gradients by global norm\n",
    "        optax.sgd(w_learning_rate)  # Apply SGD optimizer\n",
    "    ),\n",
    "    pxu.M(pxnn.LayerParam)(model)  # Masking the parameters of the model\n",
    ")\n",
    "    \"\"\"\n",
    "    #optim_w = pxu.Optim(optax.adafactor(w_learning_rate), pxu.M(pxnn.LayerParam)(model))\n",
    "    #optim_w = pxu.Optim(optax.sgd(w_learning_rate, momentum=0.95), pxu.M(pxnn.LayerParam)(model))\n",
    "    #optim_w = pxu.Optim(optax.adamw(w_learning_rate, weight_decay=5e-2), pxu.M(pxnn.LayerParam)(model))\n",
    "    #optim_w = pxu.Optim(optax.adamw(w_learning_rate, nesterov=True), pxu.M(pxnn.LayerParam)(model))\n",
    "    #optim_w = pxu.Optim(optax.adamw(w_learning_rate, nesterov=False), pxu.M(pxnn.LayerParam)(model))\n",
    "    optim_w = pxu.Optim(lambda: optax.adam(w_learning_rate), pxu.M(pxnn.LayerParam)(model))\n",
    "\n",
    "# Initialize lists to store differences and energies\n",
    "MAEs = []\n",
    "SHDs = []\n",
    "energies = []\n",
    "\n",
    "# Calculate the initial MAE, SID, and SHD\n",
    "MAE_init = MAE(W_true, model.get_W())\n",
    "print(f\"Start difference (cont.) between W_true and W_init: {MAE_init:.4f}\")\n",
    "\n",
    "SHD_init = SHD(B_true, compute_binary_adjacency(model.get_W()))\n",
    "print(f\"Start SHD between B_true and B_init: {SHD_init:.4f}\")\n",
    "\n",
    "# print the values of the diagonal of the initial W\n",
    "print(\"The diagonal of the initial W: \", jnp.diag(model.get_W()))\n",
    "\n",
    "# Start timing\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "# Training loop\n",
    "with tqdm(range(nm_epochs), position=0, leave=True) as pbar:\n",
    "    for epoch in pbar:\n",
    "        # Train for one epoch using the dataloader\n",
    "        W, epoch_energy = train(dl, T=T, model=model, optim_w=optim_w, optim_h=optim_h)\n",
    "        \n",
    "        # Calculate the metrics and store them\n",
    "        W = np.array(W)\n",
    "        MAEs.append(float(MAE(W_true, W)))\n",
    "        SHDs.append(float(SHD(B_true, compute_binary_adjacency(W))))\n",
    "        energies.append(float(epoch_energy))\n",
    "        \n",
    "        # Update progress bar with the current status\n",
    "        pbar.set_description(f\"MAE {MAEs[-1]:.4f}, SHD {SHDs[-1]:.4f} || Energy {energies[-1]:.4f}\")\n",
    "\n",
    "# End timing\n",
    "end_time = timeit.default_timer()\n",
    "\n",
    "# Print the average time per epoch\n",
    "average_time_per_epoch = (end_time - start_time) / nm_epochs\n",
    "print(f\"An epoch (with compiling and testing) took on average: {average_time_per_epoch:.4f} seconds\")\n",
    "# print the values of the diagonal of the final W\n",
    "print(\"The diagonal of the final W: \", jnp.diag(model.get_W()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)\n",
    "print()\n",
    "with pxu.step(model, clear_params=pxc.VodeParam.Cache):\n",
    "    _, g = pxf.value_and_grad(\n",
    "            pxu.M(pxnn.LayerParam).to([False, True]), \n",
    "            has_aux=True\n",
    "        )(energy)(model=model) # pxf.value_and_grad returns a tuple structured as ((value, aux), grad), not as six separate outputs.\n",
    "    print(g[\"model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the style and color palette\n",
    "sns.set(style=\"whitegrid\")\n",
    "palette = sns.color_palette(\"tab10\")\n",
    "\n",
    "# Create a figure and axis with a 1x3 layout for side-by-side plots\n",
    "fig, axs = plt.subplots(1, 3, figsize=(18, 5))  # Adjusting layout to 1 row and 3 columns\n",
    "fig.suptitle('Performance Metrics Over Epochs', fontsize=16, weight='bold')\n",
    "\n",
    "# Plot the MAE\n",
    "sns.lineplot(x=range(len(MAEs)), y=MAEs, ax=axs[0], color=palette[0])\n",
    "axs[0].set_title(\"Mean Absolute Error (MAE)\", fontsize=14)\n",
    "axs[0].set_xlabel(\"Epoch\", fontsize=12)\n",
    "axs[0].set_ylabel(\"MAE\", fontsize=12)\n",
    "axs[0].grid(True)\n",
    "\n",
    "# Plot the SHD\n",
    "sns.lineplot(x=range(len(SHDs)), y=SHDs, ax=axs[1], color=palette[2])\n",
    "axs[1].set_title(\"Structural Hamming Distance (SHD)\", fontsize=14)\n",
    "axs[1].set_xlabel(\"Epoch\", fontsize=12)\n",
    "axs[1].set_ylabel(\"SHD\", fontsize=12)\n",
    "axs[1].grid(True)\n",
    "\n",
    "# Plot the Energy\n",
    "sns.lineplot(x=range(len(energies)), y=energies, ax=axs[2], color=palette[3])\n",
    "axs[2].set_title(\"Energy\", fontsize=14)\n",
    "axs[2].set_xlabel(\"Epoch\", fontsize=12)\n",
    "axs[2].set_ylabel(\"Energy\", fontsize=12)\n",
    "axs[2].grid(True)\n",
    "\n",
    "# Improve layout and show the plot\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjust layout to fit the suptitle\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now use a threshold of 0.3 to binarize the weighted adjacency matrix W\n",
    "W_est = np.array(model.get_W())\n",
    "B_est = compute_binary_adjacency(W_est, threshold=0.3)\n",
    "\n",
    "W_fix = ensure_DAG(W_est)\n",
    "B_fix = 1.0*(W_fix != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if B_est is indeed a DAG\n",
    "def is_dag(adjacency_matrix):\n",
    "    \"\"\"\n",
    "    Check if a given adjacency matrix represents a Directed Acyclic Graph (DAG).\n",
    "    \n",
    "    Parameters:\n",
    "        adjacency_matrix (numpy.ndarray): A square matrix representing the adjacency of a directed graph.\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if the graph is a DAG, False otherwise.\n",
    "    \"\"\"\n",
    "    # Create a directed graph from the adjacency matrix\n",
    "    graph = nx.DiGraph(adjacency_matrix)\n",
    "    \n",
    "    # Check if the graph is a DAG\n",
    "    return nx.is_directed_acyclic_graph(graph)\n",
    "\n",
    "# Example usage:\n",
    "adj_matrix = np.array([\n",
    "    [0, 1, 0],\n",
    "    [0, 0, 1],\n",
    "    [1, 0, 0]\n",
    "])\n",
    "\n",
    "print(is_dag(adj_matrix))  # Output: False, since the graph has a cycle.\n",
    "\n",
    "# Check if the estimated binary adjacency matrix B_est is a DAG\n",
    "is_dag_B_est = is_dag(B_est)\n",
    "print(f\"Is the estimated binary adjacency matrix a DAG? {is_dag_B_est}\")\n",
    "\n",
    "# Define fucntion to compute h_reg based W with h_reg = jnp.trace(jax.scipy.linalg.expm(W * W)) - d, here * denotes the hadamard product\n",
    "def compute_h_reg(W):\n",
    "    \"\"\"This function computes the h_reg term based on the matrix W.\"\"\"\n",
    "    h_reg = jnp.trace(jax.scipy.linalg.expm(W * W)) - W.shape[0]\n",
    "    return h_reg\n",
    "\n",
    "# Compute the h_reg term for the true weighted adjacency matrix W_true\n",
    "h_reg_true = compute_h_reg(W_true)\n",
    "print(f\"The h_reg term for the true weighted adjacency matrix W_true is: {h_reg_true:.4f}\")\n",
    "# Compute the h_reg term for the estimated weighted adjacency matrix W_est\n",
    "h_reg_est = compute_h_reg(W_est)\n",
    "print(f\"The h_reg term for the estimated weighted adjacency matrix W_est is: {h_reg_est:.4f}\")\n",
    "h_reg_fix = compute_h_reg(W_fix)\n",
    "print(f\"The h_reg term for the fixed weighted adjacency matrix W_fix is: {h_reg_fix:.4f}\")\n",
    "\n",
    "# We note that B_est is a DAG even though h_reg is not equal to 0.0 (but only close to 0.0). \n",
    "# This is because the matrix exponential is not a perfect measure of the DAG constraint, but it is a good approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print first 5 rows and columsn of W_est and round values to 4 decimal places and show as non-scientific notation\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "print(\"The first 5 rows and columns of the estimated weighted adjacency matrix W_est\\n{}\".format(W_est[:5, :5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now show the adjacency matrix of the true graph and the estimated graph side by side\n",
    "plot_adjacency_matrices(B_true, B_est)\n",
    "\n",
    "# now show the adjacency matrix of the true graph and the estimated graph side by side\n",
    "plot_adjacency_matrices(B_true, B_fix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(B_est))\n",
    "print(np.sum(B_fix))\n",
    "print(np.sum(B_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot est_dag and true_dag\n",
    "GraphDAG(B_est, B_true)\n",
    "# calculate accuracy\n",
    "met_pcax = MetricsDAG(B_est, B_true)\n",
    "print(met_pcax.metrics)\n",
    "\n",
    "met_pcax_fix = MetricsDAG(B_fix, B_true)\n",
    "print(met_pcax_fix.metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pcax24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
