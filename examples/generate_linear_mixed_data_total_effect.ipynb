{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "print(\"Current Working Directory:\", os.getcwd())\n",
    "\n",
    "# Add the examples directory to sys.path so we can import the set_random_seed function and other utilities from causal_helpers\n",
    "examples_dir = os.path.abspath(os.path.join(os.getcwd(), '..', 'examples'))\n",
    "sys.path.append(examples_dir)\n",
    "# Now import set_random_seed directly from causal_helpers\n",
    "from causal_helpers import set_random_seed\n",
    "\n",
    "# Add the data directory to sys.path so we can save and load data files\n",
    "data_dir = os.path.abspath(os.path.join(examples_dir, '..', 'data'))\n",
    "\n",
    "# Use the function\n",
    "seed = 41 # main seed\n",
    "set_random_seed(seed)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Current Working Directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_parameter(B, w_ranges=((-2.0, -0.5), (0.5, 2.0))):\n",
    "    \"\"\"Simulate SEM parameters for a DAG.\n",
    "\n",
    "    Args:\n",
    "        B (np.ndarray): [d, d] binary adj matrix of DAG\n",
    "        w_ranges (tuple): disjoint weight ranges\n",
    "\n",
    "    Returns:\n",
    "        W (np.ndarray): [d, d] weighted adj matrix of DAG\n",
    "    \"\"\"\n",
    "    W = np.zeros(B.shape)\n",
    "    S = np.random.randint(len(w_ranges), size=B.shape)  # which range\n",
    "    for i, (low, high) in enumerate(w_ranges):\n",
    "        U = np.random.uniform(low=low, high=high, size=B.shape)\n",
    "        W += B * (S == i) * U\n",
    "    return W\n",
    "\n",
    "def sigmoid(x):\n",
    "    s = 1/(1+np.exp(-x))\n",
    "    return s\n",
    "\n",
    "def generate_custom_mixed_data(num_samples, w_ranges=((-2.0, -0.5), (0.5, 2.0)), noise_scale=1.0, softplus=True):\n",
    "    \"\"\"\n",
    "    Generate mixed observational data with continuous and binary variables based on a predefined causal structure.\n",
    "\n",
    "    Parameters:\n",
    "        num_samples (int): Number of samples to generate.\n",
    "        w_ranges (tuple): Ranges for sampling linear weights.\n",
    "        noise_scale (float): Standard deviation of additive noise.\n",
    "        softplus (bool): Whether to apply softplus transformation to the noise.\n",
    "\n",
    "    Returns:\n",
    "        W (np.ndarray): Weighted adjacency matrix of the DAG.\n",
    "        adjacency_matrix (np.ndarray): Adjacency matrix of the DAG.\n",
    "        data (pd.DataFrame): Simulated data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the adjacency matrix for 10 nodes\n",
    "    num_nodes = 10\n",
    "    adjacency_matrix = np.zeros((num_nodes, num_nodes))\n",
    "    adjacency_matrix[3, 0] = 1  # x3 -> x0\n",
    "    adjacency_matrix[3, 2] = 1  # x3 -> x2\n",
    "\n",
    "    adjacency_matrix[2, 1] = 1  # x2 -> x1\n",
    "    adjacency_matrix[2, 4] = 1  # x2 -> x4\n",
    "    \n",
    "    adjacency_matrix[0, 1] = 1  # x0 -> x1    \n",
    "    adjacency_matrix[0, 5] = 1  # x0 -> x5\n",
    "    adjacency_matrix[0, 4] = 1  # x0 -> x4\n",
    "    \n",
    "    adjacency_matrix[4, 6] = 1  # x4 -> x6\n",
    "    adjacency_matrix[5, 7] = 1  # x5 -> x7\n",
    "    adjacency_matrix[6, 8] = 1  # x6 -> x8\n",
    "    adjacency_matrix[7, 9] = 1  # x7 -> x9\n",
    "\n",
    "    # Generate weights for linear equations\n",
    "    W = simulate_parameter(adjacency_matrix, w_ranges)\n",
    "\n",
    "    # Helper function for linear SEM\n",
    "    def simulate_linear(parent_vars, weights, noise_scale, softplus):\n",
    "        noise = np.random.normal(scale=noise_scale, size=parent_vars.shape[0])\n",
    "        if softplus:\n",
    "            noise = np.log(1 + np.exp(noise))  # Apply softplus transformation\n",
    "        return parent_vars @ weights + noise\n",
    "\n",
    "    # Initialize data dictionary\n",
    "    data = {}\n",
    "\n",
    "    # Root nodes\n",
    "    data[\"x3\"] = np.random.uniform(size=num_samples)  # Continuous\n",
    "    data[\"x0\"] = simulate_linear(data[\"x3\"].reshape(-1, 1), W[[3], 0], noise_scale, softplus)\n",
    "    data[\"x2\"] = simulate_linear(data[\"x3\"].reshape(-1, 1), W[[3], 2], noise_scale, softplus)\n",
    "\n",
    "    # Compute x1\n",
    "    parent_vars_x1 = np.column_stack([data[\"x0\"], data[\"x2\"]])\n",
    "    data[\"x1\"] = simulate_linear(parent_vars_x1, W[[0, 2], 1], noise_scale, softplus)\n",
    "\n",
    "    # Compute x5\n",
    "    data[\"x5\"] = simulate_linear(data[\"x0\"].reshape(-1, 1), W[[0], 5], noise_scale, softplus)\n",
    "\n",
    "    # Compute x4\n",
    "    parent_vars_x4 = np.column_stack([data[\"x0\"], data[\"x2\"]])\n",
    "    data[\"x4\"] = simulate_linear(parent_vars_x4, W[[0, 2], 4], noise_scale, softplus)\n",
    "\n",
    "    # Compute downstream variables x6 to x9\n",
    "    parent_vars_x6 = data[\"x4\"].reshape(-1, 1)\n",
    "    data[\"x6\"] = simulate_linear(parent_vars_x6, W[[4], 6], noise_scale, softplus)\n",
    "\n",
    "    parent_vars_x7 = data[\"x5\"].reshape(-1, 1)\n",
    "    data[\"x7\"] = simulate_linear(parent_vars_x7, W[[5], 7], noise_scale, softplus)\n",
    "\n",
    "    parent_vars_x8 = data[\"x6\"].reshape(-1, 1)\n",
    "    data[\"x8\"] = simulate_linear(parent_vars_x8, W[[6], 8], noise_scale, softplus)\n",
    "\n",
    "    parent_vars_x9 = data[\"x7\"].reshape(-1, 1)\n",
    "    data[\"x9\"] = simulate_linear(parent_vars_x9, W[[7], 9], noise_scale, softplus)\n",
    "\n",
    "    # Convert some variables to binary\n",
    "    binary_nodes = [\"x0\", \"x1\", \"x2\", \"x5\", \"x7\"]\n",
    "    for node in binary_nodes:\n",
    "        probabilities = (np.tanh(data[node]) + 1) / 2\n",
    "        data[node] = np.random.binomial(1, probabilities)\n",
    "\n",
    "    # Convert to DataFrame and return\n",
    "    data_df = pd.DataFrame(data)[[f\"x{i}\" for i in range(num_nodes)]]\n",
    "\n",
    "    return W, adjacency_matrix, data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage\n",
    "W, adjacency_matrix, data = generate_custom_mixed_data(num_samples=10000, softplus=True)\n",
    "print(\"Adjacency Matrix:\\n\", adjacency_matrix)\n",
    "print(\"\\nSample Data:\\n\", data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "num_samples = 4000\n",
    "softplus = True\n",
    "w_ranges=((-2.0, -1.0), (1.0, 2.0))\n",
    "W, adj_matrix, data = generate_mixed_data_binary(num_samples=num_samples, w_ranges=w_ranges, softplus=softplus)\n",
    "\n",
    "# Display the adjacency matrix and a preview of the data\n",
    "print(\"Adjacency Matrix:\")\n",
    "print(adj_matrix)\n",
    "print(\"\\nGenerated Data:\")\n",
    "print(data.head())\n",
    "print(\"\\nTrue Weights:\")\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_name = os.path.join(data_dir, \"custom_mixed_confounding\")\n",
    "\n",
    "if not softplus:\n",
    "    os.makedirs(\"custom_mixed_confounding\", exist_ok=True)\n",
    "else:\n",
    "    dir_name += \"_softplus\"\n",
    "    os.makedirs(dir_name, exist_ok=True)\n",
    "\n",
    "# store the adjacency matrix as a csv file named \"adj_matrix.csv\" without header\n",
    "adj_matrix_df = pd.DataFrame(adj_matrix)\n",
    "adj_matrix_df.to_csv(f\"{dir_name}/adj_matrix.csv\", header=False, index=False)\n",
    "\n",
    "# store the data as a csv file named \"train.csv\" without header\n",
    "data.to_csv(f\"{dir_name}/train.csv\", header=False, index=False)\n",
    "\n",
    "# store the weighted adjacency matrix as a csv file named \"W.csv\" without header\n",
    "W_df = pd.DataFrame(W)\n",
    "W_df.to_csv(f\"{dir_name}/W_adj_matrix.csv\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the distribution of all variables against each other and save the plot as \"pairplot.pdf\"\n",
    "import seaborn as sns\n",
    "sns.pairplot(data)\n",
    "plt.savefig(f\"{dir_name}/pairplot.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################## MISC ##################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pcax24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
